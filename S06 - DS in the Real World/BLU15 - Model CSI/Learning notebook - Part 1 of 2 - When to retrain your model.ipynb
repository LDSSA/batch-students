{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1da60e1",
   "metadata": {},
   "source": [
    "# BLU15 - Part 1 of 2 - When to retrain your model\n",
    "In this BLU, we'll talk about updating models over time. We have briefly touched this topic in the time series specialization. Some models are stable over time because the population about which we're making predictions doesn't change. Many real world populations are changing though, so we also have to change our models by retraining them on newer data.\n",
    "\n",
    "We'll consider several retraining strategies in this notebook and in the next one, we'll explain how to diagnose a model before retraining and retrain our baseline model from BLU14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de73d1b-f0cf-4b82-baff-79cc2b5a9a30",
   "metadata": {},
   "source": [
    "## 1. The need for retraining\n",
    "\n",
    "*Train, test and deploy* – that’s it, right? Is your work done? Not quite!\n",
    "\n",
    "So far the process was this:\n",
    "\n",
    "1. Model building starts by learning the dependencies between a set of independent features and the target variable on a set of historical data.\n",
    "2. The best model is found by minimizing the predictions error on the validation dataset which is measured by the selected metric.\n",
    "3. The best model is then deployed to production with the expectation of making accurate predictions on incoming unseen data for as long as possible.\n",
    "\n",
    "One of the biggest mistake a data scientist can make is assume that their models will keep working properly forever after deployment. *But what about the data, which will inevitably keep changing?* \n",
    "\n",
    "A model deployed to production and left to itself won’t be able to adapt to changes in the data by itself.\n",
    "\n",
    "Let's look at the following example. In a [UK bank survey from August 2020](https://www.bankofengland.co.uk/bank-overground/2021/how-has-covid-affected-the-performance-of-machine-learning-models-used-by-uk-banks), 35% of asked bankers reported a negative impact on ML model performance because of the pandemic:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa240ce2-0391-4b1a-af3d-a3ec6204ab17",
   "metadata": {},
   "source": [
    "<img src=\"media/model_impact_of_change.png\" alt=\"drift\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaaaea0-6cf8-45d2-bce8-e9e2b1fb3e1a",
   "metadata": {},
   "source": [
    "Unpredictable events like this are a great example of why continuous monitoring and retraining of ML models in production is important compared to static validation and testing techniques. \n",
    "\n",
    "In the simplest case, retraining involves running the entire existing pipeline with new data, without changing the code or re-building the pipeline. However, if you end up exploring a new algorithm or a feature which was not available at the time of previous model training, then incorporating these changes into the retrained model may further improve the model performance.\n",
    "\n",
    "But what exactly can cause the decrease in model performance?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e9779f-be3e-4405-91da-2dabf8b71900",
   "metadata": {},
   "source": [
    "### 1.1 Data drift\n",
    "\n",
    "To understand this, let us recall one of the most critical assumptions in ML modeling:\n",
    "\n",
    "> The train and test data set should be drawn from the same distribution. \n",
    "\n",
    "The model will perform well if the new data is similar to the data observed in the past on which the model was trained. Therefore, it's understandable that if the test data distribution deviates from that of the train data, the model will not hold well. \n",
    "\n",
    "There are many factors that can cause such deviation. Depending on the business case, it can be a change in consumer preferences, fast moving competitive space, geographic shift, economic conditions, a pandemic, etc. Hence, a drifting data distribution calls for periodically checking the validity of the model. In short, it is critical to keep your machine learning model updated; but the key is when? We will discuss this in a bit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c11503a8-2b08-4adc-b615-bc3183654c46",
   "metadata": {},
   "source": [
    "### 1.2 Robustness\n",
    "\n",
    "As you remember from [SLU17 - Ethics and Fairness](https://github.com/LDSSA/batch8-students/tree/main/S01%20-%20Bootcamp%20and%20Binary%20Classification/SLU17%20-%20Ethics%20and%20Fairness), a model has an impact in the world that it learned from. And that impact can change the *a priori* assumptions that once were true. \n",
    "\n",
    "People/entities that get affected by the outcome of the ML models may deliberately alter their response in order to send spurious input to the model, thereby escaping the impact of the model predictions. For example, models dealing with fraud detection and cyber-security receive manipulated and distorted inputs which cause the model to output misclassified predictions. Such type of adversaries also drives down the model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd86db22-6ccd-467e-80d6-f463e8023775",
   "metadata": {},
   "source": [
    "### 1.3 When the ground truth is not available at the time of model training\n",
    "\n",
    "In some ML models, the ground truth labels are not available to train the model. For example, target variable which captures the response of the end user is not known. In that case, your best bet could be to mock the user action based on certain set of rules coming from business understanding or leverage the open source datasets to initiate model training. But, this model might not necessarily represent the actual data and hence will not perform well until after a burn-in period where it starts picking up (aka learning) the true actions of the end user."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1a86fd6-f4f2-4f04-ab04-e67eec4d1532",
   "metadata": {},
   "source": [
    "### 1.4 Concept drift\n",
    "\n",
    "Concept drift is a phenomenon where the meaning the labels of the target variable you’re trying to predict changes over time. This means that the concept has changed but the model doesn’t know about the change. \n",
    "\n",
    "Concept drift happens when the original idea your model had about the target class changes. For example, you build a model to classify positive and negative sentiment of tweets around certain topics, and over time people’s sentiment about these topics changes. Tweets belonging to positive sentiment may evolve over time to be negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8240798-4f30-40a6-8920-15bb922338e1",
   "metadata": {},
   "source": [
    "## 2. How to measure the decline in model performance?\n",
    "\n",
    "If the ground truth values are stored alongside the predictions, such as with the success of a search, the model accuracy is calculated on a continuous basis to assess the drift.\n",
    "\n",
    "<img src=\"media/model_decay_retraining.png\" alt=\"retraining\" width=\"550\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a46af2d-9951-4b70-b561-d0812d90a381",
   "metadata": {},
   "source": [
    "But what if the prediction horizon is farther into the future and we can’t wait till the ground truth label is observed to assess the model goodness? In that case, we can roughly estimate the retraining window from back-testing. This involves using the ground truth labels and predictions from the historical data to estimate the time frame around which the accuracy begins to taper off.\n",
    "\n",
    "Effectively, the whole exercise of finding the model drift boils down to inferring whether the two data sets (training and test) are coming from the same distribution, or if the performance has fallen below acceptable range.\n",
    "\n",
    "Lets look at some of the ways to assess the distribution drift:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35fb8285-7e2d-4102-9f3d-651761caed6a",
   "metadata": {},
   "source": [
    "### 2.1 Histogram\n",
    "A quick way to visualize the populations is to draw the histogram — the degree of overlap between the two histograms gives a measure of similarity.\n",
    "\n",
    "<img src=\"media/histogram_data_drift.png\" alt=\"histogram\" width=\"600\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6b8385a-33c1-446c-9709-b855e3970e47",
   "metadata": {},
   "source": [
    "### 2.2 K-S statistic\n",
    "\n",
    "The [Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) is a useful tool to check if the upcoming new data belongs to the same distribution as that of the training data. In short, this test quantifies the distance between two distributions. Python has an implementation of this test provided by SciPy ([scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html)).\n",
    "\n",
    "See an illustration of the Kolmogorov–Smirnov statistic below. The red line is the reference cumulative distribution function and the blue line is the sample empirical cumulative distribution function. The difference between the two curves illustrated by the black arrow is the K–S statistic.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cf/KS_Example.png\" alt=\"ks\" width=\"350\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cae141be-f889-453e-b805-271e6f9d479b",
   "metadata": {},
   "source": [
    "### 2.3 Target distribution\n",
    "Another quick way to check the consistency of the predictive power of the ML model is to examine the distribution of the target variable. For example, if your training data set is imbalanced with 99% data belonging to class 1 and the remaining 1% to class 0 and the predictions reflect this distribution to be around 90%-10%, then it should be treated as an alert for further investigation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc0b47f1-915a-4eed-895d-ed4bc5c27f18",
   "metadata": {},
   "source": [
    "### 2.4 Correlation\n",
    "\n",
    "Additionally, monitoring pairwise correlations between variables will help bring out an underlying drift. Variables which were previously correlated may start to diverge and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b67c3f5-bf9a-452e-93fb-339180c469a1",
   "metadata": {},
   "source": [
    "## 3. Retraining strategy\n",
    "\n",
    "There are three approaches to handling model retraining.\n",
    "\n",
    "### 3.1 Retraining at a fixed periodic intervals\n",
    "If the incoming data is changing frequently, the model retraining can happen even daily!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829f803-c459-4253-92de-5feddf7b622b",
   "metadata": {},
   "source": [
    "### 3.2 Retrained based on monitoring results\n",
    "#### 3.2.1 Trigger based on performance metrics\n",
    "The model is retrained when the monitored metric crosses a threshold. This approach is more effective than the one above but the threshold specifying the acceptable level of performance divergence needs to be decided beforehand. The following factors need to be considered while choosing the threshold:\n",
    "- Too low a threshold will lead to frequent retraining which will lead to increased overhead in terms of computing cost.\n",
    "- Too high a threshold may lead to “strayed predictions”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d60b040-58bc-4f90-9678-c6e932ae53bc",
   "metadata": {},
   "source": [
    "<img src=\"media/retraining_model_graph.png\" alt=\"histogram\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c33ba-0a18-4ac5-8e1f-349a1018edf2",
   "metadata": {},
   "source": [
    "#### 3.2.2 Trigger based on data changes\n",
    "By monitoring the incoming data during production, you can identify changes in the distribution of your data. This can indicate that your model is outdated or that you’re in a dynamic environment. This is a good approach in situations where don't get immediate feedback on your predictions, so you can't compare them to the ground truth.\n",
    "<img src=\"media/trigger_data_changes.png\" alt=\"histogram\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e9e479-b041-4735-801a-3bc98e374e74",
   "metadata": {},
   "source": [
    "### 3.3 Retraining on demand\n",
    "Of all the options, this is the least efficient as it does not rely on automation, but it's the most simple to implement and therefore sometimes favoured."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5247e441-4586-49d1-8ae5-a10e61af38f2",
   "metadata": {},
   "source": [
    "## 4. How much data is needed for retraining?\n",
    "\n",
    "In addition to knowing why and when you need to retrain your models, it’s also important to know how to select the right data for retraining and whether or not to drop the old data. \n",
    "\n",
    "Three things to consider when choosing the data for retraining:\n",
    "- What is the size of your data?\n",
    "- Is your data drifting?\n",
    "- How often do you get new data?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ed06975-134a-4610-8e2a-339905e2d9a4",
   "metadata": {},
   "source": [
    "### 4.1 Fixed window size\n",
    "\n",
    "This is a straightforward approach to selecting the training data. Selecting the right window size is a major drawback to using this approach.\n",
    "\n",
    "- If the window size is too large, we may introduce noise into the data. \n",
    "- If it’s too narrow, it might lead to underfitting.\n",
    "\n",
    "Overall, this approach is a simple heuristic approach that will work well in some cases, but will fail in a dynamic environment where data is constantly changing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8149818c-5739-48ff-ad26-aa67c92e37a7",
   "metadata": {},
   "source": [
    "### 4.2 Dynamic window size\n",
    "\n",
    "This is an alternative to the fixed window size approach. This approach helps to determine how much historical data should be used to retrain your model by measuring a metric on a changing window size. It’s an approach to consider if your data is large and you also get new data frequently. \n",
    "\n",
    "<img src=\"https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Training-data-vs-test-data.png?resize=900%2C420&ssl=1\" alt=\"histogram\" width=\"700\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8bcb06dc-e3ef-4e71-8309-93654a04f0a7",
   "metadata": {},
   "source": [
    "### 4.3 Combining all of the data\n",
    "\n",
    "The simplest way to handle this problem, resources permitting, is simply to combine all of the data and retrain your model. This approach is suitable if the data has not drifted too much. It may not be viable option in production due to increase in computational load as the data continues to grow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80618163-837a-47fa-9fa0-a95dfde4ddeb",
   "metadata": {},
   "source": [
    "## 5. Final considerations\n",
    "\n",
    "Before we move on to a more practical demonstration, I hope you're now aware that retraining and redeployment is a constant need for any ML model. The *when* and the *how* are key questions that rely on the sensitivity of not only the methods, but of the data scientists. For the *data scientists*, a critical evaluation of results is a fundamental skill. Now let's get our hands dirty in Part 2! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
