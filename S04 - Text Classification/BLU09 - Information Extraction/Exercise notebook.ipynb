{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a3dc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f881c7002cca9b48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# BLU09 - Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125f899",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4efa4dd0f5a58872",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import utils\n",
    "\n",
    "cpu_count = int(os.cpu_count()) if os.cpu_count() != None else 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9f847",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a882aedc21b3fc8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this exercise notebook you are going to tackle a very real problem: **Detecting fake news!** You'll create a classification workflow to determine if a piece of news is considered 'reliable' or 'unreliable'. You will start by building some basic features, then extract information from the text, go on to build more features, and finally put it all together.\n",
    "\n",
    "The data set we will be using is the [Fake News data set](https://www.kaggle.com/c/fake-news/overview) from Kaggle. Each piece of news is either reliable or trustworthy, '0', or unreliable and possibly fake, '1'. First, let's load the data and see what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c4d9f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-62fce116d684ff08",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"data/fakenews/train.csv\"\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"text\"] = df[\"text\"].astype(str)\n",
    "\n",
    "df = df[:5000]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814b45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38f5eb775841d955",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have 4 columns that are pretty self-explanatory. Let's drop the author column since we only want to practice our text analysis and drop the title as well for simplicity sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118e6f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e4555d789c2c7417",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"author\", \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ec050",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-043e168c017b27d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's also load SpaCy's module with the [merged entities](https://spacy.io/api/pipeline-functions#merge_entities) (which will come in handy later) and stopwords. We insert the merged entities module into the SpaCy pipeline after the NER module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a289d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb489232f7a726b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp.add_pipe(\"merge_entities\", after=\"ner\")\n",
    "en_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d931a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ed6699286cf9b2f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here we process the news data with SpaCy to use later on. This might take a while depending on your hardware (a break to walk the dog? üê∂)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313691e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88f6782b20750823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "docs = list(tqdm(nlp.pipe(df[\"text\"], batch_size=20, n_process=cpu_count-1), total=len(df[\"text\"])))\n",
    "docs[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f722c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-276cdb3161f052ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Overall, the text looks good! Not too many errors, well written... as expected from a news article. Fake news is a very tough, recent problem that is now appearing more and more frequently in the wild. Usually there aren't many ortographic mistakes or slang (as it may happen with spam) since it's coming from news sources that want to appear credible but also clickbaity so that they can profit on that good ad revenue and create distrust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c38a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f9871459d26c91a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - Pipeline\n",
    "\n",
    "Let's create a baseline classification workflow. We'll use the TfidfVectorizer to get a simple, fast and trustworthy baseline.\n",
    "\n",
    "Create a function that applies a pipeline to the given train data, makes a prediction for the test data, and returns the accuracy of the prediction. The pipeline should consist of a `TfidfVectorizer` and a `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ff301",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1cf7d1d751604527",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_rf_pipeline(X_train, X_test, y_train, y_test, seed=42):\n",
    "    \"\"\"\n",
    "    Trains a TfidfVectorizer + RandomForestClassifier pipeline on the given train data.\n",
    "    Makes a prediction on the test data.\n",
    "    Returns the trained pipeline and the accuracy of the prediction.\n",
    "\n",
    "    Parameters:\n",
    "        X_train, y_train: train data, pd.Series\n",
    "        X_test, y_test: test data, pd.Series\n",
    "        seed (int): random state seed for the classifier\n",
    "    \n",
    "    Returns:\n",
    "        pipe: fitted pipeline\n",
    "        acc (int): accuracy of the prediction\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac3eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-31eb2fa6359bdecb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For the baseline, we will preprocess the text - remove punctuaction and stopwords and tokenize it - then run it through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59999879",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c345c9379c620ae2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(utils.remove_punctuation)\n",
    "df_processed[\"text\"] = df_processed[\"text\"].apply(utils.remove_stopwords, stopwords = en_stopwords, \n",
    "                                                  tokenizer = WordPunctTokenizer())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_processed[\"text\"], df_processed[\"label\"], \n",
    "                                                    test_size=0.2, random_state=42, stratify=df_processed[\"label\"])\n",
    "baseline_model, baseline_acc = tfidf_rf_pipeline(X_train, X_test, y_train, y_test)\n",
    "\n",
    "assert isinstance(baseline_model, Pipeline)\n",
    "assert hashlib.sha256(json.dumps(str(baseline_model[0])).encode()).hexdigest() == \\\n",
    "'e68c8e581c16f0d62f3b9cb33a7967b17890e18c1fe819d013181e6714e7a303', \"The pipeline parameters are not correct.\"\n",
    "assert hashlib.sha256(json.dumps(str(baseline_model[1])).encode()).hexdigest() == \\\n",
    "'36a4f3295ffa4c170fc0addee2a8cac5613970f06e3dde6956fa31daf19aa329', \"The pipeline parameters are not correct.\"\n",
    "np.testing.assert_almost_equal(baseline_acc, 0.908, decimal=2, err_msg=\"The accuracy is not correct.\")\n",
    "print(f'Baseline accuracy: {baseline_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1ec49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ecc682a6b13cabf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Wow, the accuracy is quite good for such a simple text model! This just proves that a trustworthy baseline is all you need. I can't stress enough that it's really important to have a simple first iteration, and afterwards we can add complexity and study which features make sense or not. \n",
    "\n",
    "Sometimes, data scientists focus right off the bat on the most complex solutions and a simple one would be enough. Real life problems will obviously achieve lower scores as the data sets are not controlled or cleaned for you but that should not stop you from starting with a simpler and easier solution.\n",
    "\n",
    "Now let's see if we can engineer more features. We will extract information with SpaCy and see if we can use it to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc3f611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9acda58125c99fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 - SpaCy Matcher\n",
    "\n",
    "Let's see if we can extract some useful features with the SpaCy Matcher.\n",
    "\n",
    "### Exercise 2.1 - Simple matcher\n",
    "\n",
    "You think of some words that could be related with the detection of Fake News. Something starts ringing in your mind about \"propaganda\", \"USA\" and \"fraud\", so you decide to use the SpaCy Matcher to check how many of those words appear in the news articles.\n",
    "\n",
    "Use the `docs` list preprocessed by SpaCy and count the number of occurences of these words in all documents. Make sure to match the words regardless of the case. The output should be the sum of occurencies in all news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f158c1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca5f9a618bc32ee6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words = [\"propaganda\", \"usa\", \"fraud\"]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# count = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6002d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-57893459c2e5a1ac",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(json.dumps(str(count)).encode()).hexdigest() == \\\n",
    "'9d44059c29e077b9fd8496ebcc41c94aeb203bf1adce7729d3ecda30bc885a90', 'Not correct, try again.'\n",
    "print(f'Count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374a2f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9a8e7ce0cab87fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2 - POS-tagging search\n",
    "\n",
    "Ok, this doesn't look like the way to go, let's look at other theories. You start thinking that fake news might exaggerate on adjectives and adverbs by using over the top descriptions. So you decide to create a feature that counts the number of _Adjectives_ and _Adverbs_ in a piece of news article. The count should be normalized to the token count of the article.\n",
    "\n",
    "The result should be a list of adjective and adverb counts for each document normalized to the token count of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170ff4c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d048ae4dde4a43cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# nb_adj_adv = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618744a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3961c5c5cb9b18aa",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(nb_adj_adv,list), \"The result should be a list.\"\n",
    "assert len(nb_adj_adv) == len(docs), \"The length of the result list is wrong. You should have a count for every news article.\"\n",
    "np.testing.assert_almost_equal(np.var(nb_adj_adv), 0.00105, decimal=4, err_msg='The result is not correct.')\n",
    "np.testing.assert_almost_equal(np.sum(nb_adj_adv), 462.5, decimal=1, err_msg='The result is not correct.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4f20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fa92a0511c909523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's add this feature to our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa3004",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-820cb2d992a833a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_processed[\"nb_adj_adv\"] = nb_adj_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74452de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54363f2d61151f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3 - Adjectivized proper nouns\n",
    "\n",
    "Another theory that might be worth testing is that adjectives with proper nouns are often used in this kind of news to induce sentiments towards people or organizations. So you decide to extract proper nouns preceeded by adjectives to maybe use in a later analysis.\n",
    "\n",
    "Create a `Matcher` to search for adjective + proper noun combinations. Count the number of occurences of each combination. Store the 10 most common combinations and the number of their occurences as tuples in a list, sorted in descending order by the number of occurencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89fe123",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a5547e1de72b597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# most_common_adj_propn = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6de656a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a55276f7a5cb1f58",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(most_common_adj_propn,list), \"The output should be a list.\"\n",
    "assert len(most_common_adj_propn) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_adj_propn[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert hashlib.sha256(json.dumps(most_common_adj_propn).encode()).hexdigest() == \\\n",
    "'0b12899bfedce520180f460bfd6742c1241ac7270ee98d4dcb482284e134cde8', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79678441",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5de1f6e3a47a8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at the 10 most common combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b06dc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a03eaf795b36ce50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_adj_propn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d7e98e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7ba7a85acd3ab24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The counts are too low to use these terms as features. Maybe running a vectorizer on all the results could work better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f299d11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-13de41c333217acd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.4 - Objects of preposition\n",
    "The objects in the sentences could indicate something. For instance, 'NGO financed by Soros' is more likely to appear in fake news than 'NGO financed by UNESCO'. Both objects in these sentences are objects of preposition (hint: SpaCy has a dependency label for this).\n",
    "\n",
    "Create a `Matcher` to search for objects of preposition which are nouns. Again, count the number of occurences of each. Store the 10 most common combinations and their occurences as tuples in a list, sorted in descending order by the number of occurencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea459a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d047424aca69651a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# most_common_pobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938f090a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dd2eee9295599d16",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(most_common_pobj,list), \"The output should be a list.\"\n",
    "assert len(most_common_pobj) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_pobj[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert  hashlib.sha256(json.dumps(most_common_pobj).encode()).hexdigest() == \\\n",
    "'8b947c095d53dc4ccc4f28d1a448e60ef2f0c509eeac370cb0448ba9418d25c3', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f44647",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b9f1ddf5f7c7ec1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This time the counts are higher and might be more interesting for a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568accf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-53fa28fe45c63cfb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_pobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98e7c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ad6aa5c72ba1a3e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.5 - Verbs with direct objects\n",
    "As the last point, you decide to look at verbs with direct objects. These should indicate actions taken towards something or someone. This exercise can be solved without a Matcher.\n",
    "\n",
    "Search for verbs with direct objects which are not pronouns. This time it's a bit trickier - you need to look at the [parse tree](https://spacy.io/usage/linguistic-features#navigating) because the object does not necessarily come right after the verb. Lemmatize both the verb and the object and count the occurences of the lemmatized verb and direct object separated by a space, like this: 'verb_lemma dobj_lemma'. Don't forget to exclude objects that are pronouns.\n",
    "\n",
    "Again, output the 10 most common combinations and their occurences as tuples in a list, sorted in descending order by the number of occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d62ce",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9be3562d21acdf58",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# most_common_dobj = []\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8593b48",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-aa931f62fa91ab35",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(most_common_dobj,list), \"The output should be a list.\"\n",
    "assert len(most_common_dobj) == 10, \"It should be the top 10!\"\n",
    "assert isinstance(most_common_dobj[0],tuple), 'The elements of the list should be tuples of (combination, occurences).'\n",
    "assert hashlib.sha256(json.dumps(most_common_dobj).encode()).hexdigest() == \\\n",
    "'4aa91fbf85175e56f4a132fb253c40869c6f839fc2cfd4cee821ac1422217f29' or \\\n",
    "hashlib.sha256(json.dumps(most_common_dobj).encode()).hexdigest() == \\\n",
    "'7135e8ae4d25d6cf68db7f5083830236e38dae2843b24b6435342d7ded486e45', 'The top ten list is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85479cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-849a37fbcedc4049",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Not so many occurencies, but again the whole list could be used in a vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b81e2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-218910800564bd8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_common_dobj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8244da23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59afa572e05bc308",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 - Feature unions\n",
    "\n",
    "We're going to create a few more numerical features here, then use them in a feature union pipeline and see if the baseline improves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca16469",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-692f18d2b61996fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.1 - More features\n",
    "\n",
    "There are a few more simple features that we can extract from the data set to try to enrich our model. Let's add the following features to the `df_processed` dataframe:\n",
    "- number of words in the news article\n",
    "- character length of the news article\n",
    "- average word length\n",
    "- average sentence length.\n",
    "\n",
    "Use the SpaCy processed `Doc`s for calculating the average sentence length (note that you will obtain sentence length in tokens).\n",
    "\n",
    "Use the tokenized text in `df_processed` for everything else. Punctuation and stopwords were already removed from this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb93e53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6910256a13fa82fd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# df_processed[\"nb_words\"] = ...\n",
    "# df_processed[\"doc_length\"] = ...\n",
    "# df_processed[\"avg_word_length\"] = ...\n",
    "# df_processed[\"avg_sentence_length\"] = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff5b354",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-31106be84628e3c9",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert df_processed.shape == (5000, 7), \"Something wrong about the shape, do you have all columns/rows?\"\n",
    "assert \"nb_words\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"doc_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_word_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "assert \"avg_sentence_length\" in df_processed, \"Missing column! Maybe wrong name?\"\n",
    "\n",
    "assert np.sum(df_processed[\"nb_words\"]) == 1963935, \"Something is wrong with the nb_words column.\"\n",
    "assert np.sum(df_processed[\"doc_length\"]) == 14636737, \"Something is wrong with the doc_length column.\"\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_word_length\"]), 32100.0, decimal=1, \n",
    "                               err_msg='Something is wrong with the avg_word_length column.')\n",
    "np.testing.assert_almost_equal(np.sum(df_processed[\"avg_sentence_length\"]), 118628.9, \n",
    "                               decimal=1, err_msg='Something is wrong with the avg_sentence_length column.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1e3de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b12d83fd4d036457",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.2 - Define a feature union for preprocessing\n",
    "\n",
    "Let's create a processing pipeline for every feature in `df_processed` and join them all in a feature union. The pipeline for textual features should have one step, a `TfidfVectorizer` with default parameters. The pipeline for numerical features should have one step, a `Standard Scaler`. Afterwards, join the features' pipelines in a feature union.\n",
    "\n",
    "Use the `Selector` classes in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935c549",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-61e270fd37131ec7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Selector(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Transformer to select a column from a dataframe \n",
    "    on which to perform additional transformations.\n",
    "    \"\"\" \n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single text column from the dataframe\n",
    "    on which to perform additional transformations.\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "    \n",
    "class NumberSelector(Selector):\n",
    "    \"\"\"\n",
    "    Transformer to select a single numerical column from the dataframe\n",
    "    on which to perform additional transformations.\n",
    "    \"\"\"\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726e0bf1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a5c8a9abba2b5e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# text_pipe = ...\n",
    "# nb_adj_adv_pipe = ...\n",
    "# nb_words_pipe = ...\n",
    "# doc_length_pipe = ...\n",
    "# avg_word_length_pipe = ...\n",
    "# avg_sentence_length_pipe = ...\n",
    "# feats = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402bc439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-98151e411b73b1f5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(feats, FeatureUnion)\n",
    "assert len(feats.transformer_list) == 6, \"Did you create a pipeline for each feature?\"\n",
    "for pipe in feats.transformer_list:\n",
    "    \n",
    "    selector = pipe[1][0]\n",
    "    if not (isinstance(selector, TextSelector) or isinstance(selector, NumberSelector)):\n",
    "        raise AssertionError(\"The first step of the pipeline is not correct.\")\n",
    "        \n",
    "    feature_builder = pipe[1][1]\n",
    "    if not (isinstance(feature_builder, TfidfVectorizer) or isinstance(feature_builder, StandardScaler)):\n",
    "        raise AssertionError(\"The second step fo the pipeline is not correct.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb7c8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8c92614d69d640a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 3.3 Fit the feature union\n",
    "Define a function with pipeline that will apply the preprocessing steps from the previous exercise and fit a classifier to the provided data. The pipeline should have two steps, the feature union from the previous exercise and a `RandomForestClassifier`.\n",
    "The function should fit the pipeline to the train data, make a prediction on the test data and calculate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431c958",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fb15cc415e6737d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def improved_pipeline(feats, X_train, X_test, y_train, y_test, seed=42):\n",
    "    \"\"\"\n",
    "    Creates a pipeline with the provided feature union and a Random Forest classifier.\n",
    "    Fits the pipeline to the train data and makes a prediction with the test data.\n",
    "    Outputs the fitted pipeline and the accuracy of the prediction.\n",
    "\n",
    "    Parameters:\n",
    "        feats: feature union\n",
    "        X_train, y_train: train data\n",
    "        X_test, y_test: test data\n",
    "        seed (int): seed for random state in the classifier\n",
    "\n",
    "    Returns:\n",
    "        pipe: fitted pipeline\n",
    "        acc (int): accuracy of the prediction for the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return pipe, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04da0430",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-873e3b1b663a7b12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Y = df_processed[\"label\"]\n",
    "X = df_processed.drop(columns=\"label\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "pipeline_model, pipeline_acc = improved_pipeline(feats, X_train, X_test, y_train, y_test)\n",
    "\n",
    "assert isinstance(pipeline_model, Pipeline)\n",
    "assert isinstance(pipeline_model[0],FeatureUnion), \"The first step of the pipeline is not correct.\"\n",
    "assert isinstance(pipeline_model[1],RandomForestClassifier),  \"The second step of the pipeline is not correct.\"\n",
    "np.testing.assert_almost_equal(pipeline_acc, 0.908, decimal=3, err_msg=\"The accuracy score is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e0258",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec9f67cb4f0f5b43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this more complex approach we have achieved basically the same performance as our baseline. This might mean a lot of things: our features might have no real relevance to the model (which you can check with feature importances) or we have achieved a plateau and can't improve the score with this technique. \n",
    "\n",
    "Nevertheless it is a good score for this problem and data set. Regardless of the score, you have learnt a lot about SpaCy, feature unions and also that the sky is the limit when creating features. Anything can be a feature really - now good features are a totally different thing that might need more research and validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
