{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79f2337e7779945a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SLU07 - Regression With Linear Regression: Exercise notebook\n",
    "\n",
    "In the learning notebook we have presented you with several new concepts. \n",
    "\n",
    "With this exercise notebook we want to take a closer look at some of the formulas we introduced in the learning notebook and help you understand the concepts from a more practical experience.\n",
    "\n",
    "Let's dive right into it and enjoy the ride!\n",
    "\n",
    "![ride](media/ride.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4cb0eb9c3a32286f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hashlib\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=(4.8, 3.6)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de563f7c23e748a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - Simple Linear Regression\n",
    "\n",
    "As a first exercise, let's imagine you're thinking about changing your career and now you want to become a full-time blogger (I know that in these days, bloggers are not a thing but let's assume it is).\n",
    "\n",
    "![blog](media/blog.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-74ff3f984bb74106",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Although being a full-time blogger seems to be a lot of fun, you need to have a sense of how much money you will make with the blog so you can manage your financial life. \n",
    "\n",
    "After doing some online research, you find income reports of full-time bloggers and you put that information into a spreadsheet. This spreadsheet has information about the number of months of blogging experience and the corresponding amount of money earned.\n",
    "\n",
    "You will use this dataset to predict approximately how much money can be earned based on the number of months of blogging experience.\n",
    "\n",
    "You probably already have a hunch that, the older a blog is, the more money it makes, but by using linear regression youâ€™ll be able to support or refute this hypothesis with actual data. You know, the kind of data you can take to the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6c8fc43cd0410a8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_blog = pd.read_csv('data/blogging_income.csv')\n",
    "df_blog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32f343233539ec16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by visualizing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5687e6f07d6725ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_blog = df_blog.sort_values('MonthsExperience')\n",
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 1800))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.title('Income based on Months of Experience')\n",
    "plt.ylabel('Income')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-67395307d5b7356e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.1 - Simple Linear Model\n",
    "\n",
    "As you can see, our data has only one variable ($x$: 'Months of Experience') and one label ($y$: 'Income'), so we can try to fit it with a simple linear regression. This model is represented by the following expression:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "where $\\hat{y}$ are the predictions, $\\beta_0$ is the intercept, $\\beta_1$ is the coefficient and $x$ is the input sample. Expanding to several samples, we can write this equation in a vector form:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + \\beta_1 \\vec{x}$$\n",
    "\n",
    "In this exercise, we would like you to implement the function <em>simple_linear_regression</em>, which should compute the output of the equation above:\n",
    "\n",
    "Ensure that the f1_1, f2_1, and y_hat1 in the first assert are floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0658efc6a6964c6e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_linear_regression(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples,) - The input data\n",
    "        betas: numpy.array with shape (2,) - parameters of the model [b_0, b_1]\n",
    "    \n",
    "    Returns:\n",
    "        f1, f2 : numpy.array with shape (num_samples,) - intermediate calculations\n",
    "        y_hat : numpy.array with shape (num_samples,) - The prediction made by \n",
    "                the simple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Let's begin with the first term of the equation\n",
    "    # f1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, the second term\n",
    "    # f2 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now let's sum the two terms of the equation\n",
    "    # y_hat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return f1, f2, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-651ae1ad52511300",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "f1_1, f2_1, y_hat1 = simple_linear_regression(np.arange(0, 5), np.array([-15, 20]))\n",
    "assert isinstance(f1_1,np.ndarray), 'f1_1 should be a numpy array'\n",
    "assert isinstance(f2_1,np.ndarray), 'f2_1 should be a numpy array'\n",
    "assert isinstance(y_hat1,np.ndarray), 'y_hat1 should be a numpy array'\n",
    "assert f1_1.shape==(5,), 'The shape of f1_1 is not correct.'\n",
    "assert f2_1.shape==(5,), 'The shape of f2_1 is not correct.'\n",
    "assert y_hat1.shape==(5,), 'The shape of y_hat1 is not correct.'\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in f1_1]).encode('utf-8')).hexdigest() == \\\n",
    "'cea1134cb91f64b3115314832605f52301489493e50b9d68b26a127e269b0f9f', \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in f2_1]).encode('utf-8')).hexdigest() == \\\n",
    "'7b5c77635d12c0effb79b027441923a93a3cceecd4311110493e42a7146fb3cb', \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in y_hat1]).encode('utf-8')).hexdigest() == \\\n",
    "'1f9889fe39dad0bec2c25537846a49bd88d1943ad9c79e760fd6552029c5cfb4', \"Perhaps your y_hat variable is not well calculated!\"\n",
    "\n",
    "# Test using our dataset\n",
    "f1_2, f2_2, y_hat2 = simple_linear_regression(df_blog['MonthsExperience'].to_numpy(),np.array([3.0, 2.5]))\n",
    "assert isinstance(f1_2,np.ndarray), 'f1_2 should be a numpy array'\n",
    "assert isinstance(f2_2,np.ndarray), 'f2_2 should be a numpy array'\n",
    "assert isinstance(y_hat2,np.ndarray), 'y_hat2 should be a numpy array'\n",
    "assert f1_2.shape==(30,), 'The shape of f1_2 is not correct.'\n",
    "assert f2_2.shape==(30,), 'The shape of f2_2 is not correct.'\n",
    "assert y_hat2.shape==(30,), 'The shape of y_hat2 is not correct.'\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in f1_2]).encode('utf-8')).hexdigest() == \\\n",
    "'813da1018ad40a91070c69f05e642b36345d4cae740bd47b03ca12d1ba96a586', \"Perhaps your f1 variable is not well calculated!\"\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in f2_2]).encode('utf-8')).hexdigest() == \\\n",
    "'5abc33114cbddc5f935f11fe481597f2983f3ad2cb3e1a24891d4859c7c77f06', \"Perhaps your f2 variable is not well calculated!\"\n",
    "assert hashlib.sha256(''.join([str(float(i)) for i in y_hat2]).encode('utf-8')).hexdigest() == \\\n",
    "'0546bb6076f6a41d155e8a69a247effa6f16ed1201b3a630d9458faa559f5d08', \"Perhaps your y_hat variable is not well calculated!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b2f4bcb7e195509c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now that you have a function to construct the model, the next step is to discover the values of the betas.\n",
    "\n",
    "The first approach you can take is to implement the closed form solution. That is, solving the equation that minimizes the error across all of the samples - ordinary least squares. For that, however, we need to implement a function calculating the error. We'll use the cost function that you have learned about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc935eebba6de6f9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.2 Cost Function\n",
    "\n",
    "Implement the mean-squared-error cost function\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N e_i^2 = \\frac{1}{N} \\sum_{n=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where the error, $e_i$, is the difference between your prediction and the actual sample value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1049431fa099d4b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        y : numpy.array with shape (num_samples, ) - real values\n",
    "        y_hat : numpy.array  with shape (num_samples, ) - predicted values\n",
    "    \n",
    "    Returns:\n",
    "        mean_squared_error : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the error\n",
    "    # error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now, square the difference\n",
    "    # squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Finally, take the mean and return the mean sqared error\n",
    "    # mean_squared_error = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4510d1dc5df769f3",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "np.testing.assert_almost_equal(cost_function(np.array([.24]), np.array([.36])), 0.0144)\n",
    "np.testing.assert_almost_equal(cost_function(np.array([1.13]), np.array([2.65])), 2.3104)\n",
    "\n",
    "# Test using our dataset\n",
    "x_rnd = df_blog['MonthsExperience'].to_numpy()\n",
    "y_rnd = df_blog['Income'].to_numpy()\n",
    "beta_rnd = np.array([120, 120])\n",
    "_, _,y_hat_rnd = simple_linear_regression(x_rnd, beta_rnd)\n",
    "\n",
    "np.testing.assert_almost_equal(cost_function(y_rnd, y_hat_rnd), 294785.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab4aadf231356588",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see from the previous test, picking just random values for the betas will probably yield very high error values. You can even visualize this to see that, in fact, these random betas don't fit our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3963d719eccbc48",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 2500))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.ylabel('Income')\n",
    "plt.title('Prediction with random betas')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.', label='True')\n",
    "plt.plot(df_blog['MonthsExperience'], y_hat_rnd, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b7d6627f93ad176",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.3 - Closed Form Solution\n",
    "\n",
    "Now we're ready to implement the closed form solution in order to get the optimal values for the betas. Remember that the solution to minimize the error can be written as\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\sum_{i}^{N}{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sum_{i}^{N}{(x_i - \\bar{x})^2}} = \\frac{cov(x, y)}{var(x)}$$\n",
    "\n",
    "where $cov(x,y)$ and $var(x)$ are, respectively, the covariance and variance of the samples, and\n",
    "\n",
    "$$ \\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$ \n",
    "\n",
    "where $\\bar{y} = \\frac{1}{N}\\sum_{i}^{N}{y_i}$ and $\\bar{x} = \\frac{1}{N}\\sum_{i}^{N}{x_i}$ are the means of the sample.\n",
    "\n",
    "Complete the function for the closed form solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c6179999833a9a8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, ) - values of the independent variable\n",
    "        y : numpy.array with shape (num_samples, ) - real values of the dependent variable\n",
    "    \n",
    "    Returns:\n",
    "        b0: float\n",
    "        b1: float\n",
    "    \"\"\"\n",
    "    # The sample covariance and variance for 1-d arrays in \n",
    "    # numpy for this particular case are computed as follows\n",
    "    # We covered this part so you don't lose too much time on these details\n",
    "    cov_xy = np.cov(x, y, bias=True)[0][1]\n",
    "    var_x = np.var(x)\n",
    "    \n",
    "    # Compute coefficient beta_1\n",
    "    # b1 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute intersect beta_0\n",
    "    # b0 = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return b0, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ed0cf5f5ace73f0a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    closed_form_solution(np.arange(0, 10), np.arange(0, 20, 2)),\n",
    "    (0.0, 2.0)\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    closed_form_solution(np.arange(-2, 3), np.array([-1.5, -.56, .26, 1.3, 2.5])),\n",
    "    np.array([.4, .986])\n",
    ")\n",
    "\n",
    "#Test using our dataset\n",
    "x_cf = df_blog['MonthsExperience'].to_numpy()\n",
    "y_cf = df_blog['Income'].to_numpy()\n",
    "beta_cf = closed_form_solution(x_cf, y_cf)\n",
    "y_hat_cf = simple_linear_regression(x_cf, beta_cf)[2]\n",
    "\n",
    "np.testing.assert_array_almost_equal(cost_function(y_cf, y_hat_cf), 10170.468711301624)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f64b1ed948d0959c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can also visualize how well the solution fits the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c294660043fd604",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((2, 20))\n",
    "plt.ylim((200, 2000))\n",
    "plt.xlabel('MonthsExperience')\n",
    "plt.ylabel('Income')\n",
    "plt.title('''Prediction using the closed form solution\n",
    "          with the calculated parameters beta_cf''')\n",
    "plt.plot(df_blog['MonthsExperience'], df_blog['Income'], 'b.', label='True')\n",
    "plt.plot(df_blog['MonthsExperience'], y_hat_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-87eed9f8fb8bc294",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 1.4 - Final Answer\n",
    "\n",
    "Letâ€™s say you decide that youâ€™re willing to give blogging 6 months of your time. At that point, if youâ€™re not making any money, you call it quits. Use the simple linear model implemented in the previous exercise to calculate the prediction for 6 months of blogging.\n",
    "\n",
    "We have already calculated the parameters in the asserts to the previous exercise and stored them in the variable `beta_cf`, but let's do it again just to practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79b1288c851440b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the input variables for the closed_form_solution function\n",
    "x_simple = df_blog['MonthsExperience'].to_numpy()\n",
    "y_simple = df_blog['Income'].to_numpy()\n",
    "\n",
    "# Calculate the betas again\n",
    "beta_simple = closed_form_solution(x_simple, y_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-63a7d5b792053937",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Now, use the calculated betas and the simple linear regression function from above \n",
    "# to calculate the predicted income at 6 months of blogging.\n",
    "# Mind the type of the input parameters!\n",
    "# y_hat_simple = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('After 6 months in blogging, you will make: {} â‚¬'.format(round(y_hat_simple[0], 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0e90b401208965c6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(y_hat_simple, np.ndarray), 'The output should be a numpy array'\n",
    "# Check the error between the prediction and the real values at 6 months\n",
    "np.testing.assert_array_almost_equal(cost_function(df_blog[df_blog.MonthsExperience==6].Income.mean(), y_hat_simple), 36.78,\n",
    "                                     decimal=2,err_msg='The calculated value is not correct.') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28ca0978a921d6b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![mission](media/rich.gif)\n",
    "\n",
    "So, maybe blogging isn't a very profitable job in a short/medium-term, at least...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-791ecdf680235b04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2. Multiple Linear Regression\n",
    "\n",
    "In this section, we will upgrade  to a linear regression with multiple features. We will use a very specific scenario, so that we are able to visualize it better - we will try to model a polynomial function, in particular a cubic function, which can be written as:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x + \\beta_2 x^{2} + \\beta_3 x^{3}$$\n",
    "\n",
    "You will basically be considering each power of x as a different feature. To simplify, we are going to construct a dataset with the powers we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-349d6ec88f5f307d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x = 2 - 3 * np.random.normal(0, 1, 20)\n",
    "y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)\n",
    "\n",
    "# transform the data to include another axis\n",
    "x = x[:, np.newaxis]\n",
    "y = y[:, np.newaxis]\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=3)\n",
    "x_poly = polynomial_features.fit_transform(x)\n",
    "\n",
    "data_ml = pd.DataFrame(x_poly, columns=['x1', 'x2', 'x3', 'x4']).drop(['x1'], axis=1)\n",
    "data_ml.columns = ['x0', 'x1', 'x2']\n",
    "data_ml['y'] = pd.DataFrame(y)[0]\n",
    "data_ml.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81b488f24ce2f5d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's plot our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e2ef3862e8aae7f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_ml = data_ml.sort_values('x0')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Relation between y and and x0')\n",
    "plt.plot(data_ml['x0'], data_ml['y'], 'b.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c9f9d3c78f620b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Although this model is non-linear in its features, notice that it is linear with respect to the parameters, and the equation above can be rewritten as\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 x_0 + \\beta_2 x_1 + \\beta_3 x_2$$\n",
    "\n",
    "where $[x_0, x_1, x_2]$ is our feature vector for a given sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cf3b682e1ef16ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.1  Linear Model Extended\n",
    "\n",
    "The multiple linear regression problem is just the linear regression problem with more than one feature. Let's recall the expression:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^K \\beta_i x_i$$\n",
    "\n",
    "We can also write it in matrix form to consider several samples, as before:\n",
    "\n",
    "$$\\vec{\\hat{y}} = \\beta_0\\vec{1} + X' \\vec{\\beta'_{1-k}}$$\n",
    "\n",
    "where $X'$ is now a matrix containing all k features for all n samples: \n",
    "\n",
    "$$ X' = \\begin{bmatrix} \n",
    "x_1^1 & x_2^1 & ... & x_k^1 \\\\\n",
    "x_1^2 & x_2^2 & ... & x_k^2 \\\\\n",
    "... & ... & ... & ...\\\\\n",
    "x_1^n & x_2^n & ... & x_k^n \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d79fedc69b50dbe6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As before in the learning notebook, we can include also the intercept into the samples matrix and the parameter vector for a neater look and easier manipulation:\n",
    "\n",
    "$$ X = [\\vec{1} | X'] $$\n",
    "\n",
    "and rewrite as\n",
    "\n",
    "$$\\vec{\\hat{y}} = X \\vec{\\beta}$$\n",
    "\n",
    "where $\\vec{\\beta}$ is now the full parameter vector including the intercept.\n",
    "\n",
    "Implement this linear model for multiple features in the function below. Remember that the matrix dimensions must match in matrix concatenation and multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10eb3e761476d6d9",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_linear_model(x, betas):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - features of the samples\n",
    "        betas : numpy.array with shape (num_features + 1,) - parameters of \n",
    "                our model, with the intercept in the first position of \n",
    "                the array\n",
    "    \n",
    "    Returns:\n",
    "        y_pred : numpy.array with shape (num_samples, ) - prediction \n",
    "                made by the multiple linear regression.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We do the proper reshaping of the parameter vector into a column vector\n",
    "    # so you don't have to worry about that and focus on the remaining logic\n",
    "    betas = betas.reshape((-1, 1))\n",
    "    \n",
    "    # Extend the features matrix with a column of ones\n",
    "    # X_mat = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Compute the output of the linear model\n",
    "    # y_pred = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Now we again reshape the output for you\n",
    "    return y_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d853a799cc5992d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple test\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(0, 10).reshape(-1, 1), np.array([-12, 30])), \n",
    "    [-12.,  18.,  48.,  78., 108., 138., 168., 198., 228., 258.])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-5, 5).reshape(-1, 1), np.array([1, 1])), \n",
    "    [-4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.arange(-10, 10, 2).reshape(-1, 1), np.array([0.25, 2.1])), \n",
    "    [-20.75, -16.55, -12.35, -8.15, -3.95, 0.25, 4.45, 8.65, 12.85, 17.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8c9cab5fa1ebfc99",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test with multiple features\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.array([[1., 2.], [3., 4.], [5., 6.]]), np.array([-1., 0., 1.])), \n",
    "    [1., 3., 5.])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(np.ones((10, 2)), np.array([1., 2., 3.])), \n",
    "    [6., 6., 6., 6., 6., 6., 6., 6., 6., 6.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ae733317d24d320d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Mean Squared Error\n",
    "\n",
    "As before, we can use the mean squared error to calculate the cost function. However, since this function does not receive anything other than the predictions and true values, there is no need to reimplement it. As before, let's see how large is the error if we pick random parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ebb1e0761f537021",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Error in our dataset with random parameter values\n",
    "x_pln_rnd = data_ml['x0']\n",
    "X_pln_rnd = data_ml.drop('y', axis=1).to_numpy()\n",
    "y_pln_rnd = data_ml['y'].to_numpy()\n",
    "# Create random parameter values\n",
    "beta_pln_rnd = np.array([1., 1., -.5, 0.])\n",
    "y_hat_pln_rnd = extended_linear_model(X_pln_rnd, beta_pln_rnd)\n",
    "\n",
    "# Plot the data and the model output\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('Multiple Linear Regression')\n",
    "plt.plot(x_pln_rnd, y_pln_rnd, 'b.', label='true')\n",
    "plt.plot(x_pln_rnd, y_hat_pln_rnd, 'r-', label='pred')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Error: {}'.format(cost_function(y_pln_rnd, y_hat_pln_rnd))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e3366d377d3c9b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, the solution is clearly not a fit and the error is very high. So let's move into our closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1ff21e3c52fafce2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 2.2 - Closed Form Solution\n",
    "\n",
    "Let's now implement the closed form solution for the multiple linear regression. Remember that the solution in matrix form looks like this:\n",
    "\n",
    "$$ \\vec{\\beta} = (X^TX)^{-1}(X^T\\vec{y})$$\n",
    "\n",
    "where X is the matrix of samples extended by a column of 1s, $X = [\\vec{1} | X] $ , $\\vec{y}$ is the vector of true sample values and $\\vec{\\beta}$ the vector of parameters $\\beta_0$ - $\\beta_k$\n",
    "\n",
    "Implement the closed form solution for the multiple linear regression problem below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e87f47eb6ec320ee",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def extended_closed_form_solution(x, y):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        x : numpy.array with shape (num_samples, num_features) - features of the samples\n",
    "        y : numpy.array with shape (num_samples, ) - true values of the samples\n",
    "    \n",
    "    Returns:\n",
    "        betas : numpy.array with shape (num_features + 1, ) - parameter vector \n",
    "    \"\"\" \n",
    "    \n",
    "    # Proper reshaping of the sample values\n",
    "    y = y.reshape((-1, 1))\n",
    "\n",
    "    # Extend the features matrix with a column of ones\n",
    "    # X_extended = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Compute betas\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Once again, we reshape your array to get the proper output\n",
    "    return betas.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-47587567e05c2602",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Simple tests\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(0, 10).reshape(10, 1), np.arange(0, 20, 2).reshape(10, 1)),\n",
    "    [0., 2.])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.arange(-2, 3).reshape(5, 1), np.array([-1.25, -.5, .25, 1., 1.75]).reshape(5, 1)),\n",
    "    [.25, .75])\n",
    "\n",
    "# Extended test cases\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[1., -1.], [2., 1.], [3., -5.]]), np.array([0., 1., 0.])), \n",
    "    [-0.25, 0.5, 0.25])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_closed_form_solution(np.array([[10., -2.], [-4., 5.], [-7., -8.]]), np.array([2., 1., -.5])), \n",
    "    np.array([1.019704, 0.115764, 0.08867]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bbdc5cd776150269",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now apply the closed form solution to our dataset to get the best parameters and measure the error across the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fd38814320080448",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_pln_cf = data_ml['x0'].to_numpy()\n",
    "y_pln_cf = data_ml['y'].to_numpy()\n",
    "\n",
    "# All columns except y\n",
    "X_pln_cf = data_ml.drop('y', axis=1).to_numpy()\n",
    "\n",
    "beta_pln_cf = extended_closed_form_solution(X_pln_cf, y_pln_cf)\n",
    "y_hat_pln_cf = extended_linear_model(X_pln_cf, np.array(beta_pln_cf))\n",
    "\n",
    "assert math.isclose(cost_function(y_pln_cf, y_hat_pln_cf), 7.159451499759534), \"Check your closed form function!\"\n",
    "print('The betas for our dataset:')\n",
    "print(beta_pln_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d204ff04dacb5dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, we plot the solution to see how it fits the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-33d9aab23eb2dadd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-100, 40))\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('y')\n",
    "plt.title('''Prediction with parameters calculated \n",
    "   with the closed form solution for multiple linear regression''')\n",
    "plt.plot(x_pln_cf, y_pln_cf, 'b.', label='True')\n",
    "plt.plot(x_pln_cf, y_hat_pln_cf, 'r-', label='Pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3ecb84f49c6e98b2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Much better! It's a much better fit.\n",
    "\n",
    "![reaction](media/reaction.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4bbf62d226129f9a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 3 . ScikitLearn Linear Regression\n",
    "\n",
    "Luckily, ScikitLearn already provides us with a solver for the Linear Regression problem, which implements a closed form solution internally. It also provides some extra error measures such as the $R^2$ score. \n",
    " \n",
    "For this exercise, we'll be using the [California housing dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) and try to model the house pricing based on the provided features. Start by loading and looking into the dataset. The dataset is scaled so that we can meaningfully compare the feature importance and also use it for gradient descent later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e7adbac1b3fefce4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/california_scaled.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8cc95feb70c42b3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# All features except house price\n",
    "columns_housing = data.drop(['Price100k','Latitude','Longitude'], axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "\n",
    "# House price \n",
    "y_housing = data['Price100k'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbd3a2e4e2107803",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Each of the columns in the table is a feature that we're going to input into our model. Implement the function below to fit the provided data with the `sklearn.linear_model.LinearRegression` module that you've learned about. The function should return the fitted coefficients and the intercept, and the $R^2$ score. Check the reference guide if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4896df37a4f5670c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sklearn_model_regression(x, y):\n",
    "    \"\"\"\n",
    "    Fits the sklearn linear regression model to the provided data and \n",
    "    returns the fitted parameters and the R2 score.\n",
    "    \n",
    "    Args: \n",
    "        x: numpy.ndarray with shape (num_samples, num_features) - features of the samples\n",
    "        y: numpy.array with shape (num_samples, ) - true values of the samples\n",
    "        \n",
    "    Return:\n",
    "        coefs: numpy.array (num_features,) - coefficients vector\n",
    "        intercept: float - intercept value\n",
    "        score: float - R squared score of regression\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the linear regressor\n",
    "    # lr = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the coefficients\n",
    "    # coefs = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Extract the intercept\n",
    "    # intercept = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Calculate the R2 score\n",
    "    # score = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return coefs, intercept, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a710d3ea831ce0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Let's see what are our coefficients for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-85395c9b2bedc497",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coefs_housing, intercept_housing, score_housing = sklearn_model_regression(x_housing, y_housing)\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.Series(coefs_housing, columns_housing.columns))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept: {}'.format(intercept_housing))\n",
    "print('\\n')\n",
    "\n",
    "print('RÂ² score: {}'.format(score_housing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1aca587618ebd00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Finally, check that your solution is an approximate of the true solution for the following tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-dfb0a7be54a211de",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "betas_housing = np.concatenate((np.array([intercept_housing]), np.array(coefs_housing)), axis=0)\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[:10], betas_housing),\n",
    "    np.array([-1.22557268,  0.11585174,  3.73106594,  0.72709071,  0.6652216 ,\n",
    "       -0.30509711, -0.07626561, -0.27029226,  0.78532166, -1.30559169]),\n",
    "    decimal=2, err_msg='The calculated betas are not correct.')\n",
    "\n",
    "np.testing.assert_array_almost_equal(\n",
    "    extended_linear_model(x_housing[-10:], betas_housing),\n",
    "    np.array([ 0.36492159, -0.31943796, -0.27246927, -0.13630818,  2.38036882,\n",
    "        0.27150915, -0.67166059, -0.46275535, -0.66935684,  0.21438923]),\n",
    "    decimal=2, err_msg='The calculated betas are not correct.')\n",
    "\n",
    "y_hat_housing = extended_linear_model(x_housing, betas_housing)\n",
    "np.testing.assert_allclose(cost_function(y_housing, y_hat_housing), 0.4003, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f8b2e2340ecbae20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Last one before you go home!\n",
    "\n",
    "![tired](media/tired.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d547f4157ab02c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4. Gradient Descent\n",
    "\n",
    "Now we will see how to get to a similar solution through learning/optimization methods. In this section, you will implement gradient descent. This method is an iterative process that finds the best betas in stepwise process. In each step, it updates the parameters in the direction that minimizes the error given by the cost function. For this, it makes use of derivatives according to the following formula:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\Delta_\\vec{\\beta}$ is the vector of the derivatives - also called gradients - of the cost function with respect to the parameters, $\\beta_{i+1}$ is the updated parameter and $\\beta_{i}$ the current parameter. Therefore, we need to be able to compute these gradients to be able to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e54fd3a31f51054",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Multiple Linear Regression partial derivatives\n",
    "\n",
    "The vector $\\Delta_\\vec{\\beta}$ in the formula above is just a vector with the partial derivatives of the cost function with respect to each of the parameters. The formulas for these partial derivatives with respect to each parameter are defined as follows:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_0} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n) $$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_1} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{1_n} $$\n",
    "\n",
    "$$...$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial b_K} = - \\frac{1}{N} \\sum_{n=1}^N 2(y_n - \\hat{y}_n)x_{K_n} $$\n",
    "\n",
    "Since the focus of this notebook is for you to implement the methods to solve linear regression, and you already have quite some work, we'll solve this one for you. Check below the code for the derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-315f5fa32e6031cb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_partial_derivatives(x, y, y_hat):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - features of the samples\n",
    "        y : numpy.array with shape (num_samples,) - true values of the samples\n",
    "        y_hat : numpy.array with shape (num_samples,) - predicted values of the samples\n",
    "    \n",
    "    Returns:\n",
    "        deltas : numpy.array with shape (num_features + 1,)\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    # Compute the difference between the targets and the predictions.\n",
    "    y_diff = y - y_hat\n",
    "    \n",
    "    # Initialize the numpy array of partial derivatives\n",
    "    deltas = np.zeros((x.shape[1] + 1, ))\n",
    "    \n",
    "    # Compute the partial derivative for b0\n",
    "    deltas[0] = -(2 * y_diff).mean()\n",
    "    \n",
    "    # Extract the partial derivatives of the remaining betas  \n",
    "    for col in range(x.shape[1]): \n",
    "        deltas[col+1] = -((2 * y_diff) * x[:, col]).mean()\n",
    "    \n",
    "    return deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-664e5ef6f8b4ed2e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Exercise 4.1 Adjusting  parameters with gradient descent\n",
    "\n",
    "Now we want to adjust the parameters with the update rule:\n",
    "\n",
    "$$ \\vec{\\beta}_{i+1} = \\vec{\\beta}_i - \\eta \\Delta_\\vec{\\beta} J$$\n",
    "\n",
    "where $\\eta$ is our learning rate - how fast we want to move in the direction of the gradient. We will be implementing the standard gradient descent, also know as batch gradient descent, where for each iteration we will compute the derivatives for all the samples in the dataset.\n",
    "\n",
    "1. _For epoch in 1...epochs:\n",
    "    1. Predict the outputs with current parameters $\\hat{y} = X \\vec{\\beta}_i$\n",
    "    2. $\\Delta_{\\beta_0}J = -\\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})$\n",
    "    3. $\\Delta_{\\beta_{i=1...N}}J = -\\frac{1}{N} \\sum_{n=1}^N 2 (y - \\hat{y})x_{i_n} $\n",
    "    4. $\\beta_i = \\beta_i - \\eta \\Delta_{\\beta_i}$\n",
    "\n",
    "Notice that you can get the gradients in steps B, C, and D with the function implemented above. Also note that steps B and C can be executed at the same time if you are using the extended forms of the feature matrix and the parameter vector.\n",
    "\n",
    "The number of epochs and the learning rate will impact how fast we converge to the solution and how good the solution will be. Besides the number of epochs, there are more clever ways of knowing when to stop this procedure, but, for simplicity, we will only use this one here.\n",
    "\n",
    "Implement this gradient descent function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f64edb60cc15b1a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_regression_gradient_descent(x, y, betas, learning_rate, epochs): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x : numpy.array with shape (num_samples, num_features) - features of the samples\n",
    "        y : numpy.array with shape (num_samples,)  - true values of the samples\n",
    "        betas : numpy.array with shape (num_features + 1,) - initial parameters\n",
    "        learning_rate : float - factor that will define the size of update step\n",
    "        epochs : int - number of iterations\n",
    "\n",
    "    Returns:\n",
    "        betas : numpy.array with shape (num_features + 1) - final parameters\n",
    "            \n",
    "    \"\"\"    \n",
    "\n",
    "    for epoch in range(epochs): \n",
    "\n",
    "        # Compute estimates for this iteration \n",
    "        # y_hat = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Compute the partial derivatives of the error function \n",
    "        # (hint: check linear_regression_partial_derivatives)\n",
    "        # deltas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        # Update betas with the Gradient Descent rule \n",
    "        # Hint: update all betas in one step\n",
    "        # betas = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df6cd2cbfa50b7be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(84)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 1\n",
    "\n",
    "linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b26ec5b38f334783",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([ 0.04021594,  0.67406805,  0.33511679,  0.03024199,  0.06876607,0.14584739, -0.23413881]),\n",
    "    decimal=4)\n",
    "\n",
    "np.random.seed(84)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 1\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    betas_, \n",
    "    np.array([0.03683279, 0.37950474, 0.28872774, 0.45251198, 0.765261, 0.14199484, 0.11542501]),\n",
    "    decimal=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-84a4265ff22c40c5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "In order to compare the coefficients between the closed form solution and the result from the sklearn model, run the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a3aea3af7aafc4f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Read Dataset\n",
    "data = pd.read_csv('data/california_scaled.csv')\n",
    "columns_housing = data.drop(['Price100k','Latitude','Longitude'], axis=1)\n",
    "x_housing = columns_housing.to_numpy()\n",
    "# House price \n",
    "y_housing = data['Price100k'].to_numpy()\n",
    "\n",
    "np.random.seed(42)\n",
    "betas = np.random.rand(x_housing.shape[1] + 1)\n",
    "learning_rate = 0.1\n",
    "epochs = 200\n",
    "\n",
    "betas_ = linear_regression_gradient_descent(x_housing, y_housing, betas, learning_rate, epochs)\n",
    "coefs_housing, intercept_housing, score_housing = sklearn_model_regression(x_housing, y_housing)\n",
    "\n",
    "intercept_housing_sgd = betas_[0]\n",
    "coefs_housing_sgd = betas_[1:]\n",
    "\n",
    "series_sgd = pd.Series(coefs_housing_sgd, columns_housing.columns, name='SGD')\n",
    "series_ols = pd.Series(coefs_housing, columns_housing.columns, name='OLS')\n",
    "\n",
    "print('Feature coefficients: ')\n",
    "print(pd.concat([series_sgd, series_ols], axis=1))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept SGD: {}'.format(intercept_housing_sgd))\n",
    "print('\\n')\n",
    "\n",
    "print('Intercept OLS: {}'.format(intercept_housing))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a4e45d100fa0d313",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Same results, as expected! If you like, test how many epochs are necessary for the gradient descent to get close to the results of the closed form solution.\n",
    "\n",
    "This is it! The end of this learning unit! \n",
    "\n",
    "![sum](media/sum.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
