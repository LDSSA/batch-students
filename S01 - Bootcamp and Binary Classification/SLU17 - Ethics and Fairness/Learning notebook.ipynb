{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU17 - Ethics & Fairness - Learning notebook\n",
    "\n",
    "When you work as a data scientist, the predictions of your model have consequences in the real world on real people.  The model predictions are influenced by the data that is used for training, by the variables we choose to include, by the metrics we optimize against. This notebook explains how these choices might build in bias and unfairness into the predictions and how to avoid it.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[1. Components of a learning system](#1.-Components-of-a-learning-system)   \n",
    "[2. Privacy by default](#2.-Privacy-by-default)   \n",
    "[3. Bias and fairness](#3.-Bias-and-fairness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Components of a learning system\n",
    "\n",
    "We can think of learning systems as circular loops, i.e., learning loops, influenced and influencing society.\n",
    "\n",
    "![components-of-a-learning-system](./media/components-of-a-learning-system.png)\n",
    "\n",
    "*Fig. 1: Components of a learning system*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examine its constituent parts.\n",
    "\n",
    "**State of the world**   \n",
    "In short, \"State of World\" stands for **reality**, including all the disparities, biases, and general messiness out there.\n",
    "\n",
    "**Measurement**   \n",
    "Measurement is the process of **reducing** the World's messiness to a neat set of values or measurements.\n",
    "\n",
    "**Data**   \n",
    "A **representation** of the object of interest to provide the model with a useful vantage point.\n",
    "\n",
    "**Annotation**    \n",
    "The act of manually **labeling** the data to generate new features or for supervised learning.\n",
    "\n",
    "**Learning**   \n",
    "**Training** a general(izable) model based on the available data.\n",
    "\n",
    "**Model**   \n",
    "The set of **rules** or operations, previously learned from the available data, to make or inform decisions.\n",
    "\n",
    "**Product**   \n",
    "The **experience** that encapsulates the model to create value, or benefit.\n",
    "\n",
    "**Action**   \n",
    "The **effect** or outcome delivered by the product based on the model results.\n",
    "\n",
    "**Feedback**   \n",
    "The **perception** of the action. It can be **explicit**, if given directly, or **implicit**, when inferred from the response.\n",
    "\n",
    "**Context**   \n",
    "Many times overlooked, the context is the **setting** in which the product is being used by the user.\n",
    "\n",
    "**Users**   \n",
    "Self-explanatory, the end-user is someone who **uses** or is intended to use a product or service.\n",
    "\n",
    "**Back to the beginning!**   \n",
    "Since users, as people, are part of the World, **actions directly affect the state of the World**.\n",
    "\n",
    "Moreover, there are, often times, indirect effects through the user. Think [deepfakes](https://en.wikipedia.org/wiki/Deepfake), for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Privacy by default\n",
    "\n",
    "The default behavior of a Data Scientist should be the most privacy friendly.\n",
    "\n",
    "Privacy means that we should be most careful about personal data, respecting and protecting it, by design, at all times.\n",
    "\n",
    "### 2.1 Personal data\n",
    "\n",
    "Personal data is **any specific information relating to an identifiable person**.\n",
    "\n",
    "Things that can be used to identify an individual include:\n",
    "* Name\n",
    "* Location\n",
    "* Physical, physiological, mental information\n",
    "* Genetic and biometric data\n",
    "* Economic or cultural characteristic.\n",
    "\n",
    "Sensitive information (e.g., ethnicity, gender, political opinions and religious beliefs) requires a higher-level of scrutiny than general personal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data collection\n",
    "\n",
    "The central theme in data collection is **informed consent** and it applies to the purpose the data is being collected for. \n",
    "\n",
    "Additionally, it must be **limited to relevant data** for the task at hand, particularly when it concerns personal data.\n",
    "\n",
    "Finally, the data should be accurate and updated (otherwise you should discard it, seriously).\n",
    "\n",
    "If you are using data that was not collected by you, you should know where it comes from, how reliable it is, and for which purposes it was originally created.\n",
    "\n",
    "A recent infamous example is the Boston housing dataset which was part of several modelling packages like sklearn. This data set contained built-in racism and discrimination. Moreover, it was used as a benchmarking model to test the performance of predictive models and so propagating the built-in biases, although its original use was for an explanatory model. See the links in further reading if you'd like to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Data storage\n",
    "\n",
    "Once we collect data, we are responsible for protecting it and, ultimately, our users and/or subjects of interest.\n",
    "\n",
    "We need a plan to **secure and protect the data** against unintended use. It should be defined who has access to the data. Only the people who work with the data should have access to it.\n",
    "\n",
    "It's also important that the users have the ability to access, rectify, and erase their personal data (aka right to be forgotten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Processing\n",
    "\n",
    "The users should be able to **restrict the processing of their data**, if they so choose.\n",
    "\n",
    "Personal information should not be used (much less displayed) unless absolutely necessary. Ideally, the data should be anonymized. The personal data should be stored in separate tables and connected with the rest of the data by an identifier. The access to the personal data can also be restricted to a smaller group of people.\n",
    "\n",
    "We must ensure, at all times, honest representation of our subjects, in line with the underlying data.\n",
    "\n",
    "All the processing should be auditable, properly documented and reproducible.\n",
    "\n",
    "Finally, old and/or unnecessary data should be periodically deleted, according to a data retention plan.\n",
    "\n",
    "### 2.5 Modeling\n",
    "\n",
    "Once a model is in place, we must be vigilant and ready to react at all times.\n",
    "\n",
    "We need to be able to evaluate the model in regards to its effect on users, performance deterioration and unintended use.\n",
    "\n",
    "Given the results, we should be able to roll-back a model if we need to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bias and fairness\n",
    "\n",
    "When evaluating a model a responsible Data Scientist should do more than calculating a loss metric.\n",
    "\n",
    "Fairness implies fair predictions for different subgroups, e.g., different demographics. For that we need to:\n",
    "* Audit the training data\n",
    "* Evaluate predictions in a way that is fairness aware.\n",
    "\n",
    "As you explore your data to figure out the best representation, it's important to proactively audit for potential sources of bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Biases in training data\n",
    "\n",
    "As we’ve seen, the World isn’t without bias: representing it and measuring it with good accuracy would only incorporate the existing disparities. \n",
    "\n",
    "The subjectivity associated with measurement introduces new biases, on top of the unavoidable, worldly ones. \n",
    "\n",
    "#### Reporting bias\n",
    "\n",
    "Reporting bias, for example, occurs when the frequency of events is poorly measured. \n",
    "\n",
    "Often the case with online reviews: since people are more likely to submit reviews when they respond very strongly to the experience, i.e., love or hate. \n",
    "\n",
    "It is common to have artificially imbalanced data towards the extremes and under-represent the ordinary that “goes without saying”.\n",
    "\n",
    "#### Selection bias\n",
    "\n",
    "Selection bias arises when the examples in the data aren’t reflective of the real-world distribution. \n",
    "\n",
    "##### Coverage bias\n",
    "\n",
    "You can fail to cover part of the population in your sample, known as coverage bias. \n",
    "\n",
    "When you survey a customer base for customer satisfaction but fail to include past customers, that’s coverage bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Participation bias\n",
    "\n",
    "Another type of selection bias is participation bias and it happens when a segment of the population is underrepresented, due to lack of responses. \n",
    "\n",
    "If customer satisfaction surveys are sent to a representative sample but response rates are lower for past customers, leading, once again, to an imbalanced sample. \n",
    "\n",
    "##### Sampling bias\n",
    "\n",
    "The last case of selection bias is sampling bias, that arises due to a lack of proper randomization. \n",
    "\n",
    "Imagine that you use only the first few responses when doing the customer satisfaction survey above, representing only the most engaged customers.\n",
    "\n",
    "#### Automation bias\n",
    "\n",
    "Automation bias favors results coming from automation, regardless of error rates and ignoring contradictory information. \n",
    "\n",
    "So, one might prefer a decision from an automated system, even if flawed, to more accurate expert knowledge.\n",
    "\n",
    "#### Group attribution\n",
    "\n",
    "In-group bias occurs when you give preference to members of a group you belong to. \n",
    "\n",
    "Out-group bias is the tendency to stereotype individual members of a different group. \n",
    "\n",
    "This bias has been reinforcing the glass ceiling for many demographics. The hiring managers preferred people of their own kind, typically white male."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implicit bias\n",
    "\n",
    "Implicit bias derives from assumptions made from one’s own experiences, which may or may not generalize.\n",
    "\n",
    "#### Confirmation bias\n",
    "\n",
    "Confirmation bias manifests itself as the propensity to affirm all or some of the preexisting beliefs and hypothesis of the experimenter.\n",
    "\n",
    "### 3.2 Evaluating predictions\n",
    "\n",
    "Metrics calculated against an entire test set don't give an accurate picture of how fair the model is.\n",
    "\n",
    "Rather, metrics can and should be evaluated separately for different groups of interest, particularly when dealing with sensitive information.\n",
    "\n",
    "We will go into specific examples in the exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interesting links\n",
    "\n",
    "1. [Privacy not included](https://foundation.mozilla.org/en/privacynotincluded/) initiative from Mozilla\n",
    "2. [Revisiting the Boston housing dataset](https://fairlearn.org/main/user_guide/datasets/boston_housing_data.html) from the fairlearn project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
