{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Derivatives and closed form solution for the simple linear model\n",
    "\n",
    "###  1.1 Cost function\n",
    "\n",
    "Recall our cost function definition from the learning notebook\n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$ \n",
    "\n",
    "Expanding with the linear model $\\hat{y}_i = \\beta_0 + \\beta_1 x_i$ we get\n",
    "\n",
    "$$J(y, \\hat{y}) = \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Derivatives of the cost function for simple linear regression\n",
    "\n",
    "#### 1.2.1 Derivative with respect to the intercept\n",
    "\n",
    "We'll now derivate with respect to $\\beta_0$, starting from the initial formulation\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2) $$\n",
    "\n",
    "As the only term depending on $\\beta_0$ is inside the sum and the derivative of a sum is a sum of the derivatives, we can rewrite as\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N \\frac{d}{d \\beta_0} ((y_i - \\beta_0 - \\beta_1 x_i)^2) $$\n",
    "\n",
    "The derivative of the squared term is 2 times that term multiplied by the derivative of that term\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\beta_1 x_i) \\frac{d}{d \\beta_0} (y_i - \\beta_0 - \\beta_1 x_i) $$\n",
    "\n",
    "Proceeding with the derivative of $(y_i - \\beta_0 - \\beta_1 x_i)$, only the second term depends on $\\beta_0$ and its derivative is -1\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\beta_1 x_i) (-1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifying the expression, we get\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (\\beta_0 + \\beta_1 x_i - y_i) $$\n",
    "\n",
    "Finally, because $\\beta_0 + \\beta_1 x_i$ is just $\\hat{y}_i$, we get\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (\\hat{y}_i - y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Derivative with respect to the coefficient\n",
    "\n",
    "Using the same principles as above, we'll now derivate with respect to $\\beta_0$, starting from the initial formulation\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{d}{d \\beta_1} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2) $$\n",
    "\n",
    "As the only term depending on $\\beta_1$ is inside the sum and the derivative of a sum is a sum of the derivatives, we can rewrite as\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N \\frac{d}{d \\beta_1} ((y_i - \\beta_0 - \\beta_1 x_i)^2) $$\n",
    "\n",
    "The derivative of the squared term is 2 times that term multiplied by the derivative of that term\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\beta_1 x_i) \\frac{d}{d \\beta_1} (y_i - \\beta_0 - \\beta_1 x_i) $$\n",
    "\n",
    "Proceeding with the derivative of $(y_i - \\beta_0 - \\beta_1 x_i)$, only the third term depends on $\\beta_1$ and its derivative is $-x_i$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\beta_1 x_i) (-x_i) $$\n",
    "\n",
    "Finally, because $\\beta_0 + \\beta_1 x_i$ is just $\\hat{y}_i$, we get\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = \\frac{1}{N}\\sum_{i=1}^N 2 (\\hat{y}_i - y_i) x_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Closed form solution for simple linear regression\n",
    "\n",
    "To get to the closed form solution, we need to find the minimum of the cost function. This is achieved by setting the derivatives to zero.\n",
    "\n",
    "#### 1.3.1 Finding the intercept\n",
    "\n",
    "To find the intercept, we'll equal its derivative to zero\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = 0$$\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N 2 (\\hat{y}_i - y_i) = 0$$\n",
    "\n",
    "We can start by cutting any terms multiplying or dividing the overall equation, since we are equaling to zero. We also substitute for $\\hat{y}$\n",
    "\n",
    "$$\\sum_{i=1}^N (\\beta_0 + \\beta_1 x_i - y_i) = 0$$\n",
    "\n",
    "We will then split the sum so we can isolate the $\\beta_0$ term\n",
    "\n",
    "$$\\sum_{i=1}^N \\beta_0  +  \\sum_{i=1}^N \\beta_1 x_i - \\sum_{i=1}^N y_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging the terms, we get\n",
    "\n",
    "$$\\sum_{i=1}^N \\beta_0 = \\sum_{i=1}^N y_i - \\sum_{i=1}^N \\beta_1 x_i $$\n",
    "\n",
    "We can move the betas outside of the sums because they are the same in every term\n",
    "\n",
    "$$\\beta_0 \\sum_{i=1}^N = \\sum_{i=1}^N y_i - \\beta_1 \\sum_{i=1}^N x_i $$\n",
    "\n",
    "Now we can execute the sums. The sums over $x_i$ and $y_i$ are just their averages times N\n",
    "\n",
    "$$N \\beta_0 = N \\bar{y} - N \\beta_1 \\bar{x} $$\n",
    "\n",
    "Finally, we divide by N\n",
    "\n",
    "$$\\beta_0 = \\bar{y} - \\beta_1 \\bar{x} $$\n",
    "\n",
    "Notice that the resulting expression depends on the value of the coefficient, so let's proceed to compute the solution for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Finding the coefficient\n",
    "\n",
    "To find the coefficient, we start in the same way, by setting the derivative to zero and substituting for $\\hat{y}$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_1} = 0$$\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N 2 (\\hat{y}_i - y_i) x_i = 0$$\n",
    "\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N 2 (\\beta_0 + \\beta_1 x_i - y_i) x_i = 0$$\n",
    "\n",
    "We can then replace $\\beta_0$ by the result we got before, rearrange the terms and remove any multiplication factors\n",
    "\n",
    "$$\\frac{1}{N} \\sum_{i=1}^N 2 (\\bar{y} - \\beta_1 \\bar{x} + \\beta_1 x_i - y_i) x_i = 0$$\n",
    "\n",
    "$$\\sum_{i=1}^N [\\bar{y} - y_i - \\beta_1 (\\bar{x} - x_i )] x_i = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we isolate $\\beta_1$\n",
    "\n",
    "$$\\sum_{i=1}^N [(\\bar{y} - y_i) x_i - \\beta_1 (\\bar{x} - x_i ) x_i] = 0$$\n",
    "\n",
    "$$\\beta_1 = \\frac {\\sum_{i=1}^N (\\bar{y} - y_i) x_i}{\\sum_{i=1}^N (\\bar{x} - x_i ) x_i}$$\n",
    "\n",
    "Now we have two options how to arrange the expression into an easy-to-use form. The first option is to split the sums and remember that the sum of $x_i$ is just its average times N\n",
    "\n",
    "$$\\beta_1 = \\frac {\\bar{y} \\sum_{i=1}^N x_i - \\sum_{i=1}^N y_i x_i}{\\bar{x} \\sum_{i=1}^N x_i - \\sum_{i=1}^N {x_i}^2}$$\n",
    "\n",
    "$$\\beta_1 = \\frac {\\bar{y} \\bar{x} - \\frac{1}{N} \\sum_{i=1}^N y_i x_i}{{\\bar{x}}^2 - \\frac{1}{N} \\sum_{i=1}^N {x_i}^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option is to rearrange into the formula we've seen in the learning notebook. We will use the fact the following expressions are equal to 0, so we can insert them to the sums without consequences\n",
    "\n",
    "$\\sum_{i=1}^N (\\bar{x}^2 - x_i\\bar{x})$\n",
    "\n",
    "$\\sum_{i=1}^N (\\bar{x}\\bar{y} - y_i\\bar{x})$\n",
    "\n",
    "Continuing from above and including the zero expressions to the fraction (we need to use the minus sign so it works out)\n",
    "\n",
    "$$\\beta_1 = \\frac {\\sum_{i=1}^N (\\bar{y} - y_i) x_i}{\\sum_{i=1}^N (\\bar{x} - x_i ) x_i} = \\frac {\\sum_{i=1}^N (\\bar{y} x_i - y_i x_i)}{\\sum_{i=1}^N (\\bar{x} x_i - {x_i}^2 )}$$\n",
    "\n",
    "$$\\beta_1 = \\frac {\\sum_{i=1}^N (\\bar{y} x_i - y_i x_i) - \\sum_{i=1}^N (\\bar{x}\\bar{y} - y_i\\bar{x})}{\\sum_{i=1}^N (\\bar{x} x_i - {x_i}^2 ) - \\sum_{i=1}^N (\\bar{x}^2 - x_i\\bar{x})}$$\n",
    "\n",
    "$$\\beta_1 = \\frac {\\sum_{i=1}^N (\\bar{y} x_i - y_i x_i - \\bar{x}\\bar{y} + y_i\\bar{x})}{\\sum_{i=1}^N (\\bar{x} x_i - {x_i}^2  \\bar{x}^2 - x_i\\bar{x})}$$\n",
    "\n",
    "Finally, the numerator turns out to be the covariance and the numerator the variance and we get to the expression we already know from the learning notebook (the minus signs cancel out)\n",
    "\n",
    "$$\\beta_1 = \\frac{-\\sum_{i}^{N}{(x_i - \\bar{x})(y_i - \\bar{y})}}{-\\sum_{i}^{N}{(x_i - \\bar{x})^2}} = - \\frac{cov(x, y)}{var(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Derivatives and closed form for the multiple linear model\n",
    "\n",
    "###  2.1 Cost function\n",
    "\n",
    "Recall the cost function definition from the learning notebook, which is the same as in the simple model\n",
    "\n",
    "$$J(y, \\hat{y}) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 $$ \n",
    "\n",
    "Expanding with a multiple linear model with K features, we get\n",
    "\n",
    "$$J(y, \\hat{y}) = \\sum_{i=1}^N (y_i - \\beta_0 -  \\sum_{j=1}^K \\beta_k x_{ki})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Derivatives of the cost function for multiple linear regression\n",
    "\n",
    "The derivatives are not required for the closed form solution, however they are quite useful for other methods, such as the gradient descent, which you learn at the end of the learning notebook.\n",
    "\n",
    "#### 2.2.1 Derivative with respect to the intercept\n",
    "\n",
    "The intercept derivative is the same. Let's develop it from the equation above\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{d}{d \\beta_0} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{ji})^2) $$\n",
    "\n",
    "As the only term depending on $\\beta_0$ is inside the sum and the derivative of a sum is a sum of the derivatives, we can rewrite as\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N \\frac{d}{d \\beta_0} ((y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{ji})^2) $$\n",
    "\n",
    "The derivative of the squared term is 2 times that term multiplied by the derivative of that term\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{ji}) \\frac{d}{d \\beta_0} (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{ji}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceeding with the derivative, only the second term depends on $\\beta_0$ and its derivative is -1\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_k x_{ji}) (-1) $$\n",
    "\n",
    "Finally, simplifying we get\n",
    "\n",
    "$$\\frac{d J}{d \\beta_0} = \\frac{1}{N}\\sum_{i=1}^N 2 (\\hat{y}_i -y_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Derivative with respect to the coefficient\n",
    "\n",
    "We'll now derivate with respect to each $\\beta_k$. This might seem tricky, but in the end just one term of the sum $\\sum_{j=1}^K \\beta_k x_{k_i})$ depends on $\\beta_k$. Assume from here on that $k \\in [1, ..., K]$ where K is the number of features of the model. Let's start from the basic expression and proceed exactly as for the simple linear model\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = \\frac{d}{d \\beta_k} (\\frac{1}{N}\\sum_{i=1}^N (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji})^2) $$\n",
    "\n",
    "As the only term depending on $\\beta_1$ is inside the sum and the derivative of a sum is a sum of the derivatives, we can rewrite as\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = \\frac{1}{N}\\sum_{i=1}^N \\frac{d}{d \\beta_k} ((y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji})^2) $$\n",
    "\n",
    "The derivative of the squared term is 2 times that term multiplied by the derivative of that term\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji}) \\frac{d}{d \\beta_k} (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceeding with the derivative, only the third term, the sum, depends on $\\beta_k$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji}) \\frac{d}{d \\beta_k} (-\\sum_{j=1}^K \\beta_j x_{ji}) $$\n",
    "\n",
    "Inside the sum, only the kth term contains $\\beta_k$ and its derivative is $-x_{ki}$\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\beta_0 - \\sum_{j=1}^K \\beta_j x_{ji}) (-x_{ki}) $$\n",
    "\n",
    "Simplifying, we get\n",
    "\n",
    "$$\\frac{d J}{d \\beta_k} = (\\frac{1}{N}\\sum_{i=1}^N 2 (y_i - \\hat{y}_i) (-x_{ki}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Closed form solution for multiple linear regression\n",
    "\n",
    "The multiple linear regression closed form solution makes use of the matrix form of the expressions which provides some handy rules that simplify the process.\n",
    "\n",
    "First we define our model in matrix notation\n",
    "\n",
    "$$\\hat{y} = X\\vec{\\beta} $$\n",
    "\n",
    "where X is the feature matrix extended with abcolumn of ones\n",
    "\n",
    "$$ X = [\\vec{1} | X'] $$\n",
    "\n",
    "The cost function in matrix notation is\n",
    "\n",
    "$$ J = (\\vec{y} - X\\vec{\\beta})^T (\\vec{y} - X\\vec{\\beta}) $$\n",
    "\n",
    "where $\\vec{y}$ is the vector of true sample values.\n",
    "\n",
    "The gradient of the cost function in matrix notation is\n",
    "\n",
    "$$ \\Delta_{\\vec{\\beta}} J  = \\Delta_{\\vec{\\beta}} [(\\vec{y} - X\\vec{\\beta})^T (\\vec{y} - X\\vec{\\beta})]  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are just partial derivatives of J with respect to all $\\beta_k$. The expression in the square brackets is a multiplication of two factors. Its gradient is calculated according to the rule $\\Delta(AB) = \\Delta(A) B + A \\Delta(B)$. We also need to know that $(AB)^T = B^T A^T$\n",
    "\n",
    "$$ \\Delta_{\\vec{\\beta}} J  = \\Delta_{\\vec{\\beta}} \\vec{\\beta}^T (-X)^T (\\vec{y} - X\\vec{\\beta}) + (\\vec{y} - X\\vec{\\beta})^T \\Delta_{\\vec{\\beta}} \\vec{\\beta} (-X)$$ \n",
    "\n",
    "Now $\\Delta_{\\vec{\\beta}} \\vec{\\beta}$ is just a unit matrix, so the expression becomes\n",
    "\n",
    "$$ \\Delta_{\\vec{\\beta}} J  = (-X)^T (\\vec{y} - X\\vec{\\beta}) + (\\vec{y} - X\\vec{\\beta})^T (-X)$$ \n",
    "\n",
    "Using the transpose rule again and simplifying\n",
    "\n",
    "$$ \\Delta_{\\vec{\\beta}} J  = -X^T (\\vec{y} - X\\vec{\\beta}) + -X^T(\\vec{y} - X\\vec{\\beta})$$ \n",
    "\n",
    "$$ \\Delta_{\\vec{\\beta}} J  = -2 X^T (\\vec{y} - X\\vec{\\beta})$$ \n",
    "\n",
    "Finally, setting the derivative to zero, we get a very clean closed form solution\n",
    "\n",
    "$$  -2 X^T (\\vec{y} - X\\vec{\\beta}) = 0 $$ \n",
    "\n",
    "$$  2 X^T X\\vec{\\beta} = 2 X^T \\vec{y}$$ \n",
    "\n",
    "$$  \\vec{\\beta} = (X^T X)^{-1} X^T \\vec{y} $$ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
