{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This set of BLUs revolves around the topic of Natural Language Processing (NLP). As the name implies, this field is all about the processing and handling of natural language in a way that enables a computer to do useful things with it. There are plenty of interesting applications around it, namely:\n",
    "\n",
    "- **Speech recognition**: the task of extracting the words that are being spoken on a sample of audio, or even extracting prosody features, for example.\n",
    "- **Natural language generation**: the task of putting computational formulations into actual text, for example, automated generation of labels to images, summarisation of texts and data, creation of dialogue systems, etc.\n",
    "- **Natural language understanding**: the task of getting some meaning out of the data. For instance, recognizing entities in sentences, semantic roles, or even classify sentences according to their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, some of the main tasks and areas of research of NLP are:\n",
    "\n",
    "- **Part of Speech tagging**: Determine the role of each word in a given sentence, for instance, if it is an adjective, verb, noun, etc.\n",
    "\n",
    "- **Word Segmentation**: Break continuous text into words.\n",
    "\n",
    "- **Parsing**: Define a tree that represents the grammatical structure of a sentence.\n",
    "\n",
    "- **Machine Translation**: Translate sentences from a source language to a target language automatically.\n",
    "\n",
    "- **Named entity recognition**: Find parts of the text that correspond to certain entities, like names of places, people, companies, etc.\n",
    "\n",
    "- **Question answering**: Given a question in human language, find the most appropriate answer.\n",
    "\n",
    "- **Text to speech**: As the name implies, transform written text into audible, human-like sounds that correspond to the given input.\n",
    "\n",
    "Many of these tasks are out of the scope of these learning units, but we still think they're a motivating entry point into exploring this exciting topic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing text \n",
    "\n",
    "Most fields that use text data require, in one way or the other, transforming it into an easier to use format. In order to extract relevant information from text, it is extremely important to preprocess it. This task usually includes the following methods:\n",
    "\n",
    "- Splitting words in a sentence\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Removing common words (stop words)\n",
    "- Extracting suffixes or prefixes\n",
    "- Part of speech filtering\n",
    "\n",
    "At first, these text processing tasks may seem _easy_. For instance, separating the words in a sentence is as simple as looking at the spaces or punctuation between words. But when you really think about the diversity of languages characteristics, you start to understand how daunting all these tasks are. Take a look at Mandarin Chinese, for instance. Our heuristic is suddenly not valid anymore. And for many of the tasks, there are plenty of edge cases.\n",
    "\n",
    "<img src=\"./media/xkcd_language_nerd.png\" width=\"300\">\n",
    "\n",
    "Bottom line is: language is hard! But that's what makes this field one of the most challenging but also most rewarding to work on.\n",
    "\n",
    "\n",
    "Throughout these learning units we hope to give you some basic understanding on how to transform text into something useful for models, explain some challenges in this field, solve interesting problems and, hopefully, make you want to learn more about the topic afterwards! :)\n",
    "\n",
    "The first part of this BLU goes through some fundamental concepts that will be helpful for all the practical tasks that come next in this specialization (and also in the future, if you ever need to work with text data!). We will start by introducing **regular expressions**, followed by three important concepts in data pre-processing (**tokenization**, **stopwords**, and **stemming**). Finally, we will learn about **n-grams** and **n-gram-models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding patterns in text\n",
    "\n",
    "To be able to perform any of the text processing tasks mentioned in an efficient way, we need to be able to parse and detect patterns in the text programatically. For the purpose of simplification, we'll only focus on English for the next couple of examples.\n",
    "\n",
    "Let's say you have the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_example = \"\"\"\n",
    "Bommer Canyon is an open space preserve in southern Irvine, California featuring hiking and \n",
    "biking trails as well as private event areas. The canyon is part of the Irvine Ranch, which \n",
    "itself is a National Natural Landmark, the first California Natural Landmark,[1][2] and part \n",
    "of the City of Irvine Open Space Preserve.[3][4] The preserve is adjacent to the affluent \n",
    "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16,000 acres of \n",
    "preserved open space.[5] Approximately 15 of these acres are preserved as a \"Cattle Camp\" \n",
    "named for the area's previous cattle operations and are now rented for private events such \n",
    "as campouts, company picnics, and family reunions.[6] The trails in Bommer Canyon feature \n",
    "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
    "residents who use them for nature walks, hiking and mountain biking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you answer the question:\n",
    "\n",
    "> What are all the words in the following text that start with the letter `a`?\n",
    "\n",
    "You could obviously count them manually. But if, instead, you had thousands or millions of lines? That becomes impossible!\n",
    "\n",
    "A second option, given your recently acquired skills in python, could be to write a function that does that for you:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas.\n",
      "- a\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- a\n",
      "- area's\n",
      "- and\n",
      "- are\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "def find_all_words_starting_with_a(text):\n",
    "    \n",
    "    # Assuming all words can be split by spaces - big assumption\n",
    "    list_of_a_words = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w.startswith(\"a\"):\n",
    "            list_of_a_words.append(w)\n",
    "    \n",
    "    return list_of_a_words\n",
    "\n",
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a slightly more complex task: we want to find and remove all punctuation, replacing it with a space when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and \n",
      "biking trails as well as private event areas The canyon is part of the Irvine Ranch which \n",
      "itself is a National Natural Landmark the first California Natural Landmark  1  2 and part \n",
      "of the City of Irvine Open Space Preserve  3  4 The preserve is adjacent to the affluent \n",
      "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of \n",
      "preserved open space  5 Approximately 15 of these acres are preserved as a Cattle Camp \n",
      "named for the area s previous cattle operations and are now rented for private events such \n",
      "as campouts company picnics and family reunions  6 The trails in Bommer Canyon feature \n",
      "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
      "residents who use them for nature walks hiking and mountain biking \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    big_list_of_punctuation = [\n",
    "        \".\", \",\", \"?\", \"!\", \"-\", \"_\", \":\", \";\",\n",
    "        \"\\\"\", \"'\", \"|\", \"(\", \")\", \")\", \"/\", \"\\\\\",\n",
    "        \"[\", \"]\",\n",
    "    ]\n",
    "    \n",
    "    processed_text = \"\"\n",
    "    for idx, letter in enumerate(text):\n",
    "        # Get previous and next letters\n",
    "        previous_letter = text[idx - 1] if idx > 1 else \"\"\n",
    "        next_letter = text[idx + 1] if idx < len(text) - 2 else \"\"\n",
    "        \n",
    "        # If the letter is punctuation, and the previous and next letters are not spaces, add a space\n",
    "        # Otherwise we'll have too many spaces\n",
    "        if letter in big_list_of_punctuation:\n",
    "            if previous_letter != \" \" and next_letter != \" \":\n",
    "                processed_text += \" \"\n",
    "        else:\n",
    "            processed_text += letter\n",
    "        previous_letter = letter\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "print(remove_punctuation(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more complicated, but seems to have worked! Now let's see what would happen for a different text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adenanthos venosus is an openly branched shrub that      typically grows to a height of 1–2 m \n",
      " 3 ft 3 in – 6 ft 7 in and forms a lignotuber Its leaves are mostly arranged in clusters \n",
      "at the ends of branches egg shaped sometimes with the narrower end towards the base \n",
      "mostly 15–20 mm 0 59–0 79 in long 10 mm 0 39 in wide and sessile The leaves are \n",
      "mostly glabrous and have a pointed tip The flowers are ”dull crimson” to ”pinkish purple” \n",
      "with a cream coloured band in the centre and many glandular hairs on the outside \n",
      "\n"
     ]
    }
   ],
   "source": [
    "different_text_example = \"\"\"\n",
    "Adenanthos venosus is an openly-branched shrub that      typically grows to a height of 1–2 m \n",
    "(3 ft 3 in – 6 ft 7 in) and forms a lignotuber. Its leaves are mostly arranged in clusters \n",
    "at the ends of branches, egg-shaped, sometimes with the narrower end towards the base, \n",
    "mostly 15–20 mm (0.59–0.79 in) long, 10 mm (0.39 in) wide and sessile. The leaves are \n",
    "mostly glabrous and have a pointed tip. The flowers are ”dull crimson” to ”pinkish purple” \n",
    "with a cream-coloured band in the centre and many glandular hairs on the outside. \n",
    "\"\"\"\n",
    "\n",
    "print(remove_punctuation(different_text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this time our cleaned text is not so well formatted... In particular, we missed some `”` characters and spaces.\n",
    "\n",
    "Of course, we could go back and re-write our function... But what we want to point out is that coding all these tasks from scratch is not only inefficient, but also quite boring.\n",
    "\n",
    "When cleaning text, we may encounter endless different variations and extra conditions that will make it very hard to keep track of everything. Some examples apart from dealing with punctuation are:\n",
    "\n",
    "- Replacing all numbers with a placeholder\n",
    "- Remove all decimals from numbers\n",
    "- Count all uppercase letters in a text\n",
    "- Find all words that have less than 3 letters (or any other number)\n",
    "- ...\n",
    "\n",
    "This is where **regular expressions** come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns in a standardized way. They is one of the most fundamental concepts in computer science when working with text data.\n",
    "\n",
    "The idea is simple: by defining a text pattern using regex rules, you can easily locate and replace it within any piece of text!\n",
    "\n",
    "Most of the tasks that we defined before can be performed using regex. Let's see our first example: finding all words starting with 'a' in the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- area\n",
      "- and\n",
      "- are\n",
      "- as\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "def find_all_words_starting_with_a_regex(text):\n",
    "    \n",
    "    pattern = r\"\\ba\\w+\\b\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a_regex(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more compact, right? Now let's do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and biking trails as well as private event areas The canyon is part of the Irvine Ranch which itself is a National Natural Landmark the first California Natural Landmark 1 2 and part of the City of Irvine Open Space Preserve 3 4 The preserve is adjacent to the affluent Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of preserved open space 5 Approximately 15 of these acres are preserved as a Cattle Camp named for the area s previous cattle operations and are now rented for private events such as campouts company picnics and family reunions 6 The trails in Bommer Canyon feature groves of oak and sycamore trees as well as rough rock outcrops and are popular with area residents who use them for nature walks hiking and mountain biking \n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation_regex(text):\n",
    "    \n",
    "    pattern_punkt = r\"[\\.,\\?\\!\\-\\_\\:\\;\\\"'\\|\\(\\)/\\\\\\[\\]]\"\n",
    "    \n",
    "    # Replaces all punctuation characters by spaces\n",
    "    text_no_punkt = re.sub(pattern_punkt, \" \", text)\n",
    "    \n",
    "    # Collapses multiple spaces\n",
    "    text_no_punkt = re.sub(r\"\\s+\", \" \", text_no_punkt)\n",
    "\n",
    "    return text_no_punkt\n",
    "    \n",
    "\n",
    "print(remove_punctuation_regex(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool right? \n",
    "\n",
    "Regex enables us to do a bunch of interesting things with text. But if you're still not convinced, try this [website](https://regex101.com/) out! You can play around with regex and validate that the patterns you wrote are performing as expected.\n",
    "\n",
    "See some extra examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\/'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\/'\n",
      "/tmp/ipykernel_36193/4085194098.py:5: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  pattern_words_until_3 = \"\\b\\w{1,3}\\b\"\n",
      "/tmp/ipykernel_36193/4085194098.py:8: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  pattern_urls = \"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n"
     ]
    }
   ],
   "source": [
    "# Find digits\n",
    "pattern_digits = \"[0-9]*\"\n",
    "\n",
    "# Find words smaller than 3 characters \n",
    "pattern_words_until_3 = \"\\b\\w{1,3}\\b\"\n",
    "\n",
    "# Find URLs in a text\n",
    "pattern_urls = \"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may be looking a bit like this\n",
    "\n",
    "<img src=\"./media/regex-confusion.png\" width=\"500\">\n",
    "\n",
    "But worry not! \n",
    "\n",
    "Most of us don't know regex by heart, and we need to take a look at cheatsheets from time to time, like the one shown below. Still, as you move forward in the NLP world, we hope you see how helpful regex can be.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cheatsheet ([see more](https://cheatography.com/davechild/cheat-sheets/regular-expressions/))\n",
    "\n",
    "`.` - matches any character, except newline.\n",
    "\n",
    "`\\d, \\s \\S` - match digit, match whitespace, not whitespace.\n",
    "\n",
    "`\\b, \\B` - word, not word boundary.\n",
    "\n",
    "`[xyz]` - matches x, y or z.\n",
    "\n",
    "`[^xyz]` - matches anything that is not x, y or z.\n",
    "\n",
    "`[x-z]` - matches a character between x and z.\n",
    "\n",
    "`^xyz$` - `^` is the start of the string, `$` is the end of the string.\n",
    "\n",
    "`\\.` - use escaping to match special characters.\n",
    "\n",
    "`\\t`, `\\n` - matches tab and newline.\n",
    "\n",
    "`x*` - matches 0 or more symbols x.\n",
    "\n",
    "`x+` - matches 1 or more symbols x.\n",
    "\n",
    "`x?` - matches 0 or 1 symbol x.\n",
    "\n",
    "`.?`, `*?`, `+?`, etc - represent non-greedy search. \n",
    "\n",
    "`x{5}` - matches exactly 5 symbols x.\n",
    "\n",
    "`x{5,}` - matches 5 or more symbols x.\n",
    "\n",
    "`x{5, 8}` - matches between 5 and 8 symbols x.\n",
    "\n",
    "`xy|yz` - matches `xy` or `yz`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python `re` library\n",
    "\n",
    "The python library that we are using - [re](https://docs.python.org/3/library/re.html) - has many other powerful methods. We'll deep dive into each of them in the following sections.\n",
    "\n",
    "#### Finding one instance with `search()` \n",
    "\n",
    "Using `search()` we can take a certain pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the first text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is already possible to observe some things about `re.search()`:\n",
    "\n",
    "- When there is no match, `search()` returns `None`.\n",
    "\n",
    "- The `Match` object has the index of the beginning and end of the match. Might be used via `match.start()` and `match.end()`.\n",
    "\n",
    "- If there is more than one instance of the word in the text, only the first will be retrieved.\n",
    "\n",
    "#### Finding all instances with `findall()`  or `finditer()`\n",
    "\n",
    "If we want to return all the matches to our pattern in a given text we might use the function `findall()`. In this case, the matched portions of the text will be returned, instead of the `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "pattern = r\"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, one of the words was written as _Lisbona_ , but we still match the _Lisbon_ portion of that word. If we add the condition of having a white space after the letter *n* we will only get two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon \n",
      "Lisbon \n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Lisbon\\s\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` should be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing all instances with `sub()`\n",
    "\n",
    "If we want to replace the matches of our pattern in a given text with something else we need to use the function `sub()`. In this case, the matched portions of the text will be replaced, and the changed text will be returned.\n",
    "\n",
    "For example if we wanted to remove the word `Lisbon` from a text we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Madrid  Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "# \\b indicates a word boundary so using it around a word \n",
    "# will only replace the text when it shows as a proper word, \n",
    "# between spaces or punctuation \n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"\", text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we wanted to replace by another word we could just specify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisboa Madrid Lisboa Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "# \\b indicates a word boundary so using it around a word \n",
    "# will only replace the text when it shows as a proper word, \n",
    "# between spaces or punctuation \n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"Lisboa\", text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### A primer on patterns\n",
    "\n",
    "Now that you are familiar with the `re` functions, we'll use them to explore a bit better the possible different pattern that can be expressed with regex\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "Looking at some of the previously shown codes at cheatsheet, let's see in some simple examples how that may help us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"x xy xyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember what we've shown previously: `.` will match any character after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x ', 'xy', 'xy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"x.\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` will match 0 or more y symbols after xy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xyy']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy*\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+` will match 1 or more y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xy', 'xyy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` will match 0 or 1 y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{i}` will match i y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy{2}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Jani Senna conway Kobayashi Lopez buemi Nakajima alonso\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match only the names that start with capital letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text) # find substrings starting with a capital letter\n",
    "                                # followed by 1 or more lowercase letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match all the names that don't start with letters \"B\" and \"L\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'conway', 'Kobayashi', 'Nakajima', 'alonso']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\b[^bBlL\\s][A-Za-z]+\", text) # find substrings after a word boundary that...\n",
    "                                          # do not begin with B or L or whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be wondering what that hacky `r` is doing before the actual regex we are using. This has no connection with regex. It is just a way of telling python that it should interpret backslashes `\\` literally (Notice how our regex has `\\b` and `\\s`). For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With r:\n",
      "\n",
      "lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\n",
      "\n",
      "\n",
      "Without r:\n",
      "\n",
      "lotterer \n",
      " Jani \n",
      " Senna conway Kobayashi Lopez buemi Nakajima alonso\n"
     ]
    }
   ],
   "source": [
    "print(\"With r:\\n\")\n",
    "print(r\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")\n",
    "print(\"\\n\")\n",
    "print(\"Without r:\\n\")\n",
    "print(\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, since we are using `r`, python takes `\\n` literally. On the other hand, on the second case, python interprets it as the escaped symbol for newline.\n",
    "\n",
    "Another important thing to know is that, since regex interprets several characters in a special way, you need to escape them if you want to match them perfectly. For that purpose you also use the `\\`. Whatever comes after this character is considered escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=r\"Text \\ with + special [characters].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\".+[ ]\\ \", text): # If we don't escape the characters we mean\n",
    "    print(m)                           # find, we won't match anything and could \n",
    "                                       # even have a broken regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n",
      "\\\n",
      "+\n",
      "[\n",
      "]\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\"[\\.\\+\\[\\]\\\\]\", text): # If we escape the characters we mean to \n",
    "    print(m)                                 # find, we'll match them literally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice in particular the `\\\\`. This is just a corner case, as the backlash is used to escape any character, it's also used to escape itself.  \n",
    "\n",
    "<img src=\"./media/backslashes.png\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Imagine now we have some extra information after the names, and we receive a file with many lines. We still want only names starting with capital letters. So we run the previous regex and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Rebellion\\nJani Rebellion\\nSenna Rebellion\\nconway Toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima Toyota\\nalonso Toyota\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion',\n",
       " 'Jani',\n",
       " 'Rebellion',\n",
       " 'Senna',\n",
       " 'Rebellion',\n",
       " 'Toyota',\n",
       " 'Kobayashi',\n",
       " 'Toyota',\n",
       " 'Lopez',\n",
       " 'Toyota',\n",
       " 'Toyota',\n",
       " 'Nakajima',\n",
       " 'Toyota',\n",
       " 'Toyota']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, actually we just want the first name! So let's try to add the symbol `^` to make sure the expression only captures the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum.. we got a handful of nothing. Why is this happening? Well, the regex processes all the text as a single line, and the first name doesn't start with a capital letter. To make sure this is the case, let's change `lotterer` to `Lotterer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still only capture one line. Luckily, we have [`re.MULTILINE`](https://docs.python.org/3/library/re.html#re.MULTILINE), that allows us to process multiline strings easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we were able to get all the information we wanted! And what if we wanted the second name on each line? Well, in this case, that is the last word of the line, so we may use `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion', 'Rebellion', 'Toyota', 'Toyota', 'Toyota', 'Toyota']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+$\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want all full lines ending with `rebellion`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer Rebellion', 'Jani rebellion', 'Senna Rebellion']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\".*rebellion$\", text, flags=(re.MULTILINE|re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that here we are also taking advantage of the flag `re.IGNORECASE`. This is a convenient flag to add if you want case-insensitive matches. Multiple regex flags can be strung together with pipes: `|`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can get hard to read really fast, but even knowing the basics will be certainly helpful sometime in the future. To better understand how they work, nothing is better than practicing, and sites like [this](https://regex101.com/) are valuable visual tools to do so. The python library that we used has a lot of more powerful methods which might be useful to future tasks, so make sure to check them out if you're interested!\n",
    "\n",
    "Here are some more reading suggestions about regex:\n",
    "- https://towardsdatascience.com/regular-expressions-clearly-explained-with-examples-822d76b037b4\n",
    "- https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how to find patterns in text, let's explore how to partition it into smaller, meaningful tokens.\n",
    "\n",
    "This task is called _tokenization_ and it's an essential step when dealing with text. In practice _tokenization_ is about splitting the strings of a corpus into substrings. This is important because we can transform a string into parts that are more suitable to be used by natural language processing tools. For instance, if we are working with the sentence:\n",
    "\n",
    "_\"The car went too fast on the second lap. This damaged the tires.\"_ ,\n",
    "\n",
    "it will get easier if split it into substrings:\n",
    "\n",
    "_[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tires\", \".\"]_ .\n",
    "\n",
    "<img src=\"./media/tokenizer.png\" width=\"500\">\n",
    "\n",
    "Hopefully by now you've realized that this task is does more than just splitting space and requires a bit more thought. To simplify things, we'll use some libraries and methods that help us implement tokenization.\n",
    "\n",
    "We will be using [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The car went too fast on the second lap. This damaged the tires...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the tokenizer is created by taking advantage of the regular expressions we learned earlier. This means that we can make different tokenizers according to what we want to split on. For instance, if we had used `[A-Z]\\w+`, the tokenizer would only select the words that begin with capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'This']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z]\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are already some pre-defined implementations we can use by taking advantage of `nltk.tokenize` module. These are:\n",
    "- `BlanklineTokenizer` - Tokenize a string using blank lines as the delimiter.\n",
    "- `WordPunctTokenizer` - Tokenize a string into alphabetic and non-alphabetic characters.\n",
    "- `WhitespaceTokenizer`-  Tokenize a string using spaces, tabs, and newlines as delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The car went too fast on the second lap. This damaged the tires...']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap',\n",
       " '.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires',\n",
       " '...']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires...']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordPunctTokenizer()` is similar to the first one we defined (`RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')`. This tokenizer is one of the most commonly used. So, when we talk about tokenization without specifying further details, this is by default the type of tokenization that we expect you to use (for example, in exercises).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Note**: Even though we are using NLTK library during this BLU, some other libraries are commonly used as well, and are probably better. Here is a list of some to consider in your future challenges in NLP:\n",
    "\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is another very important concept in natural language processing. This technique allows us to get the \"root\" of words by ignoring suffixes. This is important because, in certain tasks, we are more interested in the broader semantics of a word and not the specific variation of it, for instance, if it's a verb or an adjective.\n",
    "\n",
    "So, let's see what this step gets us for the same example we have been using. To do that, we will be using the NLTK implementation of the [snowball stemmer](https://www.nltk.org/api/nltk.stem.snowball.html). Notice that there are other stemmers, some of them specific to certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'this', 'damag', 'the', 'tire', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('better')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that _\"damage\"_ and _\"tires\"_ are transformed semi-words consisting only of the suffixes of the original words. Notice as well that all the words have been lowercased. Lowercasing the data is also a common step in text pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stemming there is also the process of **lemmatization**. Both processes share the goal of getting the root of the word, or more formally, reduce inflectional forms of a word to a common base form [\\[7\\]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), but they act differently. While stemming follows an heuristic approach that drops the suffix of words in order to get closer to the common base form, lemmatization returns the base or dictionary form of a word, known as _lemma_.\n",
    "\n",
    "For example, if given the word _consultations_, stemming would return only *consult*, while lemmatization would take into account if the word was the verb or the noun, as well as if it's in the plural or singular form, returning *consultation*\n",
    "\n",
    "As you may expect, lemmatization is much more expensive in computational terms and, for certain applications, stemming might be more than enough to obtain good results. We will be using only stemming throughout the NLP learning units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords \n",
    "\n",
    "**Stopwords** are common words that don't have much semantic meaning. Examples of stop words are pronouns or articles. Most times, it is better to remove this less meaningful words.\n",
    "\n",
    "As an example of why removing stop words might be an important step, imagine a search engine going through a big range of documents. Words as \"*the*\", \"*a*\", \"*at*\", etc. will be present in so many documents that using them in the search will not help at all at finding the best documents to answer our query. So filtering them out will speed up the search and remove noise, making it beneficial to our goal.\n",
    "\n",
    "Let's start by downloading the stopwords corpus provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ines/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a list of stop words in any language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_eng = set(stopwords.words('english'))\n",
    "\n",
    "stop_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "stop_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all stop words from out tokens list. We need to first lowercase the words since stop words are all saved as lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'went', 'fast', 'second', 'lap', '.', 'damaged', 'tires', '...']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in words if word.lower() not in stop_eng]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating _N-grams_ is one of the preprocessing strategies that has been longer used for NLP tasks. They consist of sequences of N consecutive tokens on a given sentence. Each element is usually a word, but we may define it as we wish for the task at hand. Depending on the value choosen for N, we may have unigrams, bigrams, trigrams, four-grams, etc.\n",
    "\n",
    "For instance, for the sentence\n",
    "\n",
    "`\"The driver made a mistake\"`,\n",
    "\n",
    "we would have:\n",
    "\n",
    "- unigrams: `The`, `driver`, `made`, `a`, `mistake`\n",
    "- bigrams: `The driver`, `driver made`, `made a`, `a mistake`\n",
    "- trigrams: `The driver made`, `driver made a`, `made a mistake`\n",
    "- four-grams: `The driver made a`, `driver made a mistake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create _N-grams_ by taking advantage of the [NLTK ngram](http://www.nltk.org/_modules/nltk/model/ngram.html) implementation. We will be using the tokenized list `words` created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The',), ('car',), ('went',), ('too',), ('fast',), ('on',), ('the',), ('second',), ('lap',), ('.',), ('This',), ('damaged',), ('the',), ('tires',), ('...',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car'), ('car', 'went'), ('went', 'too'), ('too', 'fast'), ('fast', 'on'), ('on', 'the'), ('the', 'second'), ('second', 'lap'), ('lap', '.'), ('.', 'This'), ('This', 'damaged'), ('damaged', 'the'), ('the', 'tires'), ('tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car', 'went'), ('car', 'went', 'too'), ('went', 'too', 'fast'), ('too', 'fast', 'on'), ('fast', 'on', 'the'), ('on', 'the', 'second'), ('the', 'second', 'lap'), ('second', 'lap', '.'), ('lap', '.', 'This'), ('.', 'This', 'damaged'), ('This', 'damaged', 'the'), ('damaged', 'the', 'tires'), ('the', 'tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use N-grams for several reasons. For instance, we can look at them as features of our corpus, where the feature space is defined by all N-grams of our vocabulary. These features may be used, for instance, as input of a classification model.\n",
    "\n",
    "These concepts will be explored in depth in the following notebooks. For now let's just enumerate some specific examples of why N\n",
    "-grams might be useful:\n",
    "\n",
    "- By comparing the frequency of each N-gram on two different texts, we can calculate the similarity between them\n",
    "- By looking at the frequency of the 2-gram 'Very good' we may have an indicator the sentiment associated with the text. On the other hand, solely looking at the frequency of the unigrams 'Very' and 'good' might not convey the same meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping it up\n",
    "\n",
    "And you've reached the end of our first notebook, congratulation! You've learned the following concepts:\n",
    "\n",
    "* regex\n",
    "* tokenization\n",
    "* stemming\n",
    "* stop words\n",
    "* n-grams\n",
    "\n",
    "\n",
    "It may seem overwhelming for now, but you'll see everything will become more intuitive as you navigate trough your journey into the NLP world! So keep the motivation and see you in Part 2!\n",
    "\n",
    "<img src=\"./media/info_everywhere.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\[1\\] - [RegExr](https://regexr.com/3lvai)\n",
    "\n",
    "\\[2\\] - [Python Module of the Week](https://pymotw.com/2/re/)\n",
    "\n",
    "\\[3\\] - [NLTK Book](https://www.nltk.org/book/)\n",
    "\n",
    "\\[4\\] - [N-grams](https://en.wikipedia.org/wiki/N-gram#n-gram_models)\n",
    "\n",
    "\\[5\\] - [Language Model](https://en.wikipedia.org/wiki/Language_model)\n",
    "\n",
    "\\[6\\] - [Stanford CS124 Language Modeling slides](https://web.stanford.edu/class/cs124/lec/lm2021.pdf)\n",
    "\n",
    "\\[7\\] - [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Regex: [RegexOne](https://regexone.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
