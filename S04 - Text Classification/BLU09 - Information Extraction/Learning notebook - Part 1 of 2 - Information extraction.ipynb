{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Information Extraction\n",
    "\n",
    "Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents. Information Extraction can be a task like:\n",
    "\n",
    "* **Named Entity Recognition**, retrieve entities (like Person, Location, etc.) in the text. \n",
    "* **Relation Extraction**, find the relation between two entities in the text.\n",
    "* **Template Filling**, find the correct entity to fill a certain template.\n",
    "\n",
    "In this BLU, we are going to learn some of the basic techniques to extract specific information from textual sources. We are going to focus on the task of **named-entity recognition (NER)** where our objective is to **retrieve all the mentions** of entities like people, location, time, etc.\n",
    "\n",
    "We will first try a simple approach with regular expressions and then a more sophisticated one using SpaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "[1. Information Extraction with Regular Expressions](#1.-Information-Extraction-with-Regular-Expressions)   \n",
    "[2. Deeper look into information extraction using SpaCy](#2.-Deeper-look-into-information-extraction-using-SpaCy)   \n",
    "&emsp;[2.1 SpaCy 101](#2.1-SpaCy-101)   \n",
    "&emsp;[2.2 Information extraction with SpaCy](#2.2-Information-extraction-with-SpaCy)   \n",
    "&emsp;&emsp;[2.2.1 Named entity recognition](#2.2.1-Named-entity-recognition)   \n",
    "&emsp;&emsp;[2.2.2 Matcher](#2.2.2-Matcher)   \n",
    "&emsp;&emsp;[2.2.3 Information extraction with complex patterns](#2.2.3-Information-extraction-with-complex-patterns)   \n",
    "[3. Further directions and reading](#3.-Further-directions-and-reading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![robot entities](./media/robot_entities.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work on a corpus containing forum discussions. We extracted a sample from Reddit for this use. For more interesting examples, you may find more textual data available at https://files.pushshift.io/reddit/\n",
    "\n",
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read 1000 documents\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "with open('./datasets/sample_data.json') as fp:\n",
    "    for line in fp:\n",
    "        entry = json.loads(line)\n",
    "        docs.append(entry['body'])\n",
    "        \n",
    "print('I read {} documents'.format(len(docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Information Extraction with Regular Expressions\n",
    "\n",
    "In BLU07, we became pros of regular expressions. In this BLU, we're going to try to use them for entity recognition. Take a moment to think about all the possibilities of entities that we can find in a text. Do you think such a task is achievable using only regular expressions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![regex](./media/regex.gif \"regex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, let's say that you need to retrieve all the **dates** mentioned in our sample corpus. We learned in BLU07 that, if we follow a certain pattern for the dates, it is easy to use a regular expression to extract them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['14/09/30', '7/12/2007', '4/16/2007', '3/27/2007', '2/28/2007']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's find all possible dates in the format xx/xx/xxxx\n",
    "data = ' '.join(docs)\n",
    "re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this looks like it's going to be a breeze. Next task is to retrieve all the **country names** from the corpus.\n",
    "\n",
    "One possible approach is to get a list of all countries that exist and then look for the occurence of such elements in the corpus. Let's try that, shall we?\n",
    "\n",
    "![alt text](./media/countries_meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = []\n",
    "with open('./datasets/countries.txt') as fp:\n",
    "    for line in fp:\n",
    "        countries.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll try to use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('us', 763, 765)\n",
      "('United States', 827, 840)\n",
      "('UK', 6971, 6973)\n",
      "('US', 7000, 7002)\n",
      "('Puerto rico', 8026, 8037)\n",
      "('us', 8638, 8640)\n",
      "('France', 19815, 19821)\n",
      "('us', 21563, 21565)\n",
      "('Puerto Rico', 27659, 27670)\n",
      "('Puerto Rico', 27754, 27765)\n",
      "('US', 28101, 28103)\n",
      "('Canada', 29439, 29445)\n",
      "('USA', 32880, 32883)\n",
      "('Norway', 34749, 34755)\n",
      "('Korea', 34837, 34842)\n",
      "('USA', 35738, 35741)\n",
      "('United States', 41060, 41073)\n",
      "('us', 42290, 42292)\n",
      "('us', 42403, 42405)\n",
      "('Soviet', 44563, 44569)\n",
      "('us', 49625, 49627)\n",
      "('Chad', 51352, 51356)\n"
     ]
    }
   ],
   "source": [
    "# Sort country list by length. This is important to match longer names before short \n",
    "# ones (like in 'Papua New Guinea' vs. 'Papua')\n",
    "countries.sort(key=len, reverse=True)\n",
    "\n",
    "# Make a regex to recognize all possible names.\n",
    "# '|' stands for the logical OR operation\n",
    "# \\b means word boundaries (punctuation or white spaces)\n",
    "# re.escape returns all the non-alphanumeric characters backslashed to avoid \n",
    "# their misinterpretation as regex metacharacters\n",
    "countries_regex = r'\\b(' + '|'.join([re.escape(c) for c in countries]) + r')\\b'\n",
    "\n",
    "# finditer is similar to findall\n",
    "# the flag re.I means to ignore casing (accept both lowercase and uppercase letters as the same)\n",
    "for i, m in enumerate(re.finditer(countries_regex, data, flags=re.I)):\n",
    "    print( (m.group(), m.start(), m.end()) )\n",
    "    # just show the first 20\n",
    "    if i > 20:\n",
    "        break    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is this approach working?**\n",
    "\n",
    "It seems like the word **'us'**, for example, has caused some confusion. It could be the country _U.S._ or the pronoun _us_. In this case, we are not able to disambiguate the two forms by just comparing the word form. We will need either more **context** or more **linguistic information** and regular expressions won't give us none of that.\n",
    "\n",
    "Luckily, you already know an NLP library which can provide the correct information to disambiguate the word 'us'. In the next examples, we will use SpaCy as our NLP toolkit to give us just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deeper look into information extraction using SpaCy\n",
    "\n",
    "<img src=\"media/spacy.jpg\" alt=\"Spacy\" width=\"600\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember BLU08, we used SpaCy to understand word vectors (aka word embeddings). We will make use of the medium sized SpaCy English model once again. If you haven't downloaded it yet (so the code in the following cell throws an error), you can simply use this command in a Notebook cell:\n",
    "\n",
    "```\n",
    "!python -m spacy download en_core_web_md\n",
    "```\n",
    "   \n",
    "SpaCy provides English models of different sizes - small, medium, large (en_core_web_sm, en_core_web_md, en_core_web_lg) - you can use the one that suits your needs and memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 SpaCy 101\n",
    "SpaCy is a NLP library designed for use in production. That means that it is designed to get things done instead of playing around, so it doesn't let you choose between many different algorithms like libraries designed for research and teaching, e.g. NLTK and CoreNLP.\n",
    "\n",
    "SpaCy provides way more information than the word counting algorithms we were using until now. SpaCy sees the forest, not just the trees. It provides information derived from context, like is this word a noun or a verb? What is the word next to this word? Which role has this word in the sentence, is it a subject or an object? (Yes, you'll have to remember your language classes to use SpaCy). It can also recognize people and places, so-called named entities.\n",
    "\n",
    "SpaCy contains pretrained models for different languages and also the option to pretrain your own model on your own corpus data. This is important if your text is very specific. For instance, if you'd like to analyze text from social media, a model trained on romantic novels will not work that well because these two types of language data will have a very different vocabulary and sentence structure.\n",
    "\n",
    "This is the scheme of SpaCy's processing [pipeline](https://spacy.io/usage/processing-pipelines) that we loaded above. It consists of a tokenizer and other modules which add information about the tokens. The modules can be based on a model or on a set of rules. In a model based module, the results are predicted, whereas in s rule based one, they are determined following some rule. For instance, a lemmatizer can determine the base form of words based on grammatical rules.\n",
    "\n",
    "<img src=\"media/spacy_pipeline.png\" alt=\"Spacy_pipeline\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the pipeline components one by one:\n",
    "- The **tokenizer** segments the input text into tokens and converts it into a `Doc` object. All further steps operate on this object and also output it.\n",
    "- The **lemmatizer** outputs the base form of each token.\n",
    "- The **tagger** assigns part-of-speech (POS) tags like verb, proper noun, adjective.\n",
    "- The **morphologizer** predicts coarse-grained POS and morphological information like singular or plural, verb tense, first/second/third person.\n",
    "- The **parser** predicts syntactic information for each token - its role in the sentence like subject, direct object, determiner.\n",
    "- The **named entities recognizer** (NER) recognizes real world objects like countries or companies.\n",
    "- The **entity linker** links named entities to a knowledge base, e.g. the 'first female programmer' and 'Ada Lovelace' link to the same item in the knowledge base.\n",
    "- The **Tok2Vec** assigns a word vector to each token.\n",
    "- The **matcher** is a like a sophisticated regex that uses also lingustic information.\n",
    "- The **sentence boundary recognizer** detects sentences.\n",
    "- The **text categorizer** predicts labels for whole chunks of text.\n",
    "- Any **custom components** that a user can add.\n",
    "\n",
    "The information from each processing step is added as attributes to each token. Let's look at an example (shamelessly copied from the SpaCy documentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj NNP Xxxxx False Number=Sing\n",
      "is AUX aux VBZ xx True Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "looking VERB ROOT VBG xxxx False Aspect=Prog|Tense=Pres|VerbForm=Part\n",
      "at ADP prep IN xx True \n",
      "buying VERB pcomp VBG xxxx False Aspect=Prog|Tense=Pres|VerbForm=Part\n",
      "U.K. PROPN compound NNP X.X. False Number=Sing\n",
      "startup NOUN dobj NN xxxx False Number=Sing\n",
      "for ADP prep IN xxx True \n",
      "$ SYM quantmod $ $ False \n",
      "1 NUM compound CD d False NumType=Card\n",
      "billion NUM pobj CD xxxx False NumType=Card\n"
     ]
    }
   ],
   "source": [
    "# Process a text with the nlp pipeline, the result is a Doc object\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "# Show the token and some of its attributes - part-of-speech and syntactic dependency labels\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.tag_, token.shape_, token.is_stop, token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some attribute names end in an underscore. SpaCy stores its attributes as hashes to minimize memory usage. When you use an attribute name, it will show the hash. To see the attribute in a human readable form, add the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Apple', 15794550382381185553, 'NNP')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0].text, doc[0].tag, doc[0].tag_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, according to SpaCy, 'Apple' in this sentence is a proper noun (PROPN) and a nominal subject (nsubj). But what is NNP? Fortunately, SpaCy is nice and explains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'noun, proper singular'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"NNP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full list of attribute values that SpaCy can explain, look [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py).\n",
    "\n",
    "We also got the information about the word shape and if it's a stopword. This is just a small selection of token attributes, see the whole list [here](https://spacy.io/api/token).\n",
    "\n",
    "<img src=\"media/pronoun.jpg\" alt=\"Pronoun meme\" width=\"300\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Information extraction with SpaCy\n",
    "We will first process the documents with the complete NLP pipeline using the [`pipe`](https://spacy.io/api/language#pipe) method. It will process our text, tokenize it and extract information from it using all the CPU cores on our machine. The output of `pipe` is a generator, so we will convert it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use the function pipe to process all documents.\n",
    "# One of the strenghts for SpaCy is the parallel processing using all your computer cores.\n",
    "\n",
    "docs = list(nlp.pipe(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take one of the processed Docs. We want to detect country names, so we will look at the named entities detected by the NER pipeline component. This is our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was more like $10 million dollars after the previous government infringed on his rights within the Charter of Rights and Freedoms (basically the Canadian constitution). Trudeau knew Khadr would win in court, and settled for paying $10 million instead of an amount multiple times more. \n",
      "\n",
      "Not saying I'm happy with Khadr getting $10 million, but this was more of a fuck up on the previous government since they blatantly violated his rights as a Canadian.\n"
     ]
    }
   ],
   "source": [
    "example = docs[250]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Named entity recognition\n",
    "The `.ents` attribute of the `Doc` object holds the information about the [named entities](https://spacy.io/usage/linguistic-features#named-entities). Here we show the entities, the place where they start and end in the text, and their category. For the complete list of categories, look [here](https://github.com/explosion/spaCy/blob/master/spacy/glossary.py#L326) or we can also ask SpaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.get_pipe(\"ner\").labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more like $10 million dollars 7 36 MONEY\n",
      "the Charter of Rights and Freedoms 98 132 LAW\n",
      "Canadian 148 156 NORP\n",
      "Trudeau 172 179 PERSON\n",
      "Khadr 185 190 PERSON\n",
      "$10 million 234 245 MONEY\n",
      "Khadr 316 321 PERSON\n",
      "$10 million 330 341 MONEY\n",
      "Canadian 447 455 NORP\n"
     ]
    }
   ],
   "source": [
    "for ent in example.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy correctly labeled the entities in our example. 'Trudeau' and 'Khadr' are PERSON entities, 'Canadian' is a NORP entity (Nationalities or religious or political groups)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a named entities category `GPE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll take these and use the `Matcher` to sort out the countries.\n",
    "\n",
    "#### 2.2.2 Matcher\n",
    "A `Matcher` is SpaCy's version of a regular expression - it searches for patterns in your text according to the rules you give it. However, it is much more powerful since it has access to the outputs of the aforementioned NLP pipeline. That means we can search patterns that include certain named entities or part-of-speech tags. \n",
    "\n",
    "The `Matcher` is initialized with the vocabulary object from the pipeline which we used to analyze the documents. The vocabulary object is just what it sounds like - a list of words with some information about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab) # Pass the vocabulary object to Matcher.__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to regex, `Matcher` operates with patterns. We will define a pattern for country detection using the list of country names. We will define one pattern for each country and pass it to `Matcher` with the `add` method. Notice that each added pattern has a name or ID, like this `matcher.add(ID, pattern)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in countries:\n",
    "    # Build a pattern from the country name. \n",
    "    # For example: United States -> [{'LOWER': 'united'}, {'LOWER': 'states'}]\n",
    "    # LOWER means to match the words in the lowercased token.\n",
    "    pattern = [[{'LOWER': c.lower()} for c in country.split()]]\n",
    "    matcher.add(country, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the pattern on the whole list of documents and show the matches. The first number is the document number, followed by the beginning and end of the matched span."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 2 us\n",
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "69 50 51 us\n",
      "146 4 5 France\n",
      "167 29 30 us\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "320 35 36 us\n",
      "320 58 59 us\n",
      "335 38 39 Soviet\n",
      "335 38 39 Soviet\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "# for screen economy, let's just show the matches for the first 400 documents.\n",
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we used `Matcher` just like a regex and of course the results are the same. Let's now take it up a notch and also use the linguistic information. \n",
    "\n",
    "We are interested in matching the country names that were tagged as **Proper Nouns** ('PROPN' POS tag). A Proper Noun is a specific (i.e., not generic) name for a particular person, place, or thing. We will add a `'POS'` entry to the pattern dictionary with the `'PROPN'` tag as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Proper Noun.\n",
    "    pattern = [[{'LOWER': c.lower(), 'POS': 'PROPN'} for c in country.split()]]\n",
    "    matcher.add(country, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we look at matches in the first 400 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 4 6 United States\n",
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "146 4 5 France\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "229 4 5 Canada\n",
      "255 86 87 USA\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "312 4 6 United States\n",
      "349 4 5 us\n",
      "367 7 8 Chad\n",
      "369 11 12 Chad\n",
      "369 18 19 Chad\n",
      "369 41 42 Chad\n",
      "386 4 6 United States\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! We only see one incorrectly matched 'us' now. We could also try out another pattern, matching entity types instead of POS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new matcher instance\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "for country in countries:\n",
    "    # same as before, but now with one more restriction: the Part-of-speech should be a Proper Noun.\n",
    "    pattern = [[{'LOWER': c.lower(), 'ENT_TYPE': 'GPE'} for c in country.split()]]\n",
    "    matcher.add(country, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 22 23 UK\n",
      "58 28 29 US\n",
      "64 18 20 Puerto rico\n",
      "213 99 101 Puerto Rico\n",
      "213 121 123 Puerto Rico\n",
      "213 198 199 US\n",
      "263 80 81 Norway\n",
      "263 103 104 Korea\n",
      "267 2 3 USA\n",
      "367 7 8 Chad\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start:end]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the list is even shorter. Why do you think this is? We can look at the surrounding words of the matched token to understand it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 22 23 shameless where the UK version is better\n",
      "58 28 29 better but the US version exists.\n",
      "64 18 20 white people in Puerto rico. Its a\n",
      "213 99 101 his tweets about Puerto Rico from the past\n",
      "213 121 123 number of his Puerto Rico related tweets in\n",
      "213 198 199 tax code for US, The American\n",
      "263 80 81 [EU] Norway DMG looking to\n",
      "263 103 104 [ASIA] Korea, looking for\n",
      "267 2 3 \n",
      "367 7 8 was not a Chad you're fucking\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs[:400]):\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        string_id = nlp.vocab.strings[match_id] \n",
    "        span = doc[start-3:end+3]\n",
    "        print(i, start, end, span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Information extraction with complex patterns\n",
    "\n",
    "Let's now look into other types of information extraction methods which use complex structures. For example, let's say we want to extract places. Usually, places come up in text in structures similar to:\n",
    "\n",
    "* go to xx\n",
    "* went from xxx\n",
    "* going to xx\n",
    "\n",
    "Such patterns could also be interesting for the task of relation extraction we mentioned in the intro.\n",
    "\n",
    "In order to build a SpaCy pattern for the proposed sentence structure, we are going to use the lemma 'go' which is invariant for all possible verb inflections, a preposition (POS tag ADP) and a proper noun (POS tag PROPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [[{'LEMMA': 'go'}, {'POS': 'ADP'}, {'POS': 'PROPN'}]]\n",
    "matcher.add('LOC', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 27 goes to GTA\n",
      "246 249 going to Osaka\n",
      "81 84 gone to Irvine\n",
      "91 94 going with Robbie\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]  # the matched span\n",
    "        span_text = span.text  # the span as a string\n",
    "        print(start, end, span_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sure aren't all the locations that are present in our corpus and we also got some false positives! Not what we expected then 🙃 Anyway, the possibilities of `Matcher` are [endless](https://spacy.io/usage/rule-based-matching) and this kind of patterns might work in simpler situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Further directions and reading\n",
    "\n",
    "[SpaCy 101](https://spacy.io/usage/spacy-101) - a good, concise introduction to SpaCy\n",
    "\n",
    "If you run into the limits of the pretrained models, you can [train your own model on your own corpus](https://spacy.io/usage/training).\n",
    "\n",
    "Guide to all [linguistic features](https://spacy.io/usage/linguistic-features) used by SpaCy.\n",
    "\n",
    "Another possible way to go is to annotate examples in a corpus. We can train machine learning systems from scratch to automatically extract patterns from annotated corpora. This class of machine learning methods is known as sequencial labeling and the most famous approaches are [CRFs](https://people.cs.umass.edu/~wallach/technical_reports/wallach04conditional.pdf) and [Seq2seq](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf) and now most recently [Transformers](https://jalammar.github.io/illustrated-bert/). You are not expected to know what these models do since they are too complex right now for you to understand, but it's good to keep them in mind and maybe play a little bit if you wish 😊\n",
    "\n",
    "And of course, the [large language models](https://developers.google.com/machine-learning/resources/intro-llms) which you most likely heard of or interacted with already."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
