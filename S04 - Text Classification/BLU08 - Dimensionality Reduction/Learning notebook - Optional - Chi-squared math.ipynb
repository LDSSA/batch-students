{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional learning notebook about chi-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual chi-squared for the Twitter data\n",
    "Let's make functions to apply the manual $\\chi^2$ calculation to the Twitter data. Recall that $\\chi^2 = \\sum{\\frac{(O_{x_1x_2} - E_{x_1x_2})^2}{E_{x_1x_2}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(counts):\n",
    "    \"\"\"\n",
    "    Non vectorized version of chi squared function - the idea is that you see the relation with the formula above, \n",
    "    but you should never use such an inefficient version when actually performing a chi-squared analysis\n",
    "    \"\"\"\n",
    "    print(\"Applying chi-squared to {} feature and {} classes\".format(counts.shape[0], counts.shape[1]))\n",
    "    chi_values = np.zeros(counts.shape)\n",
    "    for i in range(counts.shape[0]):\n",
    "        for j in range(counts.shape[1]):\n",
    "            n = counts.sum()\n",
    "            c_tc = counts[i,j]\n",
    "            c_tx = counts.sum(axis=1)[i,0]-c_tc\n",
    "            c_xc = counts.sum(axis=0)[0,j]-c_tc\n",
    "            c_xx = n-c_tc-c_tx-c_xc\n",
    "            chi_values[i,j] = n*(((c_tc*c_xx)-(c_tx*c_xc))**2)/((c_tc+c_xc)*(c_tx+c_xx)*(c_tc+c_tx)*(c_xc+c_xx))\n",
    "    return chi_values\n",
    "\n",
    "def chi_squared_vect(counts):\n",
    "    \"\"\"\n",
    "    Vectorized version of chi squared function - this is still a non-optimized version, but it should run faster than \n",
    "    the previous function\n",
    "    \"\"\"\n",
    "    print(\"Applying chi-squared to {} feature and {} classes\".format(counts.shape[0], counts.shape[1]))\n",
    "    n = counts.sum()\n",
    "    c_tc = counts\n",
    "    c_tx = counts.sum(axis=1)-counts\n",
    "    c_xc = counts.sum(axis=0)-counts\n",
    "    c_xx = n * np.ones(counts.shape) - counts - c_tx - c_xc\n",
    "    num = n * np.square(np.multiply(c_tc, c_xx)-np.multiply(c_tx, c_xc))\n",
    "    den = np.multiply(np.multiply(np.multiply(c_tc+c_xc, c_tx+c_xx), c_tc+c_tx), c_xc+c_xx)\n",
    "    chi_values = np.divide(num, den)\n",
    "    return chi_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll apply our functions to the data and select the most important features. The features with higher chi2 values are the more important ones, i.e, the ones that are more dependent on the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_df = pd.read_csv('./data/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "hashtag_removal = lambda doc: re.subn(r'#\\w+','', doc.lower())[0]\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "\n",
    "stat_df['Tweet'] = stat_df['Tweet'].map(handle_removal)\n",
    "stat_df['Tweet'] = stat_df['Tweet'].map(hashtag_removal)\n",
    "stat_df['Tweet'] = stat_df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(stat_df, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying chi-squared to 131301 feature and 2 classes\n",
      "Most important features and their chi-squared:\n",
      "\n",
      "chairman: 25.628925195452286\n",
      "code: 12.98547983333508\n",
      "gun: 11.882957002831544\n",
      "hearing: 14.628669701803808\n",
      "pruitt: 9.71117632814275\n",
      "reform: 14.129548514719188\n",
      "tax: 22.00286461780654\n",
      "tax code: 11.748658006980634\n",
      "tax reform: 14.235518157444455\n",
      "texas: 14.780349285278556\n"
     ]
    }
   ],
   "source": [
    "small_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "small_vectorizer.fit(train_data.Tweet)\n",
    "small_X_train = small_vectorizer.transform(train_data.Tweet)\n",
    "small_y_train = train_data.Party\n",
    "\n",
    "idx_rep = np.where(small_y_train=='Republican') \n",
    "idx_dem = np.where(small_y_train=='Democrat') \n",
    "\n",
    "counts_rep = small_X_train[idx_rep[0], :].sum(axis=0)\n",
    "counts_dem = small_X_train[idx_dem[0], :].sum(axis=0)\n",
    "counts = np.concatenate((counts_rep, counts_dem))\n",
    "\n",
    "chi_values_vect = chi_squared_vect(counts.transpose())\n",
    "\n",
    "feature_names = small_vectorizer.get_feature_names_out()\n",
    "\n",
    "best_features = chi_values_vect.argsort(axis=0).tolist()\n",
    "\n",
    "print(\"Most important features and their chi-squared:\\n\")\n",
    "for idx in sorted(best_features[-10:]):\n",
    "    print(u\"{}: {}\".format(feature_names[idx[0]], chi_values_vect[idx[0], 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we got the same results as sklearn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generalization for a 2x2 contingency table\n",
    "\n",
    "In some places you will see that the chi-squared formula for a 2x2 contingency table can be written in the form\n",
    "\n",
    "$$ \\frac{N(AD-BC)^2}{(A+B)(A+C)(B+D)(C+D))} $$\n",
    "\n",
    "if we represent the values in the table like this:\n",
    "\n",
    "|                      | C_1     |     C_2      |    Total    |\n",
    "|----------------------|---------|--------------|-------------|\n",
    "|         F_1          |   A     |      B       |  A+B        |\n",
    "|         F_2          |   C     |      D       |  C+D        |\n",
    "|         total        |   A+C   |      B+D     |  N=A+B+C+D  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, remember that the expected value for a cell, let's say $E_A$, is computed from the $P(C_1)$ and $P(F_1)$, like this:\n",
    "\n",
    "$$ E_A = N\\ P(F_1)P(C_1) = \\quad N\\ \\frac{A+B}{N}\\frac{A+C}{N} =\\quad \\frac{(A+B)(A+C)}{N}$$\n",
    "\n",
    "So taking the difference $O_A - E_A$ simplifies to:\n",
    "\n",
    "$$ O_A - E_A = \\\\ A - \\frac{(A+B)(A+C)}{N} = \\\\ A - \\frac{A^2+AB + AC + BC \\ (+ AD - AD)}{N} = \\\\\n",
    "A - \\frac{A\\ (A + B + C + D) + (BC - AD)}{N} = \\\\ A - \\frac{A\\ N + (BC - AD)}{N} = \\\\ A - A + \\frac{(BC - AD)}{N} = \\\\\n",
    "\\frac{(BC - AD)}{N}$$\n",
    "\n",
    "If you write down the expression for cells B, C and D, you will get:\n",
    "\n",
    "$$ E_A = \\frac{(A+B)(A+C)}{N} \\\\ E_B = \\frac{(B+D)(A+B)}{N} \\\\ E_C = \\frac{(A+C)(C+D)}{N}\\\\ E_D = \\frac{(B+D)(C+D)}{N}$$\n",
    "\n",
    "\n",
    "$$ (O_A - E_A) = (O_C - E_C) = \\frac{(BC - AD)}{N} \\\\ (O_B - E_B) = (O_D - E_D) = \\frac{(AD - BC)}{N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And putting it all together we get\n",
    "\n",
    "$$ \\chi^2 = \\frac{(O_A - E_A)^2}{E_A} + \\frac{(O_B - E_B)^2}{E_B} + \\frac{(O_C - E_C)^2}{E_C} + \\frac{(O_D - E_D)^2}{E_D} $$\n",
    "\n",
    "$$ \\chi^2 = \\frac{(\\frac{(BC - AD)}{N})^2}{\\frac{(A+B)(A+C)}{N}} + \\frac{(\\frac{(AD - BC)}{N})^2}{\\frac{(B+D)(A+B)}{N}} + \\frac{(\\frac{(AD - BC)}{N})^2}{\\frac{(A+C)(C+D)}{N}} + \\frac{(\\frac{(BC - AD)}{N})^2}{\\frac{(B+D)(C+D)}{N}} $$\n",
    "\n",
    "$$ \\chi^2 = \\frac{(B+D)(C+D)(\\frac{(BC - AD)}{N})^2}{\\frac{(A+B)(A+C)(B+D)(C+D)}{N}} + \\frac{(A+C)(C+D)(\\frac{(AD - BC)}{N})^2}{\\frac{(A+B)(A+C)(B+D)(C+D)}{N}} + \\frac{(A+B)(B+D)(\\frac{(AD - BC)}{N})^2}{\\frac{(A+B)(A+C)(B+D)(C+D)}{N}} + \\frac{(A+B)(A+C)(\\frac{(BC - AD)}{N})^2}{\\frac{(A+B)(A+C)(B+D)(C+D)}{N}} $$\n",
    "\n",
    "Notice that\n",
    "\n",
    "$$(BC - AD)^2 = ((-1)(AD - BC))^2 = (AD - BC)^2$$\n",
    "\n",
    "so the expression simplifies to\n",
    "\n",
    "$$ \\frac{N [(B+D)(C+D) + (A+C)(C+D) + (A+B)(B+D) + (A+B)(A+C)](\\frac{(AD - BC)}{N})^2}{(A+B)(A+C)(B+D)(C+D)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The denominator is already there, let's simplify the numerator.\n",
    "\n",
    "$$ \\frac{N [BC + BD + CD + D^2 + AC + AD + C^2 + CD + AB + AD + B^2 + BD + A^2 + AC + AB + BC](\\frac{(AD - BC)}{N})^2}{(A+B)(A+C)(B+D)(C+D)} $$\n",
    "\n",
    "$$ \\frac{N [A ( A + B + C + D ) + B (A + B + C + D) + C (A + B + C + D) + D (A + B + C + D)](\\frac{(AD - BC)}{N})^2}{(A+B)(A+C)(B+D)(C+D)} $$\n",
    "\n",
    "$$ \\frac{N [( A + B + C + D ) ( A + B + C + D )](\\frac{(AD - BC)}{N})^2}{(A+B)(A+C)(B+D)(C+D)} $$\n",
    "\n",
    "$$ \\frac{N (N^2)(\\frac{(AD - BC)}{N})^2}{(A+B)(A+C)(B+D)(C+D)} $$\n",
    "\n",
    "$$ \\frac{N (N^2)(\\frac{(AD - BC)^2}{N^2})}{(A+B)(A+C)(B+D)(C+D)} $$\n",
    "\n",
    "$$ \\frac{N (AD - BC)^2}{(A+B)(A+C)(B+D)(C+D)} $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
