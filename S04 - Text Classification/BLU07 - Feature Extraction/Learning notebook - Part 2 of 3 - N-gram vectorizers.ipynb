{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "from numpy import inf\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look at some Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After learning all about tokenization and regexes, let's start doing some cool stuff and apply it to a true dataset!\n",
    "\n",
    "In Part II of this BLU, we're going to look into how to transform text into something that is meaningful to a machine. As you may have noticed, text data is a bit different from other datasets you might have seen -- it's just a bunch of words strung together! Where are the features in a tightly organized table of examples? Unlike other data you might have worked with in previous BLUs, text is unstructured and thus needs some additional work on our end to make it structured and ready to be handled by a machine learning algorithm.\n",
    "\n",
    "Language can be messy, but one thing is clear: we need features. To get features from a string (text or a **document**), one way is to **vectorize** it. Normally, this means that our feature space is the **vocabulary** of the examples present in our dataset. That is, the set of unique words we can find in all of the training examples.\n",
    "\n",
    "<img src=\"./media/vectors.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "But enough talk - let's get our hands dirty!\n",
    "\n",
    "In this BLU, we're going to work with some movie reviews from IMDB. Let's load the dataset into pandas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Aldolpho (Steve Buscemi), an aspiring film mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>An unfunny, unworthy picture which is an undes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A failure. The movie was just not good. It has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this movie Sunday afternoon. I absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney goes to the well one too many times as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  Negative  Aldolpho (Steve Buscemi), an aspiring film mak...\n",
       "1  Negative  An unfunny, unworthy picture which is an undes...\n",
       "2  Negative  A failure. The movie was just not good. It has...\n",
       "3  Positive  I saw this movie Sunday afternoon. I absolutel...\n",
       "4  Negative  Disney goes to the well one too many times as ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two columns in this dataset - one for the labels and another for the text of the movie review. Each example is labeled as a positive or negative review. Our goal is to retrieve meaningful features from the text, so a machine learning model can predict if a given unlabeled review is positive or negative.\n",
    "\n",
    "Let's see a positive and a negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "\"The Lion King\" is without a doubt my favorite Disney movie of all time, so I figured maybe I should give the sequels a chance and I did. Lion King 1 1/2 was pretty good and had it's good laughs and fun with Timon and Pumba. Only problem, I feel sometimes no explanations are needed because they can create plot holes and just the feeling of wanting your own explanation. Well, I would highly recommend this movie for lion King fans or just a night with the family. It's a fun flick with the same laughs and lovable characters as the first. So, hopefully, I'll get the same with the third installment to the Lion King series. Sit back and just think Hakuna Matata! It means no worries! <br /><br />8/10\n"
     ]
    }
   ],
   "source": [
    "pos_example = df.text[4835]\n",
    "print(df.sentiment[4835])\n",
    "print(pos_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So that is a review about *The Lion King 1 1/2* (a.k.a. *The Lion King 3* in some countries). It seems the reviewer liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Disney goes to the well one too many times as anybody who has seen the original LITTLE MERMAID will feel blatantly ripped off. Celebrating the birth of their daughter Melody, Ariel and Eric plan on introducing her to King Triton. The celebration is quickly crashed by Ursula 's sister, Morgana who plans to use Melody as a defense tool to get the King 's trident. Stopping the attack, Ariel and Eric build a wall around the ocean while Melody grows up wondering why she cannot go in there.<br /><br />Awful and terrible is what describes this direct to video sequel. LITTLE MERMAID 2 gives you that feeling everything you watch seemed to have come straight other Disney movies. I guess Disney can only plagiarize itself! Do not tell me that the penguin and walrus does not remind you of another duo from the LION KING!<br /><br />Other disappointing moments include the rematch between Sebastien and Louie, the royal chef. They terribly under played it! The climax between Morgana and EVERYONE seemed to be another disappointment.<br /><br />I will not give anything away, but in 75 minutes, everything seemed incredibly cramped and too much to handle. An embarrassment to Disney, LITTLE MERMAID 2 is better left to rent and laugh at. Then you can prepare for the rest of the other sequels Disney is going to drown you in later on.\n"
     ]
    }
   ],
   "source": [
    "neg_example = df.text[4]\n",
    "print(df.sentiment[4])\n",
    "print(neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. I guess that's a pass for this one, right?\n",
    "\n",
    "Let's get the first 200 documents of this dataset to run experiments faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned in Learning Notebook - Part 1, we can tokenize and stem our text to extract features. Let's initialize our favorite tokenizer and stemmer. For now, we choose to keep stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: using `ignore_stopwords=True` prevents the stemming of stopwords, if they are present in the sentence.\n",
    "\n",
    "Before tokenization and stemming, it is important to first clean our sentences. We can see from the examples above that our corpus has some HTML substrings `<br />` that are only adding meaningless noise to the sentences. As we've seen in Part 1, we can remove these HTML tags with `re.sub()` by substituting every substring that matches the regex `<[^>]*>` with an empty string.\n",
    "\n",
    "We will define a `preprocess()` method that removes these unnecessary HTML tags, tokenizes, and stems our corpus's sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    # string.punctuation is a utility that allows us to not have to define all punctuation characters by ourselves\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### A small note on punctuation removal\n",
    "\n",
    "Note that above, we've used `string.punctuation` instead of defining a list of characters. This is a handy constant provided by the `string` package that we can use instead of defining our own punctuation regex pattern. \n",
    "\n",
    "You can see below the characters included:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't cover everything (remember the quotation marks from the previous unit), but it is still a nice utility to use as the base for punctuation removal.\n",
    "\n",
    "In our preprocess function, we're using it in a list comprehension to remove single punctuation characters from our list of words returned by the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', '?', 'Yes', ',', 'it', 'is', 'a', 'test', '.']\n",
      "['Is', 'this', 'a', 'test', 'Yes', 'it', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? Yes, it is a test.\"\n",
    "\n",
    "# Before removing punctuation\n",
    "print([word for word in tokenizer.tokenize(text)])\n",
    "\n",
    "# After removing punctuation\n",
    "print([word for word in tokenizer.tokenize(text) if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's easy to find examples where this is suboptimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'No', 'it', 'isn', 't', '...']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "print([word for word in tokenizer.tokenize(text) if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `...` wasn't considered as punctuation under this rule. What you could do instead is to use regex combined with this utility to make sure sentences are completely cleaned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'No', 'it', 'isn', 't']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "\n",
    "sentence = \" \".join(tokenizer.tokenize(text))\n",
    "\n",
    "print(re.sub(pattern, '', sentence).split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down:\n",
    "\n",
    "- First, punctuation is transformed into a regex pattern.\n",
    "- Then, the text is tokenized and the tokens are saved in a string, separated by spaces.\n",
    "- `re.sub()` is applied to the string and removes all characters that are in the regex pattern. Since these characters include the `.` the last three dots are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "But back to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see one of the above examples again, after we cleaned the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disney goe to the well one too mani time as anybodi who has seen the origin littl mermaid will feel blatant rip off celebr the birth of their daughter melodi ariel and eric plan on introduc her to king triton the celebr is quick crash by ursula s sister morgana who plan to use melodi as a defens tool to get the king s trident stop the attack ariel and eric build a wall around the ocean while melodi grow up wonder why she cannot go in there aw and terribl is what describ this direct to video sequel littl mermaid 2 give you that feel everyth you watch seem to have come straight other disney movi i guess disney can only plagiar itself do not tell me that the penguin and walrus does not remind you of anoth duo from the lion king other disappoint moment includ the rematch between sebastien and louie the royal chef they terribl under play it the climax between morgana and everyon seem to be anoth disappoint i will not give anyth away but in 75 minut everyth seem incred cramp and too much to handl an embarrass to disney littl mermaid 2 is better left to rent and laugh at then you can prepar for the rest of the other sequel disney is go to drown you in later on'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we may not understand it very well now, but we actually just made the text much easier for a machine to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of words in the vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that many of the most common words in the reviews are what we would consider stopwords: determiners like \"the,\" \"a\"; prepositions like \"of,\" \"to\"; etc. We will probably want to filter these out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Text as Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned and tokenized our dataset, we need to find a way of making this information useful for a machine learning model. As you know, machine learning models can only deal with numerical data. Therefore, we need to find a way of summarizing information about text in numbers.\n",
    "\n",
    "There are a lot of ways of doing this, but the simplest one is called **Bag of Words (BoW)**!\n",
    "\n",
    "<img src=\"./media/bag_of_words.png\" width=\"600\">\n",
    "\n",
    "Bag of words is a type of document vectorization that consists in **word counting**. Each document is represented by a vector with the size of our vocabulary, and each feature is the number of times each word on the vocabulary appears in the document.\n",
    "\n",
    "By doing this, our data becomes structured and tabular and each column represents the number of times a word of the vocabulary appeared in the document, whereas each row corresponds to a document.\n",
    "\n",
    "Note that this type of vectorization of the document loses all of its syntactic information. That is, you could shuffle the words in the document and get the same vector (that's why it's called a bag of words). Of course, since we are trying to understand if a movie review is positive or negative, one could argue that simply looking at what kind of words appear in the document is enough to understand its feeling (positive or negative), regardless of the order of these words.\n",
    "\n",
    "Nevertheless, for other more difficult tasks this might not be enough, and a different representation of text that conserves the order of words might be needed. But let's focus on BoW for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "To transform our textual data into a vectorized Bag of Words, we first need to define our feature space.\n",
    "\n",
    "As mentioned before, the feature space of our text will be the vocabulary of our data. In our example, this is the set of unique words and symbols present in our documents.\n",
    "\n",
    "To create our vocabulary, we will use a `Counter()`. `Counter()` is a dictionary that counts the number of occurrences of different tokens in a list and can be updated with each sentence of our corpus.\n",
    "\n",
    "After getting all counts for each unique token, we sort our dictionary by counts using `Counter()`'s built-in method `.most_common()`, and store everything in an `OrderedDict()`. This makes sure our vectorized representations of the documents will be ordered according to the most common words in the whole corpus (this is not required, but makes data visualization much nicer!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2706),\n",
       " ('a', 1361),\n",
       " ('and', 1349),\n",
       " ('of', 1205),\n",
       " ('to', 1115),\n",
       " ('is', 815),\n",
       " ('it', 786),\n",
       " ('in', 719),\n",
       " ('i', 690),\n",
       " ('this', 594),\n",
       " ('that', 581),\n",
       " ('s', 541),\n",
       " ('movi', 459),\n",
       " ('film', 400),\n",
       " ('as', 377),\n",
       " ('but', 358),\n",
       " ('with', 357),\n",
       " ('for', 315),\n",
       " ('was', 305),\n",
       " ('t', 295)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "Now that we have our vocabulary, we can vectorize our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize():\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vector = np.array([doc.count(word) for word in build_vocabulary()])\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize this better if we use a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>in</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the   a  and  of  to  is  it  in   i  this  ...  championship  ...\"  \\\n",
       "0   11  94    7   4   7  16   5  11  76     3  ...             0     0   \n",
       "1    0   6    0   0   1   3   2   0  11     1  ...             0     0   \n",
       "2    8  48    6   3   1   6   3   5  35     1  ...             0     0   \n",
       "3    8  36    3   3   3   5   6   4  45     2  ...             0     0   \n",
       "4   21  70    9   4  16  13   6  15  73     1  ...             0     0   \n",
       "\n",
       "   endear  cortney  incid  erupt  semblanc  miser  shoe  mail  \n",
       "0       0        0      0      0         0      0     0     0  \n",
       "1       0        0      0      0         0      0     0     0  \n",
       "2       0        0      0      0         0      0     0     0  \n",
       "3       0        0      0      0         0      0     0     0  \n",
       "4       0        0      0      0         0      0     0     0  \n",
       "\n",
       "[5 rows x 5740 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectorize(), columns=vocab).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each document is a vector of the size of the vocabulary (5740 columns), and each feature corresponds to the number of times a word is in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned stopwords briefly in the last learning notebook, but now we will go a bit more in-depth. \n",
    "\n",
    "We're looking for the most meaningful features in our vocabulary to tell us in what category our document falls into. Text is filled with words that are unimportant to the meaning of a particular sentence like \"the\" or \"and\". This contrasts with words like \"love\" or \"hate\" that have a very clear semantic meaning. The former example of words are called **stopwords** - words that _usually_ don't introduce any meaning to a piece of text and are often just in the document for syntactic reasons.\n",
    "\n",
    "It is important to emphasize that we used \"usually\" in our previous statement. You should be aware that sometimes stopwords can be useful features, especially when we use more than just unigrams as features (ex.: bigrams, trigrams, ...), where word order and word combination starts to be relevant.\n",
    " \n",
    "You can find a list of English stopwords on the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yours',\n",
       " 'how',\n",
       " 'over',\n",
       " 'where',\n",
       " 'they',\n",
       " 'ourselves',\n",
       " 't',\n",
       " 'can',\n",
       " 'he',\n",
       " 'down',\n",
       " 'hadn',\n",
       " 'do',\n",
       " 'these',\n",
       " 'hers',\n",
       " 'because',\n",
       " 'into',\n",
       " 'then',\n",
       " 'shan',\n",
       " \"aren't\",\n",
       " 'his']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_eng = set(stopwords.words('english'))\n",
    "\n",
    "list(stop_eng)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's update our `build_vocabulary()` and `vectorize()` functions and remove these words from the text. This way we will reduce our vocabulary - and thus our feature space - making our representations more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_without_stopwords():\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split(' ') if word not in stop_eng]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "vocab_without_stopwords = build_vocabulary_without_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the new vocabulary (should be smaller than before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5595"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movi', 459),\n",
       " ('film', 400),\n",
       " ('one', 238),\n",
       " ('like', 204),\n",
       " ('time', 124),\n",
       " ('get', 118),\n",
       " ('watch', 110),\n",
       " ('make', 106),\n",
       " ('even', 105),\n",
       " ('see', 105),\n",
       " ('good', 99),\n",
       " ('stori', 99),\n",
       " ('charact', 98),\n",
       " ('end', 97),\n",
       " ('scene', 96),\n",
       " ('would', 94),\n",
       " ('well', 92),\n",
       " ('much', 92),\n",
       " ('peopl', 92),\n",
       " ('love', 86)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab_without_stopwords.items())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movi  film  one  like  time  get  watch  make  even  see  ...  \\\n",
       "0     3     5    0     0     2    1      1     3     0    0  ...   \n",
       "1     1     0    0     0     0    0      0     0     0    0  ...   \n",
       "2     6     0    2     1     0    0      0     0     1    0  ...   \n",
       "3     4     0    1     1     1    0      0     0     1    2  ...   \n",
       "4     1     0    1     0     1    1      1     0     0    4  ...   \n",
       "\n",
       "   championship  ...\"  endear  cortney  incid  erupt  semblanc  miser  shoe  \\\n",
       "0             0     0       0        0      0      0         0      0     0   \n",
       "1             0     0       0        0      0      0         0      0     0   \n",
       "2             0     0       0        0      0      0         0      0     0   \n",
       "3             0     0       0        0      0      0         0      0     0   \n",
       "4             0     0       0        0      0      0         0      0     0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize():\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vector = np.array([doc.count(word) for word in vocab_without_stopwords])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "bow = pd.DataFrame(vectorize(), columns=vocab_without_stopwords)\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that we could do is to normalize our counts. As you can see, different documents have different number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    724\n",
       "1     97\n",
       "2    434\n",
       "3    417\n",
       "4    801\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can introduce bias in our features, so we should normalize each document by its number of words. This way, instead of having word counts as features of our model, we will have **term frequencies**. This way, the features in any document of the dataset sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = bow.div(bow.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002278</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         movi      film       one      like      time  get  watch      make  \\\n",
       "156  0.003195  0.001597  0.001597  0.000799  0.000799  0.0    0.0  0.002396   \n",
       "77   0.000000  0.000000  0.002278  0.000000  0.000759  0.0    0.0  0.000759   \n",
       "139  0.000000  0.000000  0.000000  0.001572  0.001572  0.0    0.0  0.003145   \n",
       "\n",
       "         even       see  ...  championship  ...\"  endear  cortney  incid  \\\n",
       "156  0.001597  0.002396  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "77   0.000759  0.000759  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "139  0.000000  0.001572  ...           0.0   0.0     0.0      0.0    0.0   \n",
       "\n",
       "     erupt  semblanc  miser  shoe  mail  \n",
       "156    0.0       0.0    0.0   0.0   0.0  \n",
       "77     0.0       0.0    0.0   0.0   0.0  \n",
       "139    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 5595 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kicking it up a notch with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be intuitive that not all words have the same importance to find out in what category a document falls into.\n",
    "\n",
    "If we want to classify a review as positive, the word \"*good*\", for instance, is much more informative than \"*house*\", and we should give it more weight as a feature.\n",
    "\n",
    "In general, words that are very common among all classes are less informative, while words that appear particularly connected to a specific class are more informative. This is usually also the case for rare words, which show up little and may show up only on one particular class.\n",
    "\n",
    "That is the rationale behind **Term Frequency - Inverse Document Frequency (TF-IDF)**:\n",
    "\n",
    "$$ \\text{TF-IDF} _{t, d} =(log{(1 + TF_{t,d})})*(log{(1 + \\frac{N}{DF_{t}})})  $$\n",
    "\n",
    "where\n",
    "- $t$ and $d$ are the term and document for which we are computing a feature\n",
    "- $TF_{t,d}$ is the term frequency of term $t$ in document $d$\n",
    "- $DF_{t}$ is the number of documents that contain $t$\n",
    "- $N$ is the total number of documents in the dataset\n",
    "\n",
    "We are using the word frequencies from before, but now we are weighting each by the inverse of the number of times they occur in all the documents. The more a word appears in a document and the less it appears in other documents, the higher the TF-IDF of that word in that document.\n",
    "\n",
    "In short, we measure **the term frequency, weighted by its rarity in the entire corpus**.\n",
    "\n",
    "**Note**: You may find some variations on the expression for TF-IDF online. Despite that, the idea is always the same but the computation might change slightly. In our case, we are choosing to log-normalize our frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define this computation in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movi        133\n",
       "film        117\n",
       "one         147\n",
       "like        103\n",
       "time         87\n",
       "           ... \n",
       "erupt         1\n",
       "semblanc      1\n",
       "miser         1\n",
       "shoe          2\n",
       "mail          2\n",
       "Length: 5595, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bow > 0).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.00686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.00518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012601</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.003949</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002057</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.004848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movi     film       one      like      time       get     watch  \\\n",
       "0  0.003795  0.00686  0.000000  0.000000  0.003293  0.001605  0.001807   \n",
       "1  0.009413  0.00000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.012601  0.00000  0.003949  0.002483  0.000000  0.000000  0.000000   \n",
       "3  0.008762  0.00000  0.002057  0.002584  0.002859  0.000000  0.000000   \n",
       "4  0.001145  0.00000  0.001072  0.000000  0.001489  0.001450  0.001633   \n",
       "\n",
       "      make      even       see  ...  championship  ...\"  endear  cortney  \\\n",
       "0  0.00518  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "1  0.00000  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "2  0.00000  0.002823  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "3  0.00000  0.002938  0.004848  ...           0.0   0.0     0.0      0.0   \n",
       "4  0.00000  0.000000  0.005047  ...           0.0   0.0     0.0      0.0   \n",
       "\n",
       "   incid  erupt  semblanc  miser  shoe  mail  \n",
       "0    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "1    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "2    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "3    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "4    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf_idf(bow):\n",
    "    # Term frequency: divide word count by the number of words in the document\n",
    "    tf = bow.div(bow.sum(axis=1), axis=0)\n",
    "\n",
    "    # Document frequency: number of documents containing the word\n",
    "    df = (bow > 0).sum(axis=0)\n",
    "\n",
    "    # N: number of documents\n",
    "    n = bow.shape[0]\n",
    "\n",
    "    return np.log(1 + tf) * np.log(1 + n / df)\n",
    "\n",
    "tf_idf_df = tf_idf(bow)\n",
    "\n",
    "tf_idf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've learned how to represent documents using vectors, you can use these new dataset representation as inout of a classification model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "In Part 1, you were introduced to N-Grams. Now, you can see how N-Grams could serve as feature of vectors to represent documents. Essentially, the Bag of Words (BoW) technique involves creating a vocabulary and features based on individual words, or uni-grams.\n",
    "\n",
    "Everything that has been done using uni-grams can be extended to N-Grams for any value of N. This means each feature can represent an N-Gram instead of a uni-gram, while the overall functionality remains the same.\n",
    "\n",
    "However, it is important to mention that as *N* increases, so does the size of the N-Grams vocabulary due to the exponential growth in possible combinations of words. This makes our document representation sparser (vectors with many zeros), introducing more complexity and noise and potentially harming the machine learning models' ability to find patterns in data effectively. Additionally, with larger datasets, the vocabulary size already becomes bigger by itself, so combining this with an *N* greater than 1 has an even bigger consequence on data sparsity.\n",
    "\n",
    "On the other hand, using N-grams as features partially preserves word order information, which may be useful for some tasks!\n",
    "\n",
    "Therefore, the choice of *N* to use in document vectorization should take into account both these advantages and disadvantages. You'll see this decision-making process in action on the Part 3!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Words as Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we finish this notebook, let's just quickly look at word representation. Until now, we've only found ways of representing documents as vectors, where the features of these vectors are related to word counting.\n",
    "\n",
    "However, we can also think the other way around. What if instead of using word counts to vectorize documents, we used document counts to vectorize words? This may sound confusing, and not very helpful for the task of classifying movie reviews... But representing words as vectors might be, in fact, a very interesting exercise!\n",
    "\n",
    "To do this, instead of seeing the vocabulary as the features of a given document, we see each document as a feature of a given term in the vocabulary. By doing this, we get a vectorized representation of a word!\n",
    "\n",
    "When we have numerical representations of words, we can check similarities between them! A very popular way of computing similarities between vectors is by calculating the cosine similarity (the cosine of the angle between the vectors).\n",
    "\n",
    "Let's check the similarity between the word _movi_ and the word _good_ in our Bag of Words representation. These are words that typically show up side by side in movie reviews (at least the positive ones) and as such we would expect a bigger similarity when compared with other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.46827681]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(bow['movi'].values.reshape(1,-1), bow['good'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the similarity of _movi_ and _shoe_. We should get a lower similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14554543]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(bow['movi'].values.reshape(1,-1), bow['shoe'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the same similarity scores but in the tf-idf representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49023951]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf_df['movi'].values.reshape(1,-1), tf_idf_df['good'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03487183]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(tf_idf_df['movi'].values.reshape(1,-1), tf_idf_df['shoe'].values.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! The gap between the similarities of these pairs of words increased with our TF-IDF representation. This means that our TF-IDF model is computing better and more meaningful features than our BoW model. This will surely help when we feed these feature matrices to a classifier.\n",
    "\n",
    "**Note**: Our evaluation focuses only on words similarities within the context of this dataset and its document distribution (i.e. word X has a similar distribution in these docs as word Y). This means that you should not take this as a general interpretation of the english language (i.e. word X is close to word Y). That's why we speak in relative terms in our comparisons, saying that a set of words has \"higher\" or \"lower\" similarity and not on absolute terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing Part 2! The next and last part will be about sentiment analysis, a very common introductory exercise to NLP.\n",
    "\n",
    "You'll also see that `scikit-learn` comes with implementations of both Bag of Words and TF-IDF vectorizers, which will make our lives easier, as you'll see!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
