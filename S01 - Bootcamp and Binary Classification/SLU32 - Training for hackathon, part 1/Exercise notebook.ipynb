{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c8eb4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe6ee765726b9073",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# SLU32 - Training for the hackathon, part 1\n",
    "In the upcoming hackathon, you'll receive a dataset and a metric, for which you'll have to set up and optimize a model. Even though you'll learn all the necessary bits and pieces to accomplish this task by the date of the hackathon, it's probably going to seem like a daunting task. Don't worry! The hackathon is a multistep process with multiple possible solutions: a much bigger challenge than any of the exercise notebooks you'll complete in preparation, but it's also managable with preparation. \n",
    "\n",
    "This SLU is meant to prepare you. You can practice everytyhing you learned in the first part of the S01 specialization, in SLUs 01-10. You should first get to know your dataset (exploratory data analysis - EDA) and then set up the first model.\n",
    "\n",
    "We're providing hints to guide you through the workflow, but it's on you to fill in the code. Consider this workflow as just a proposal to get you started. Once you're more experienced, you'll need to design your own workflow that suits your needs. \n",
    "\n",
    "The second part of hackathon training is SLU64, where you'll practice feature engineering, model optimization, and dealing with imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all you need in this cell - we already did some imports for you\n",
    "# basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve,classification_report,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ceab9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e319a174835d907",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1. Read the problem statement\n",
    "We'll use the problem from the first hackathon from a previous batch of LDSSA for this exercise. You can read its problem statement in the `README_hckt01.md` file. The topic might be from an area completely unfamiliar to you, but this is normal in data science. Data scientists work on a wide variety of challenges, and each time they need to learn the background of those challenges in order to deliver good solutions. Of course, in a hackathon, you'll have little time to explore your topic, but you can always ask for help from instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30037655",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a2c0317ea21724e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2. Import the dataset\n",
    "There are two datasets in the `data` folder: `train` and `test`. You should train your model with the `train` dataset, and use the `test` dataset only at the end to calculate predictions; never use the test in training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and preview the dataset (you can first look at the datafile directly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b066e84c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7097a22865e686ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3. Exploratory data analysis (EDA) and data cleaning\n",
    "Use Pandas tools from SLUs 01 - 06 and plots to get to know your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651bf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the size of the dataset and the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34ee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is your data tidy? Are the variables in columns and data points in rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f869a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the datatypes.\n",
    "# Do they make sense or do you need to correct them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique values does each column have?\n",
    "# Identify the categorical and numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80abe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the ranges of the numerical values.\n",
    "# Are there outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if the dataset is balanced. How many datapoints for each class do you have?\n",
    "# You don't have the tools to do with this yet, but it's good to be aware of it this early. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e59813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See where you have missing values.\n",
    "# Think about how to deal with them. Fill in (different strategies) or drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you have duplicated data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334fd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are the variables independent?\n",
    "# Check out the correlations between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474b37f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ef5d5e954409e874",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4. Model - baseline\n",
    "This is clearly a classification problem, so you can apply the classification model you already know to get the first result (a baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4c39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the variables that you want to feed into the model.\n",
    "# You won't be familiar with the tools you'll use to deal with categorical variables by the time you work on SLU10,\n",
    "# so you can ignore them or use a simple strategy.\n",
    "# Do the variables need to be scaled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2f2cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-05403a72454a58c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5. Calculate the predictions\n",
    "Calculate the predictions for both `train` and `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932a58ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions for the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24327f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the predictions for the test data\n",
    "# and save it prediction in a variable named test_prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions from the test_prediction variable to a csv file.\n",
    "# Uncomment the code and run it.\n",
    "#submission = pd.Series(test_prediction,index=test.index, name='id')\n",
    "#submission.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81ffa4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9447088a54b1ed54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6. Model evaluation with the training data\n",
    "Use the prediction from the training data to evaluate your model. In the hackathon, you'll have access to the training data's labels, but not the test data's labels, so you'll have to use the training data's predictions to evaluate your model during development.\n",
    "\n",
    "Use the _auroc_ score to evaluate the model. For the sake of training, you can also look at other metrics and think about how well they fit this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ad838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the roc-auc-score for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c9d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check how the model does on the other metrics you learned about.\n",
    "# Which of them are relevant for this situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cbd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can calculate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67189094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot here the roc curve and calculate the auroc score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481eec4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6d886d52117f2ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 7. Calculate the score with the test data\n",
    "In this section, we'll test the model predictions for the test data. In the hackathon, you'll generate these predictions and then submit them to the portal, which will compare your prediction with the real labels and give you a score.\n",
    "\n",
    "To complete this notebook, you'll use the test predictions in the `portal` directory with the code to calculate the score. Run the following cells to calculate your score. You'll need to uncomment some parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd89c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the code to calculate the score.\n",
    "from portal.score import load, validate, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the true labels and your prediction.\n",
    "# Uncomment and run the code.\n",
    "#y_true = load(\"portal/data\")\n",
    "#y_pred = load(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f79f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function just validates if the prediction has the correct format.\n",
    "# Uncomment and run it.\n",
    "#validate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e150642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the auc-roc score for your test prediction.\n",
    "# Uncomment and run the code.\n",
    "#score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137c19c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee01e5cc4f5de757",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 8. Feature selection\n",
    "Feature selection is something we looked at only briefly in SLU09's exercise notebook, but you'll learn about it in depth in SLU14. \n",
    "\n",
    "The goal of feature selection is to look at how important is each feature for a model's outcome and then retrain the model with only the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d158a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the coefficients of the model and see which ones have the most weight in the model \n",
    "# (the highest numbers in absolute terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94184d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain the model with the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c750eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you like, tweak other model parameters.\n",
    "# Again, this is something that you will learn about in later SLUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a4816",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f280ee0ade08e13c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 9. Presentation\n",
    "The final part of the hackathon is to present your solution to the other teams. The presentation should contain all the steps in the analysis, with your justification for the decisions you made. You'll need to use tables or visualizations to support your claims. Think about how you'd present the EDA part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8522dd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96f8b599abd52e9f",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('I have completed the hackathon training!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
