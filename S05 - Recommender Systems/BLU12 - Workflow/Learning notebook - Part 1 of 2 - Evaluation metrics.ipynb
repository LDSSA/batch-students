{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU12 - Learning Notebook 1 - Evaluation Metrics for Recommender Systems\n",
    "\n",
    "You have already learned about regression metrics in SLU08 and classification metrics in SLU10 and you have successfully applied some of them in hackathons. What kind of metrics should we use to evaluate the performance of recommenders? Regression metrics? Classification metrics? It depends on what is the goal of the analysis and how the data is processed.\n",
    "\n",
    "The three types of metrics available for recommender evaluations are:\n",
    "\n",
    "1. Regression metrics\n",
    "\n",
    "2. Classification metrics\n",
    "\n",
    "3. Information retrieval metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regression metrics\n",
    "\n",
    "It is possible to conceptualize a recommendation problem as a regression problem if the objective of the analysis is to predict the value of a numerical indicator such as ratings. If you want to predict the value of a given indicator for a specific item and user, you can - theoretically - solve it as a regression problem. By defining user and item variables as independent variables and the respective rating as the dependent variable (like in the example below), it is possible to solve it with regression models. \n",
    "\n",
    "Consider the case where movie rating data is processed to obtain the following table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/regression.png\" width=\"650\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user and item variables are the independent variables and the rating is the dependent variable and thus it is possible to use regression models to predict the movie rating for a specific user. The results can be evaluated using regression metrics as discussed in SLU08."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification metrics\n",
    "\n",
    "Similarly, a recommendation problem can be reframed as a classification problem if the target variable is categorical.\n",
    "\n",
    "If the goal is to predict if a user would recommend a given item or not, the data can be represented as variables and a classification model can be trained to make predictions. The quality of these predictions can be evaluated using classification metrics as described in SLU10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/classification.png\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Information retrieval\n",
    "\n",
    "From [*Introduction to Information Retrieval*](https://www-nlp.stanford.edu/IR-book/):\n",
    "\n",
    "\"Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).\" \n",
    "\n",
    "Basically when using search engines or asking for advice from a librarian, we are engaging in IR. We are using search terms to perform a query over a large collection of resources. We won't go into much detail on IR. What is important to us is how a recommender system compares to an information retrieval system (IRS) and how an IRS evaluates the accuracy of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/IRS_RS.png\" width=\"700\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, an IRS takes a user query and tries to return documents that contain relevant information. Similarly, a recommender system tries to find items that satisfy the user. IRS usually deals with queries created by the user while with RS there isn't a direct query from the user.\n",
    "\n",
    "We can think of it in the following manner: a recommender system is an information retrieval system that answers the query \"What items best satisfy this user?\".\n",
    "The RS will provide the results of this query (the recommendations) and the user will provide feedback by accessing (or not) the items or by providing a rating.\n",
    "\n",
    "IR metrics are used to compare the results provided by the IRS and the expected results for the given query.\n",
    "Two factors indicate how good are the IR results: rank and relevance.\n",
    "\n",
    "The IR results are provided as a ranked list of documents starting with the documents that best fit the query criteria. \n",
    "Consider an example where we use a search engine to find web pages regarding regression - as in regression analysis. For us, a web page with detailed information on regression analysis is more useful than a page that simply states the definition of regression or a page about age regression or the movie *Regression (2015)*. This degree of \"usefulness\" can be used to rank the results - provided there is feedback. \n",
    "\n",
    "A relevant document (or result) is a document that is expected by the user. Using the previous example, the pages about age regression and the movie are not relevant to the user. Having a high number of non-relevant results is an indicator of poor IR performance.\n",
    "\n",
    "When a recommender system returns a ranked list of results just like an IRS, it is possible to use information retrieval metrics to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Information retrieval metrics for top-N lists\n",
    "\n",
    "Let's start by introducing some nomenclature with a dummy example. Imagine a user that has preference for fruits. We develop an RS model that creates a list of recommendations ranked from most recommended to least recommended, resulting in:\n",
    "\n",
    "<img src=\"./media/IR_sets.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row represents the actual user preferences (as available in the test data) ranked by order of preference and the second row represents the recommendations also sorted by predicted preference. The set of all user preferences is represented as $\\{Relevant\\ items\\}_u$ and the set of all recommendations is represented as $\\{Retrieved\\ items\\}_u$. The relevant items are highlighted in green and the non-relevant items (not present in $\\{Relevant\\ items\\}_u$) are highlighted in red.\n",
    "\n",
    "With this out of the way, let's define some evaluation metrics. You will recognize some of them from classification problems, here we redefine them in the recommender systems context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Precision\n",
    "\n",
    "Precision measures how many of the items recommended for user $u$ are relevant.\n",
    "\n",
    "$$Precision\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u \\cap \\{Relevant\\ items\\}_u |}{|\\{Retrieved\\ items\\}_u|}$$\n",
    "\n",
    "$|{set}|$ is the cardinality of the set, which is the number of items in the set. The operator $\\cap$ represents the intersection between two sets. The result of the intersection between two sets is another set whose elements are present in both original sets.\n",
    "\n",
    "To evaluate the RS as a whole, we average the precision for all active users $u \\in U$ where $U$ is the set containing all users.\n",
    "\n",
    "$$Precision\\ (\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} Precision\\ (\\{Retrieved\\ items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Recall\n",
    "\n",
    "Recall, on the other side, relates to how many relevant items were recommended out of all relevant items for the user $u$.\n",
    "\n",
    "$$Recall\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u \\cap \\{Relevant\\ items\\}_u |}{|\\{Relevant\\ items\\}_u|}$$\n",
    "\n",
    "Again, to evaluate the RS we average the results for all active users $u \\in U$.\n",
    "\n",
    "$$Recall\\ (\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} Recall\\ (\\{Retrieved\\ items\\}_u)}{|U|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Precision and recall @k\n",
    "\n",
    "For applications such web searches with a huge amount of retrieved items, it is not interesting to consider all the retrieved results. Rather the user is interested in the first k results (from the first page or the first n pages). In line with this, both precision and recall can be calculated only up to a defined rank (or cut-off) k - precision@k and recall@k.\n",
    "\n",
    "From the N retrieved items, we consider only the first k recommendations from rank 1 to rank k, $\\{Retrieved\\ Items\\}_u^k$:\n",
    "\n",
    "$$Precision@k\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u^k \\cap \\{Relevant\\ items\\}_u |}{|\\{Retrieved\\ items\\}_u^k|}$$\n",
    "\n",
    "$$Recall@k\\ (\\{Retrieved\\ items\\}_u) = \\frac{|\\{Retrieved\\ items\\}_u^k \\cap \\{Relevant\\ items\\}_u |}{|\\{Relevant\\ items\\}_u|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Precision-recall curve\n",
    "\n",
    "Plotting precision vs. recall, we get the precision-recall curve (PRC), similar to the ROC curve (recall or true positive rate vs the false positive rate). PRC measures the capacity of the model to identify relevant items without selecting too many non-relevant items. If the (k + 1)th document retrieved is nonrelevant then recall is the same as for the top k documents, but precision has dropped. If it is relevant, then both precision and recall increase,  and the curve jags up and to the right. Below is an illustrative precision-recall curve for a logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/AUPRC_balanced.png\" width=\"450\" title=\"https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the area under the curve - AUPRC. Note that the value of the AUPRC for a no skill model is equal to the proportion of positive cases. This contrasts with the fact that AUROC of a no skill model is always 0.5 regardless of the proportion of positive cases.\n",
    "\n",
    "$$AUPRC_u = \\sum\\limits_{k=1}^NPrecision@k(\\{Retrieved\\ items\\}_u) * \\Delta Recall@k(\\{Retrieved\\ items\\}_u)$$ \n",
    "\n",
    "The AUPRC can be particularly useful when the fraction of positive cases is small and when identifying positive cases is more important than identifying negative cases. These conditions typically happen in medical applications where the incidence of a condition is low - low number positive cases - and is more important to identify the condition than to verify that the pacient does not have the condition.\n",
    "\n",
    "In RS these conditions also apply. The users can only access a tiny amount of all available items - just consider the amount of books, movies, series, products, etc. The amount of relevant items is, therefore, significantly smaller than non-relevant items. Furthermore, it is more important to recommend relevant items than to make sure that non-relevant items are not recommended. The user might even like the - supposedly - non-relevant item after all. Take the case of [Plastic Love - Mariya Takeuchi](https://www.youtube.com/watch?v=3bNITQR4Uso). In 2018, the youtube recommendation algorithm \"spontaneously\" recommended an 80's pop song to a massive number of users that were completely unaware of the City Pop genre. This sparked the resurgence of an entire music genre ([Source](https://www.youtube.com/watch?v=PlPTXR7e6As))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When recommending something weird and the user likes it:\n",
    "\n",
    "<img src=\"./media/failed_success.jpeg\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5 Average precision @ k\n",
    "The average precision approximates the area under PRC. Average precision is the average of the precision values obtained for the set of top k items after each relevant item for the user is retrieved:\n",
    "\n",
    "$$AP@k\\ (\\{Retrieved\\ items\\}_u) = \\frac{\\sum\\limits_{i=1}^k (Precision@i\\ (\\{Retrieved\\ items\\}_u) \\cdot relevant(i^{th})}{|\\{Relevant\\ items\\}_u|}$$\n",
    "\n",
    "k is the total number of retrieved items. The $relevant(i^{th})$ bit is a boolean value, indicating whether the $i$-th item is relevant or not (so effectively, the sum contains only the terms for relevant items).\n",
    "\n",
    "AP takes into account both the rank and the number of relevant items. It increases only with correct recommendations and ignores non-relevant items. Early hits, i.e. front-ranking correct recommendations, carry over and are continuously rewarded. Therefore, AP can never decrease as you increase $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Mean average precision @ k\n",
    "\n",
    "The average precision can be further averaged over all users (all queries in case of an IRS):\n",
    "\n",
    "$$mAP@k(\\{Retrieved\\ items\\}) = \\frac{\\sum\\limits_{u \\in U} AP@k(\\{Retrieved\\ items\\}_u)}{|U|}$$\n",
    "\n",
    "Here, *mean* refers to averaging over users/queries and *average* to averaging over items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.7 Spearman's rank correlation coefficient\n",
    "\n",
    "Another approach is to consider the rank of the relevant items as one variable and the rank of the recommendations as another. In this case it is appropriate to use rank correlation coefficients to evaluate the ordinal relationship between them. One of these rank correlation coefficients was already discussed in SLU05: the Spearman's rank correlation coefficient, $r_s$. \n",
    "\n",
    "To calculate the rank correlation, the rank of each item in $\\{Relevant\\ items\\}_u$ is compared with the rank of the same item in $\\{Retrieved\\ items\\}_u$. We can define two rank variables: user preferences ranks (PR) and recommendations ranks (RR). We are then comparing the position of each item in both variables. What matters is if the position of an item is the same in both variables. If the first element of PR is \"Item X\" then the first element of RR also has to be \"Item X\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the case above, we can identify the rank of each item of $\\{Relevant\\ items\\}_u$ and $\\{Retrieved\\ items\\}_u$. The items in RR were reordered to have the same position as in PR to make the comparison easier. Each item's rank is written within paranthesis. For instance, the item \"apple\" is the user's second (2) preferred item while it is the fifth (5) recommended item.\n",
    "<img src=\"./media/spearman_table.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pearson correlation between the user preferences and recommendations ranks returns the Spearman's rank correlation coefficient.\n",
    "When all the ranks are distinct integers - such as in our case - the Spearman's rank correlation coefficient can be calculated as:\n",
    "\n",
    "$$ r_s = 1- \\frac{6 \\sum\\limits_{i=1}^{k} d_i^2}{k(k-1)}$$\n",
    "\n",
    "where $i$ iterates over the items, $k$ is the number of ranks (items) to be compared and $d_i = PR_i - RR_i$ is the difference of ranks for item $i$.\n",
    "\n",
    "The Spearman's rank correlation coefficient has values ranging between -1 and 1. Correlation of 1 indicates that the order of the ranks in both variables is exactly the same, while correlation of -1 indicates that the variables have fully opposing order. Correlation of 0 indicates that the variables are completely independent. One should note that the computation of the correlation is sensitive to the presence of missing items, mainly in $\\{Retrieved\\ items\\}_u$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final remarks and further reading\n",
    "\n",
    "We have explored some of the most common metrics for recommender systems. There are a lot more metrics that can be used to evaluate the outcomes of recommender systems. The preference from one metric to another depends on multiple factors such as output type (binary, ordinal, recommendations), the type of user feedback, business requirements, and ease of interpretation. The best metric for a specific context might not be helpful for another context. You can find more information about information retrieval and metrics below.\n",
    "\n",
    "- [*Short introduction to Information retrieval*](https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_information_retrieval.htm)\n",
    "\n",
    "- [*Introduction to Information Retrieval*](https://nlp.stanford.edu/IR-book/), more specifically [*chapter 8*](https://nlp.stanford.edu/IR-book/pdf/08eval.pdf) on evaluation.\n",
    "\n",
    "- [*Information Retrieval Wiki*](https://en.wikipedia.org/wiki/Information_retrieval)\n",
    "\n",
    "- [*Information Retrieval Metrics Wiki*](https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval))\n",
    "\n",
    "- [*Precision-Recall Curves in Python*](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "\n",
    "- [*More info on AUPRC*](https://glassboxmedicine.com/2019/03/02/measuring-performance-auprc/)\n",
    "\n",
    "- [*Rank Correlation Wiki*](https://en.wikipedia.org/wiki/Rank_correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
