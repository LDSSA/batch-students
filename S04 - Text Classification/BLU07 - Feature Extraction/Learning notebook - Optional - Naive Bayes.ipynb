{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576afa4a",
   "metadata": {},
   "source": [
    "# Naïve Bayes classifier - An introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7fa37c",
   "metadata": {},
   "source": [
    "## 1. The \"Bayes\" part\n",
    "\n",
    "In the Naive Bayes classifier, the Bayes part comes from the Bayes theorem, and a link to Bayesian statistics. Now, we won't deep dive into the details of Bayesian statistics [1] but you can take the following principle from it: the Bayesian approach to probability allows you to incorporate prior information or expectations you have about your data and update them as you see new observations, to generate a final expectation over your hypothesis.   \n",
    "\n",
    "Unlike frequentist approaches, which assume that data is generated from a distribution with fixed but unknown parameters (which are estimated), Bayesian approaches treat the parameters themselves as random variables, drawn from a specific probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe40953b-cc05-4bfd-bb3f-0c243dcd06a6",
   "metadata": {},
   "source": [
    "### 1.1 Bayes’ Theorem\n",
    "\n",
    "The Bayes' theorem is a way of representing the conditional probability of two events A and B.\n",
    "\n",
    "First, let's define some variables:\n",
    "\n",
    "* $P(A)$: probability of event A ocurring\n",
    "* $P(B)$: probability of event B ocurring\n",
    "* $P(A|B)$: probability of event A ocurring knowing that B ocurred\n",
    "* $P(B|A)$: probability of event B ocurring knowing that A ocurred\n",
    "* $P(A\\cap B)$: probability of event A and event B ocurring together, i.e., the joint probability of both events\n",
    "\n",
    "Now, starting from the conditional probabilities, we know that:\n",
    "\n",
    "$$P(A\\cap B) = P(A|B)P(B) $$ \n",
    "\n",
    "In the same way:\n",
    "\n",
    "$$P(A\\cap B) = P(B|A)P(A) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946639b6-7d07-4eb8-a44d-5367c81cb735",
   "metadata": {},
   "source": [
    "From this, the Bayes theorem derives the following:\n",
    "\n",
    "$$ P(A|B)P(B) = P(B|A)P(A)  \\Leftrightarrow  P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$ \n",
    "\n",
    "Now, if you think of event A as a hypothesis we're interested in testing, and B as the data we observe, we could write the following:\n",
    "\n",
    "$$ P(hyp|data) = \\frac{P(data|hyp)P(hyp)}{P(data)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f896d95-ed15-47fa-a6cf-9932362f70d3",
   "metadata": {},
   "source": [
    "Going a bit further with this you can describe:\n",
    "\n",
    "* $P(hyp)$: our **prior** knowledge about the hypothesis\n",
    "* $P(data)$: overall probability of our data, independent of any hypothesis\n",
    "* $P(data|hyp)$: probability of this data ocurring if our hypothesis is true, usually called the **likelihood** of our data\n",
    "* $P(hyp|data)$: how likely our hypothesis is to be true given the data we're observing, usually called the **posterior** probability\n",
    "\n",
    "$P(hyp)$ can be seen as your **prior** belief in our hypothesis, while $P(hyp|data)$ can be seen as an updated belief, or **posterior** belief after observing some data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96c90ee-18ea-44b4-acec-11594188f9e9",
   "metadata": {},
   "source": [
    "![seashell](media/seashell.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbe0666-97d2-4953-9112-66fe9d99dd30",
   "metadata": {},
   "source": [
    "### 1.2 The \"Naïve\" part\n",
    "\n",
    "Now that you got the gist of Bayes' theorem, let's dive into why it's called naïve. The theorem we presented above seems quite simple right?\n",
    "\n",
    "$$ P(hyp|data) = \\frac{P(data|hyp)P(hyp)}{P(data)} $$ \n",
    "\n",
    "But if you take a closer look, the overal notion of observed `data` can be quite complex. By data, we usually refer to all the events that we are using to predict the probability of our hypothesis. _Do you see where I'm going with this?_\n",
    "\n",
    "That's right, I'm talking about features. And if we expand that to what we typically have - multiple features - then each feature can be seen as one event, and thus we can rewrite the equation as follows:\n",
    "\n",
    "\n",
    "$$ P(hyp|\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\}) = \\frac{P(\\{x_1 \\cap x_2, ... \\cap x_n\\}|hyp)P(hyp)}{P(\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\})} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616333ad-05e6-4317-9562-f912cbcbf0a1",
   "metadata": {},
   "source": [
    "This is where the naïve part comes in. We are going to make an assumption, and a pretty big one. We're going to assume that our features are independent of each other. With this **independence assumption** we get the following:\n",
    "\n",
    "$$ P(x_1 \\cap x_2, ... \\cap x_n) = P(x_1)P(x_2) ... P(x_n) $$\n",
    "\n",
    "which is also valid for the conditional probability\n",
    "\n",
    "$$ P(x_1 \\cap x_2, ... \\cap x_n | hyp) = P(x_1|hyp)P(x_2|hyp) ... P(x_n|hyp) $$\n",
    "\n",
    "Putting all of this together we get:\n",
    "\n",
    "$$ P(hyp|\\{x_1 \\cap x_2 \\cap ... \\cap x_n\\}) = \\frac{P(x_1|hyp)P(x_2|hyp) ... P(x_n|hyp) * P(hyp)}{P(x_1)P(x_2) ... P(x_n))} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d0dad",
   "metadata": {},
   "source": [
    "## 2. A simple example\n",
    "\n",
    "Imagine that we have the following hypothesis over a piece of fruit taken from a bowl of fruit. Our hypothesis will be that the piece of fruit is an orange:\n",
    "\n",
    "$H_0$: fruit is an orange\n",
    "\n",
    "And we define some features that we observe \n",
    "\n",
    "* feature 1 ($x_1$), color  &ensp;&emsp;-> &emsp; $x_1$=orange\n",
    "* feature 2 ($x_2$), shape &emsp;-> &emsp; $x_2$=round\n",
    "\n",
    "Then by definition:\n",
    "\n",
    "$$ P(H_0|\\{x_1 \\cap x_2\\}) = \\frac{P(x_1|H_0)P(x_2|H_0)P(H_0)}{P(x_1)P(x_2)} $$ \n",
    "\n",
    "### 2.1 Prior\n",
    "\n",
    "As mentioned before, $P(H_0)$ is our prior belief in the hypothesis. If I only buy oranges and apples, and I buy them in the same quantities, for example, I could say my prior probability that I'll get an orange from my fruit bowl is the same as that - 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9682f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eae2ee9",
   "metadata": {},
   "source": [
    "### 2.2 Likelihoods\n",
    "\n",
    "If I have some previous data that I've seen:\n",
    "\n",
    "| observation | color | shape | is_orange | \n",
    "|---|-------|-------|-------|\n",
    "| 1 | orange  | round | yes | \n",
    "| 2 | orange  | round | yes | \n",
    "| 3 | orange  | round | no | \n",
    "| 4 | red  | round | no | \n",
    "| 5 | green  | round | yes | \n",
    "| 6 | red  | not round | no | \n",
    "\n",
    "Then I can extract my likelihoods from here.\n",
    "\n",
    "$$ P(x_1=orange|H_0) = \\frac{count(x_1=orange, H_0=yes)}{count(H_0=yes)}= \\frac{2}{3} \\approx 0.666 $$\n",
    "\n",
    "In plain words, for the feature color $(x_1)$, the table has 2 instances where $ x_1 = \\text{orange} $ and $ H_0 = \\text{yes} $ (observations 1 and 2) out of 3 total instances where $ H_0 = \\text{yes} $ (observations 1, 2, and 5).\n",
    "\n",
    "\n",
    "In the same way, we calculate it for the feature shape $(x_2)$:\n",
    "\n",
    "$$ P(x_2=round|H_0) = \\frac{count(x_2=round, H_0=yes)}{count(H_0=yes)}= \\frac{3}{3}=1.0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a6655-8e1b-49ea-bb95-e02df9af0ec5",
   "metadata": {},
   "source": [
    "Let's put this experiment into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385422e0-6f41-4fa2-ab57-482d4d90bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"orange\", \"round\", \"yes\"),\n",
    "    (\"orange\", \"round\", \"yes\"),\n",
    "    (\"orange\", \"round\", \"no\"),\n",
    "    (\"red\", \"round\", \"no\"),\n",
    "    (\"green\", \"round\", \"yes\"),\n",
    "    (\"red\", \"not round\", \"no\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b300602f-b71f-4743-b45a-1c3b82d1a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(data, feat_index, feat_value, hyp):\n",
    "    \n",
    "    count_hyp = 0\n",
    "    count_feat_hyp = 0\n",
    "    for row in data:\n",
    "        if row[-1] == hyp:\n",
    "            count_hyp += 1\n",
    "            if row[feat_index] == feat_value:\n",
    "                count_feat_hyp += 1\n",
    "    \n",
    "    return 1.0*count_feat_hyp/count_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0df8725f-7d46-4a82-8d97-24f5a3cac12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "p_x_1_h_0 = likelihood(data, 0, \"orange\", \"yes\")\n",
    "p_x_2_h_0 = likelihood(data, 1, \"round\", \"yes\")\n",
    "\n",
    "print(p_x_1_h_0)\n",
    "print(p_x_2_h_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23315bf0",
   "metadata": {},
   "source": [
    "### 2.3 Evidence\n",
    "\n",
    "Finally, we'll do $P(x_1)$ and $P(x_2)$, although it is slightly less important, because it's just a normalizing constant for multi-class problems. Let's extract it for our current dataset:\n",
    "\n",
    "For $P(x_1=orange)$: out of 6 observations, 3 have $x_1 = orange$.\n",
    "\n",
    "$$P(x_1 = \\text{orange}) = \\frac{3}{6} = 0.5$$\n",
    "\n",
    "For $P(x_2=round)$: out of 6 observations, 5 have $x_2 = round$.\n",
    "\n",
    "$$P(x_2 = \\text{round}) = \\frac{5}{6} \\approx 0.833$$\n",
    "\n",
    "As you can see, the evidence terms reflect the overall probability of observing a feature in the entire dataset.\n",
    "\n",
    "Let's put this into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "653cc481-b20f-4dcf-bc0d-692c2a85a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_evidence(data, feat_index, feat_value):\n",
    "    \n",
    "    count_rows = 0\n",
    "    count_feat = 0\n",
    "    for row in data:\n",
    "        count_rows += 1\n",
    "        if row[feat_index] == feat_value:\n",
    "            count_feat += 1\n",
    "    \n",
    "    return 1.0*count_feat/count_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c87e5cdd-52f2-41aa-b89a-533b95de405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "p_x_1 = prob_evidence(data, 0, \"orange\")\n",
    "p_x_2 = prob_evidence(data, 1, \"round\")\n",
    "\n",
    "print(p_x_1)\n",
    "print(p_x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e30bdc",
   "metadata": {},
   "source": [
    "### 2.4 Posterior probability\n",
    "Now we can infer our final posterior probability for the hypothesis by pluging all these values into Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(H_0 \\mid x_1, x_2) = \\frac{P(x_1 \\mid H_0) P(x_2 \\mid H_0) P(H_0)}{P(x_1) P(x_2)}\n",
    "$$\n",
    "\n",
    "Substitute the values:\n",
    "\n",
    "- $P(x_1 \\mid H_0) = 0.666$\n",
    "- $P(x_2 \\mid H_0) = 1.0$\n",
    "- $P(H_0) = 0.5$\n",
    "- $P(x_1) = 0.5$\n",
    "- $P(x_2) = 0.833$\n",
    "\n",
    "$$\n",
    "P(H_0 \\mid x_1, x_2) = \\frac{0.666 \\times 1.0 \\times 0.5}{0.5 \\times 0.833} = 0.7999999999999999\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40cec6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7999999999999999\n"
     ]
    }
   ],
   "source": [
    "p_h_0_X = (p_x_1_h_0 * p_x_2_h_0 * prior)/(p_x_1*p_x_2)\n",
    "print(p_h_0_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b766e88",
   "metadata": {},
   "source": [
    "### 2.5 Effect of changing the prior\n",
    "If my prior had been different, let's say that I buy oranges very rarely, *$P(H_0) = 0.1$*, we would get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2818022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16\n"
     ]
    }
   ],
   "source": [
    "prior = 0.1\n",
    "p_h_0_X = (p_x_1_h_0 * p_x_2_h_0 * prior)/(p_x_1*p_x_2)\n",
    "print(p_h_0_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045648a-15e7-4456-8468-f668ef1a99ce",
   "metadata": {},
   "source": [
    "This shows how the posterior probability is influenced by the prior belief.\n",
    "\n",
    "By now, the idea that we're combining prior knowledge or beliefs that we have with actual observations should be more or less obvious to you. The following image from this [article](https://towardsdatascience.com/what-is-bayesian-inference-4eda9f9e20a6) shows you a visual interpretation of this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8e15d-cf8b-4059-af6e-89c6e7077884",
   "metadata": {},
   "source": [
    "![bayes_charts](media/bayes_visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48047c-08a3-4d73-9dc4-26d810b30414",
   "metadata": {},
   "source": [
    "## 3. Applying Naïve Bayes to text classification\n",
    "\n",
    "Now for the interesting part - applying it to text classification!\n",
    "\n",
    "Imagine that we want to classify a particular review as:\n",
    "\n",
    "* $C_0$: positive review\n",
    "* $C_1$: negative review\n",
    "* $C_2$: neutral review\n",
    "\n",
    "According to our formulas we have:\n",
    "\n",
    "$$P(C_i|D_j) = \\frac{P(D_j|C_i)*P(C_i)}{P(D_j)}$$\n",
    "\n",
    "Where: \n",
    "\n",
    "* $P(C_i)$ is the overall probability of the class, the **prior**;\n",
    "* $P(D_j)$ is the overall probability of the document;\n",
    "* $P(D_j|C_i)$ is the **likelihood** of a document showing up for a particular class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9b2e93-6bc1-42a5-a3b5-90ef8817eb9b",
   "metadata": {},
   "source": [
    "To make things simpler notice that $P(D_j)$ is constant across the two classes, i.e., when comparing among classes, the value doesn't change. Since we're only interested in knowing which class has higher probability, we can just say that $P(C_i|D_j)$ is proportional to the following quantity:\n",
    "\n",
    "$$P(D_j|C_i)*P(C_i)$$\n",
    "\n",
    "And so this is the formula we will use to compute our **comparative probabilities** between classes. \n",
    "\n",
    "Now, a document is a set of words, which we consider our features. Using (and abusing) the independence assumption of Naïve Bayes, the likelihood of a document showing in a particular class is just the likelihood of its set of words showing up in that class:\n",
    "\n",
    "$$P(D_j|C_i) = P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)$$\n",
    "\n",
    "And our final formula becomes:\n",
    "\n",
    "$$P(C_i|D_j) = [P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)]*P(C_i)$$\n",
    "\n",
    "\n",
    "In an analogous way to our previous fruit example, we consider $P(w_j|C_i)$ the likelihoods that we can calculate for the classes, and $P(C_i)$ the prior on all the classes.\n",
    "\n",
    "So, similar to the example we did above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed4a5d",
   "metadata": {},
   "source": [
    "### 3.1 Training\n",
    "\n",
    "* estimate $P(w_k|C_i)$ for all words, based on counts of the words in documents\n",
    "* set or estimate $P(C_i)$, either from dataset, previous knowledge or just assign equal probabilities\n",
    "\n",
    "### 3.2 Predicting\n",
    "\n",
    "* Compute $P(C_i|D_j) = [P(w_1|C_i) * P(w_2|C_i) * P(w_3|C_i) * ... * P(w_n|C_i)] * P(C_i)$\n",
    "* Choose the class satisfying $C = argmax_i P(C_i|D_j)$\n",
    "\n",
    "\n",
    "This is exactly what `MultinomialNB` does for you, and the reason why it's a good baseline follows (quoting directly from [3]):\n",
    "\n",
    "> Because of the independence assumption, naive Bayes classifiers can quickly learn to use high dimensional feature spaces with limited training data compared to more sophisticated methods. This can be useful in situations where the dataset is small compared to the number of features, such as images or texts.\n",
    "\n",
    "There are other ways of handling the dimensionality curse, as you'll see in the next BLU, but now you understand why Naïve Bayes is such an interesting method in these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985e5b27",
   "metadata": {},
   "source": [
    "## 4. References\n",
    "\n",
    "1. [MIT OpenCourseWare - Statistics for Applications: 17. Bayesian Statistics](https://www.youtube.com/watch?v=bFZ-0FH5hfs)\n",
    "2. [What Is Bayesian Inference?](https://towardsdatascience.com/what-is-bayesian-inference-4eda9f9e20a6)\n",
    "3. [Understanding mathematics behind naive bayes](https://shuzhanfan.github.io/2018/06/understanding-mathematics-behind-naive-bayes/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
