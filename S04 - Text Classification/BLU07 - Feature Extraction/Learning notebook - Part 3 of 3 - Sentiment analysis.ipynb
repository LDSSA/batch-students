{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU07 - Part 3 of 3 - Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "\n",
    "# NLTK imports\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned about _preprocessing_ of text data in notebook 1 and then about creating features from text data or _vectorization_ in notebook 2. Here we're going to look at vectorization implementations in sklearn and finally add a machine learning model and so create a complete workflow for sentiment analysis.\n",
    "\n",
    "Let's load our movie reviews data again and split it into a train and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data cleaning and preprocessing\n",
    "We want to look very professional, so we're going to use pipelines. We'll be using sklearn vectorizers and models later on which are designed to fit into a pipeline, but we have to prepare our preprocessing steps to fit into a pipeline too. We're going to achieve this by building a preprocessing transformer. You already saw transformers a few times before, so hopefully they are less daunting by now. It's good to get used to pipelines because they make the workflow much more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleanerTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Custom transformer to clean movie reviews data.\n",
    "    Removes unwanted strings like html tags, lowercases,\n",
    "    removes punctuation, tokenizes, and stems the data.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, stemmer, regex_list=[('','')],\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list # list of tuples [(pattern, sub)] for regex substitution\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "     \n",
    "    def fit(self, X, y=None):\n",
    "        return self   \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = X.map(self._clean_document)\n",
    "        return X\n",
    "\n",
    "    def _clean_document(self, doc):\n",
    "        \"\"\" \n",
    "        Helper function to perform the cleaning.\n",
    "        \"\"\"\n",
    "        for regex in self.regex_list:\n",
    "            doc = re.sub(regex[0], regex[1], doc)\n",
    "        if self.lower:\n",
    "            doc = doc.lower()\n",
    "        tokens = self.tokenizer.tokenize(doc)\n",
    "        if self.remove_punct:\n",
    "            pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "            sentence = \" \".join(tokens)\n",
    "            tokens = re.sub(pattern, '', sentence).split()\n",
    "        if self.stemmer:\n",
    "            tokens = map(self.stemmer.stem, tokens)\n",
    "        return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a custom transformer class with a `transform()` method that will apply the private method `_clean_document` to every item of its input `X`. You need to choose a tokenizer and a stemmer when initializing the transformer. You can also give the transformer a list of tuples of type (pattern, sub) which will be used in a regex substitution.\n",
    "\n",
    "Let's use the transformer with a `WordPunctTokenizer`, a `SnowballStemmer`, and the regex for HTML tag substitution that we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")]\n",
    "\n",
    "cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "train_preprocessed = cleaner.transform(train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an output example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a film about life the triumph over advers and the wonder of the human spirit i defi anyon not to shed a tear by the end of the movi this is more than just a tear jerker its an engag thought provok drama with excel perform from all the cast but especi derek luke and denzel washington 7 year on i m amaz that luke is still a virtual unknown and washington only direct one other film nevertheless apart from a slow build up the stori of this foster child s trial and tribul and how it still affect him in adulthood is the sort of movi that stay with you long after you have seen it like mani fox searchlight pictur this was more of a sleeper hit and didn t get the mass critic acclaim it deserv the scene where antwon final meet his mother sum up the movi for me there were so mani way that could have been done and it could have been all schmaltzi or it could have been unrealist but washington struck exact the right tone his mother never said a word and could only shed a tear while antown ask simpli why her overwhelm guilt prevent her from say anyth what could she say to defend herself one of the most move cinemat scene i have seen'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preprocessed.iloc[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Now onto what we learned in notebook 2, but this time with real vectorizer classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sklearn vectorizers\n",
    "\n",
    "As you can imagine, there are many implementations of Bag of Words and TF-IDF. We're hardcore scikit-learn fans here, so we're going to use scikit-learn implementations. We will look at three different ways to create the features.\n",
    "\n",
    "### 2.1 Bag of words - CountVectorizer\n",
    "The BoW from sklearn is the [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). We still haven't removed stopwords as you may have noticed. We're going to do it with the `CountVectorizer()` - we just pass the parameter `stop_words=english`. You can also give this parameter a custom list of stopwords if your application requires something special (there are many stopwords lists out there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(train_preprocessed)\n",
    "train_bow = vectorizer.transform(train_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the vectorizer is a sparse matrix where the rows are samples and the columns are feature values - word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 350769 stored elements and shape (4000, 23715)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to look into sparse matrices later on, for now just know that it's a matrix with mostly zeros and just a few non-zero elements. Because it is so huge and mostly empty, it doesn't make sense to waste memory to store it in the usual way, so it's storing just the non-zero elements in a very smart way.\n",
    "\n",
    "We can feed this matrix directly into a ML model, but let's examine it first. Seeing the nonzero elements requires a bit of work. We can get the indices of the nonzero elements like this (for a random document number 12):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " array([   44,   511,   599,   635,   744,  1015,  1519,  1551,  1573,\n",
       "         1589,  1641,  1708,  2234,  3842,  4115,  4443,  4465,  4513,\n",
       "         4517,  4559,  4745,  4863,  4873,  5234,  5703,  5927,  6488,\n",
       "         7027,  7695,  7968,  7985,  8198,  8569,  8748,  8932,  9376,\n",
       "        10121, 10210, 10568, 11614, 11628, 11924, 12297, 12576, 12634,\n",
       "        12859, 12883, 13647, 14021, 14117, 14485, 14598, 15080, 15524,\n",
       "        15601, 15774, 16050, 16334, 17064, 17425, 18278, 18506, 18616,\n",
       "        19012, 19013, 19059, 19551, 19937, 20100, 20382, 21079, 21252,\n",
       "        22079, 22628, 23122, 23334, 23476], dtype=int32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow[12].nonzero()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a 2D matrix, so we get two indices, the first being 0 everywhere. These are the values of the non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bow[12][train_bow[12].nonzero()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But for which words are these word counts? The words are given by the indices and the vectorizer provides a vocabulary to translate the indices to words. Let's preview a sample of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of distinct words: 23715\n",
      "Small sample of the vocabulary: [('watch', 22872), ('dvd', 6558), ('movi', 14021), ('come', 4443), ('excel', 7228), ('commentari', 4463), ('track', 21381), ('english', 6939), ('cambodia', 3353), ('subtitl', 20247), ('say', 18224), ('charact', 3820), ('speak', 19613), ('thai', 20953), ('violent', 22628), ('evil', 7203), ('man', 12920), ('rais', 16896), ('boy', 2800), ('killer', 11614)]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = vectorizer.vocabulary_\n",
    "print(\"\\nNumber of distinct words:\", len(vocabulary))\n",
    "print(\"Small sample of the vocabulary:\", list(vocabulary.items())[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 23715 distinct words in our training set reviews and each review contains maybe a few hundred of them. That explains why we need a sparse matrix to store the data.\n",
    "\n",
    "Ok, let's proceed in our quest to take a peak at the features of the first review. This vocabulary maps the words to the indices, so we're going to make an inverse vocabulary mapping the indices to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_vocabulary = {v: k for k, v in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract feature names - the words - and the corresponding word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of non-zero features\n",
    "feat_indices = train_bow[12].nonzero()[1]\n",
    "\n",
    "# Extract the corresponding word and count\n",
    "wordcounts = [(inv_vocabulary[i], train_bow[12,i]) for i in feat_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 12: this is the kind of movi which show the pauciti of french cinema when it come to make thriller the director s desir to sound american is so glare that you will not be fool a minut unless you have not seen a serial killer movi sinc peep tom two male cop or one and a half more like as you will see horribl murder a plot more complic than complex charl berl is not lucki with the genr see the astoundl dumb l inconnu de strasbourg a coupl of year ago the scene with his pregnant wife which are suppos to be a counterpart for the otherwis noir atmospher of the rest of the plot are among the worst ever film add a steami love scene between them and a gori autopsi to get a pg 12 and thus to attract the huge adolesc audienc a violent and absurd conclus follow by a silent epilogu who could make a nice commerci for the côte d azur it s realli the silenc of the lame \n",
      "\n",
      "12 :  1\n",
      "absurd :  1\n",
      "add :  1\n",
      "adolesc :  1\n",
      "ago :  1\n",
      "american :  1\n",
      "astoundl :  1\n",
      "atmospher :  1\n",
      "attract :  1\n",
      "audienc :  1\n",
      "autopsi :  1\n",
      "azur :  1\n",
      "berl :  1\n",
      "charl :  1\n",
      "cinema :  1\n",
      "come :  1\n",
      "commerci :  1\n",
      "complex :  1\n",
      "complic :  1\n",
      "conclus :  1\n",
      "cop :  1\n",
      "counterpart :  1\n",
      "coupl :  1\n",
      "côte :  1\n",
      "desir :  1\n",
      "director :  1\n",
      "dumb :  1\n",
      "epilogu :  1\n",
      "film :  1\n",
      "follow :  1\n",
      "fool :  1\n",
      "french :  1\n",
      "genr :  1\n",
      "glare :  1\n",
      "gori :  1\n",
      "half :  1\n",
      "horribl :  1\n",
      "huge :  1\n",
      "inconnu :  1\n",
      "killer :  1\n",
      "kind :  1\n",
      "lame :  1\n",
      "like :  1\n",
      "love :  1\n",
      "lucki :  1\n",
      "make :  2\n",
      "male :  1\n",
      "minut :  1\n",
      "movi :  2\n",
      "murder :  1\n",
      "nice :  1\n",
      "noir :  1\n",
      "otherwis :  1\n",
      "pauciti :  1\n",
      "peep :  1\n",
      "pg :  1\n",
      "plot :  2\n",
      "pregnant :  1\n",
      "realli :  1\n",
      "rest :  1\n",
      "scene :  2\n",
      "seen :  1\n",
      "serial :  1\n",
      "silenc :  1\n",
      "silent :  1\n",
      "sinc :  1\n",
      "sound :  1\n",
      "steami :  1\n",
      "strasbourg :  1\n",
      "suppos :  1\n",
      "thriller :  1\n",
      "tom :  1\n",
      "unless :  1\n",
      "violent :  1\n",
      "wife :  1\n",
      "worst :  1\n",
      "year :  1\n"
     ]
    }
   ],
   "source": [
    "print('Document 12:', train_preprocessed.iloc[12], '\\n')\n",
    "for word, count in wordcounts:\n",
    "    print(word, \": \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tf-idf - TfidfTransformer\n",
    "We can further process the word counts to a tf-idf vectorization with the [TfidfTransformer()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "tfidf.fit(train_bow)\n",
    "\n",
    "train_tfidf = tfidf.transform(train_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is again a sparse matrix of the same size as the input. The word counts were just transformed to term frequencies scaled by inverse document frequency. If we want to know the feature names, we can use the vocabulary created by the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 350769 stored elements and shape (4000, 23715)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this matrix as input for a model, but before doing that, we're going to create another vectorization with N-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Bag of words with N-grams\n",
    "\n",
    "As we learned in the first notebook, we can use not only single words, unigrams, as features but also a combination of words, known as N-grams. By using `CountVectorizer()` with the parameter `ngram_range`, we can easily get all the possibilities in a chosen range of N. This parameter takes a tuple `(min_n, max_n)`, where `min_n` is the smallest N-gram size (e.g., 1 for unigrams), and `max_n` is the largest N-gram size (e.g., 3 for trigrams). For example, if we wanted to extract all unigrams and bigrams in a corpus we would pass to the model `ngram_range=(1,2)`.\n",
    "\n",
    "Let's get all unigrams, bigrams, and trigrams from our movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 773254)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_123_grams = CountVectorizer(stop_words= 'english', ngram_range=(1,3))\n",
    "vectorizer_123_grams.fit(train_preprocessed)\n",
    "train_bow_123_grams = vectorizer_123_grams.transform(train_preprocessed)\n",
    "train_bow_123_grams.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we end up with a much larger feature space - which can be quite computationally expensive to our model! Fortunately, there are some parameters in `CountVectorizer()` that we can use to reduce our feature space while keeping the representation as informative as possible:\n",
    "\n",
    "- `max_features` - receives an `int` that will be the size of the feature space of the model. The chosen N features will be the ones with the highest term frequency across the corpus. This goes a bit against the idea that rarer words are more interesting, but on the other hand words that are too rare and appear only once in the whole corpus won't bring us too far either.\n",
    "- `min_df` - the minimum document frequency an N-gram can have to be considered. Often called the *cut-off*. Here we are selecting N-grams that appear in a reasonable minimal number of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling pipeline\n",
    "\n",
    "Now that we learned how to create features with sklearn, let's add a model and prepare a workflow to predict if a review is positive or negative. In NLP, we call this task _sentiment analysis_.\n",
    "\n",
    "<img src=\"./media/dwight.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by putting all our workflow into a pipeline.\n",
    "\n",
    "We will use Scikit's implementation of Naive Bayes as the classifier. You can read about this classifier in the optional learning notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english')),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final missing piece is converting the sentiment labels into numeric labels through the Scikit [Label Encoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'])\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'])\n",
    "test_df['sentiment'] = le.transform(test_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our final input dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4227</th>\n",
       "      <td>1</td>\n",
       "      <td>I watched the DVD of this movie which also com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4676</th>\n",
       "      <td>0</td>\n",
       "      <td>You, know, I can take the blood and the sex, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>0</td>\n",
       "      <td>I must say that I am fairly disappointed by th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3671</th>\n",
       "      <td>0</td>\n",
       "      <td>I saw this not too long ago, and I must say: T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>0</td>\n",
       "      <td>*** WARNING! SPOILERS CONTAINED HEREIN! ***&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4426</th>\n",
       "      <td>0</td>\n",
       "      <td>Seeing as I hate reading long essays hoping to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>0</td>\n",
       "      <td>If an auteur gives himself 2 credits before th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>1</td>\n",
       "      <td>Perspective is a good thing. Since the release...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3772</th>\n",
       "      <td>0</td>\n",
       "      <td>The shame of it! There I was, comfortable in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1</td>\n",
       "      <td>**Warning! Mild Spoilers Ahead!**&lt;br /&gt;&lt;br /&gt;(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "4227          1  I watched the DVD of this movie which also com...\n",
       "4676          0  You, know, I can take the blood and the sex, b...\n",
       "800           0  I must say that I am fairly disappointed by th...\n",
       "3671          0  I saw this not too long ago, and I must say: T...\n",
       "4193          0  *** WARNING! SPOILERS CONTAINED HEREIN! ***<br...\n",
       "...         ...                                                ...\n",
       "4426          0  Seeing as I hate reading long essays hoping to...\n",
       "466           0  If an auteur gives himself 2 credits before th...\n",
       "3092          1  Perspective is a good thing. Since the release...\n",
       "3772          0  The shame of it! There I was, comfortable in t...\n",
       "860           1  **Warning! Mild Spoilers Ahead!**<br /><br />(...\n",
       "\n",
       "[4000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we feed the training data into the pipeline and fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 TextCleanerTransformer(regex_list=[(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;)],\n",
       "                                        stemmer=&lt;nltk.stem.snowball.SnowballStemmer object at 0x7fa38a79b920&gt;,\n",
       "                                        tokenizer=WordPunctTokenizer(pattern=&#x27;\\\\w+|[^\\\\w\\\\s]+&#x27;, gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL))),\n",
       "                (&#x27;vect&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;prep&#x27;,\n",
       "                 TextCleanerTransformer(regex_list=[(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;)],\n",
       "                                        stemmer=&lt;nltk.stem.snowball.SnowballStemmer object at 0x7fa38a79b920&gt;,\n",
       "                                        tokenizer=WordPunctTokenizer(pattern=&#x27;\\\\w+|[^\\\\w\\\\s]+&#x27;, gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL))),\n",
       "                (&#x27;vect&#x27;, CountVectorizer(stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">TextCleanerTransformer</label><div class=\"sk-toggleable__content fitted\"><pre>TextCleanerTransformer(regex_list=[(&#x27;&lt;[^&gt;]*&gt;&#x27;, &#x27;&#x27;)],\n",
       "                       stemmer=&lt;nltk.stem.snowball.SnowballStemmer object at 0x7fa38a79b920&gt;,\n",
       "                       tokenizer=WordPunctTokenizer(pattern=&#x27;\\\\w+|[^\\\\w\\\\s]+&#x27;, gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL))</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfTransformer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\">?<span>Documentation for TfidfTransformer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfTransformer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MultinomialNB<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.naive_bayes.MultinomialNB.html\">?<span>Documentation for MultinomialNB</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MultinomialNB()</pre></div> </div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('prep',\n",
       "                 TextCleanerTransformer(regex_list=[('<[^>]*>', '')],\n",
       "                                        stemmer=<nltk.stem.snowball.SnowballStemmer object at 0x7fa38a79b920>,\n",
       "                                        tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=re.UNICODE|re.MULTILINE|re.DOTALL))),\n",
       "                ('vect', CountVectorizer(stop_words='english')),\n",
       "                ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(train_df['text'], train_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we predict on the test set and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(test_df['text'])\n",
    "np.mean(predicted == test_df['sentiment'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!\n",
    "\n",
    "Let's try to use also bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_bigrams = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf_bigrams.fit(train_df['text'], train_df['sentiment'])\n",
    "\n",
    "predicted_bigrams = text_clf_bigrams.predict(test_df['text'])\n",
    "np.mean(predicted_bigrams == test_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the performance on the test set got worse! This is an example of when it can be hurtful to your model to remove stopwords. If you look at the stopwords list, words like \"no\" are part of it. This can be crucial to the bigram representation since if we remove them, relevant bigrams will not appear in the feature space (ex.: \"no fun\").\n",
    "\n",
    "Let's remove the `stop_words` parameter from the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.837"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_bigrams_stopwords = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "\n",
    "text_clf_bigrams_stopwords.fit(train_df['text'], train_df['sentiment'])\n",
    "\n",
    "predicted_bigrams_stopwords = text_clf_bigrams_stopwords.predict(test_df['text'])\n",
    "np.mean(predicted_bigrams_stopwords == test_df['sentiment'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better, but still not as good as the unigram model. This can be due to the feature space having too many dimensions, which can hurt model performance. Fortunately, we learned that `CountVectorizer()` has parameters that help to reduce feature dimensionality.\n",
    "\n",
    "You should try and see if you can get higher accuracy by reducing dimensionality with `max_features` and `min_df` (Spoiler alert: you will!). You can then also try to use trigrams in addition to unigrams and bigrams.\n",
    "\n",
    "You can also try leaving out the tfidf transformation and use just word counts as features.\n",
    "\n",
    "Try out different classifiers and play around with `CountVectorizer()` and `TfidfTransformer()` parameters to see if you can get better scores!\n",
    "\n",
    "And when you're ready, move to the exercise notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
