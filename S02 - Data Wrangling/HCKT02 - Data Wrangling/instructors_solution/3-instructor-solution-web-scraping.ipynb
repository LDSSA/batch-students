{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a93ec458-a2e1-4849-baaf-c10e52d0af63",
   "metadata": {},
   "source": [
    "# DATA WRANGLING HACKATHON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e2b150-03c8-4a65-a81e-f8d0c96795c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# WEB SCRAPING STEP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258a7c2-5351-4d3b-a39d-ec0fc6c45dce",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This data dictionary describes High Volume FHV trip data. Each row represents a single trip in an FHV dispatched by one of NYC’s licensed High Volume FHV bases. On August 14, 2018, Mayor de Blasio signed Local Law 149 of 2018, creating a new license category for TLC-licensed FHV businesses that currently dispatch or plan to dispatch more than 10,000 FHV trips in New York City per day under a single brand, trade, or operating name, referred to as High-Volume For-Hire Services (HVFHS). This law went into effect on Feb 1, 2019.\n",
    "\n",
    "### Objective\n",
    "The main goal of this hackathon is to determine if the client is going to give a tip. \n",
    "Your submission file should be a CSV file with two columns (see example in sample_\tsubmission.csv):\n",
    "ID:  Id of the observation\n",
    "Tipped: If the client Tipped or not\n",
    "\n",
    "A dataset spread over several data sources has been provided for you. The total number of features is plentiful and it’s up to you to use as many or as little as you want. Given that, some features might be more relevant than others. \n",
    "Keep in mind that this is a Data Wrangling specialization. \n",
    "\n",
    "### Datasets:\n",
    "| **Dataset** | **Information**   | Location|\n",
    "|-------------|-------------------|---------------------|\n",
    "|API          | Trip Mileage      | https://hckt02-api.lisbondatascience.org/docs#/default/get_data_data_get |\n",
    "|Webpage      | Taxi Zone Data    | https://s02-infrastructure.s3.eu-west-1.amazonaws.com/hackathon-02-batch8/index.html |\n",
    "|Files        | Detailed Trip Data| https://drive.google.com/drive/folders/12MhOAVrplggHVTm6-CtjqkkjI9xrVPek?usp=drive_link|\n",
    "|Database     | Weather Data      | batch-s02.ctq2kxc7kx1i.eu-west-1.rds.amazonaws.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb65f08-070b-4103-a156-814d71ad6c30",
   "metadata": {},
   "source": [
    "# Selenium WebDriver API\n",
    "Selenium is a widely used automation tool primarily designed for testing web applications. It allows developers and testers to simulate browser actions, such as navigating to web pages, interacting with elements, and validating functionality, without manual intervention. \n",
    "\n",
    "Beyond testing, Selenium is also commonly employed for web scraping. Its ability to mimic human interactions with dynamic web pages, including those that rely heavily on JavaScript, makes it a powerful tool for extracting data from websites where traditional scraping methods may fall short. This versatility allows users to scrape data even from sites with complex structures or interactive features.\n",
    "\n",
    "### API docs: \n",
    "https://www.selenium.dev/selenium/docs/api/py/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41eb5b0f-40be-499a-966a-fe08e851fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b80ea7-6534-4156-a3cc-30f4b4660c75",
   "metadata": {},
   "source": [
    "## Code Modularization\n",
    "### We'll apply a little modularization to the code to improve maintenance and readability by creating some functions in Python language for all extraction operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1c086f-6b16-4e99-bcaf-86715cb58d85",
   "metadata": {},
   "source": [
    "## Selenium WebDriver initialization\n",
    "### Initializes the Chrome driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc458a9c-8d0e-40b6-bdf7-6fc52f4dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_driver():\n",
    "    return webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f567932-e282-42d6-89bd-0ef2afe79665",
   "metadata": {},
   "source": [
    "# Webpage Extraction Step\n",
    "### Opens the page and checks for title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "881b552e-b5bf-4852-b644-b70bba85df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_page(driver, url, title_check):\n",
    "    driver.get(url)\n",
    "    assert title_check in driver.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29914882-1a41-4908-a798-5a64797a793a",
   "metadata": {},
   "source": [
    "## Extract Table Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01471518-2578-46ff-9ccb-8e4fe213750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_header(driver):\n",
    "    # Extracts the cols names from the webpage\n",
    "    headers = driver.find_elements(By.TAG_NAME, \"th\")\n",
    "    col_names = [th.text for th in headers]\n",
    "    # print(col_names)\n",
    "    return col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a591f63-c1f4-4528-b6d3-9cb986b677a5",
   "metadata": {},
   "source": [
    "## Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea41722b-7a55-40d9-8fb9-26e7a66b8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_data(driver):\n",
    "    \"\"\"Extracts the data from the table..\"\"\"\n",
    "    elements = driver.find_elements(By.TAG_NAME, \"td\")\n",
    "    data = []\n",
    "    row = []\n",
    "    for i, td in enumerate(elements):\n",
    "        row.append(td.text)\n",
    "        if (i + 1) % 4 == 0:  # Considering 4 columns per row and dividing by 4, a remainder of zero gives us the start of the next row.\n",
    "            data.append(row)\n",
    "            row = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f297308-7672-4006-bb8a-c527356406ed",
   "metadata": {},
   "source": [
    "## Generating a Pandas DataFrame for the Webpage data\n",
    "### We need the data in data frames format to operate the merges when possible in order to use the data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb0ff01b-bd12-488d-ba1f-c2b0f1840eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ea692-d5d5-49f3-9ac8-7d0342e48e78",
   "metadata": {},
   "source": [
    "## Control Function - main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "256ec699-9575-47c0-9d6f-136dfdd7e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    driver = initialize_driver()\n",
    "    try:\n",
    "        url = \"https://s02-infrastructure.s3.eu-west-1.amazonaws.com/hackathon-02-batch8/index.html\"\n",
    "        open_page(driver, url, \"Taxi Zone Data - Full\")\n",
    "\n",
    "        col_names = extract_table_header(driver)\n",
    "        # print(f\"Column Names: {col_names}\")\n",
    "        \n",
    "        web_data = extract_table_data(driver)\n",
    "        # print(f\"Data: {data}\")\n",
    "        \n",
    "        # generating a data frame with the collected data for later use\n",
    "        df_webpage = pd.DataFrame(data=web_data, columns=col_names)\n",
    "\n",
    "        # returning the scraped data\n",
    "        return df_webpage\n",
    "        \n",
    "        # Checks for page errors\n",
    "        assert \"No results found.\" not in driver.page_source\n",
    "    finally:\n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2223f9e-7b7f-4901-a1ac-8e03791103a5",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "### Column header and data will be extracted and added to a data frame for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4998ca2-7447-4cd9-9d16-8799c2b63f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_webpage = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab358345-6d76-4d31-a57a-6359f548eafc",
   "metadata": {},
   "source": [
    "## Checking data frame content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e2e9a7c-246d-4cdb-959f-6713b8112bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  LocationID Borough            Zone service_zone\n",
       "0          1     EWR  Newark Airport          EWR\n",
       "1          2  Queens     Jamaica Bay    Boro Zone"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_webpage.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d839f189-dfcf-4704-96a0-7f932ff03212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def filesize(filename):\n",
    "    file_path = filename\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    message = f\"physical file size: {file_size:.2f} MB\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "def82dfa-e623-49f7-9fc3-72120bff8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_webpage.to_parquet(\".data/webpage/bronze/webpage_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55de9508-edc9-460e-a529-4ea35d8977c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\".data/webpage/bronze/webpage_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3828b4f3-44ac-4256-bf84-e5407ac10279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>264</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NV</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>265</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LocationID        Borough                     Zone service_zone\n",
       "0            1            EWR           Newark Airport          EWR\n",
       "1            2         Queens              Jamaica Bay    Boro Zone\n",
       "2            3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3            4      Manhattan            Alphabet City  Yellow Zone\n",
       "4            5  Staten Island            Arden Heights    Boro Zone\n",
       "..         ...            ...                      ...          ...\n",
       "260        261      Manhattan       World Trade Center  Yellow Zone\n",
       "261        262      Manhattan           Yorkville East  Yellow Zone\n",
       "262        263      Manhattan           Yorkville West  Yellow Zone\n",
       "263        264        Unknown                       NV          NaN\n",
       "264        265        Unknown                      NaN          NaN\n",
       "\n",
       "[265 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1859a5f-686e-4b3e-8d87-335101b634d3",
   "metadata": {},
   "source": [
    "## FINISHED WEB SCRAPING COMPUTATION\n",
    "\n",
    "**Conclusion:** Now we'll work on other data sources to bring everything together and create a single file for the ML model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12faee4a-5d15-49c4-9374-4c6d521ed729",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc619ed-3113-4f14-a968-471ae3552ff6",
   "metadata": {},
   "source": [
    "### Other Web Scraping Options:\n",
    "\n",
    "1. **BeautifulSoup**  \n",
    "   **Description**: A Python library that simplifies the extraction of information from HTML and XML files.  \n",
    "   **Use**: Ideal for simple scraping tasks where the content is static.  \n",
    "   **Limitation**: Not suitable for handling dynamic pages generated by JavaScript.  \n",
    "\n",
    "2. **Scrapy**  \n",
    "   **Description**: A robust Python framework for web scraping and crawling.  \n",
    "   **Use**: Allows the creation of spiders that traverse entire websites to extract structured data.  \n",
    "   **Advantage**: Extremely efficient and well-suited for complex scraping projects.  \n",
    "   **Limitation**: Does not include native support for interacting with JavaScript; it focuses more on static content.  \n",
    "\n",
    "3. **Puppeteer**  \n",
    "   **Description**: A Node.js library that controls the Google Chrome or Chromium browser via API.  \n",
    "   **Use**: Ideal for scraping dynamic pages that rely on JavaScript.  \n",
    "   **Advantage**: Offers full control over the browser with less overhead compared to Selenium.  \n",
    "   **Limitation**: Written in JavaScript/Node.js, which can be a barrier for Python users.  \n",
    "\n",
    "4. **Octoparse**  \n",
    "   **Description**: A visual scraping tool that requires no programming knowledge.  \n",
    "   **Use**: Designed for users who prefer a graphical interface to configure data extractions.  \n",
    "   **Advantage**: Very intuitive, with support for both static and dynamic pages.  \n",
    "   **Limitation**: May be less flexible than programmatic libraries.  \n",
    "\n",
    "5. **ParseHub**  \n",
    "   **Description**: Another visual scraping tool, focused on dynamic and interactive websites.  \n",
    "   **Use**: Popular among users who need a quick, no-code solution.  \n",
    "   **Advantage**: Easy to use and cloud-based.  \n",
    "   **Limitation**: May be limited for more customized projects.  \n",
    "\n",
    "6. **Apify**  \n",
    "   **Description**: A cloud-based platform for web scraping and automation.  \n",
    "   **Use**: Provides a user-friendly interface for creating crawlers and automating data extraction.  \n",
    "   **Advantage**: Integration with multiple programming languages and support for JavaScript scripts.  \n",
    "   **Limitation**: Higher costs for large data volumes.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
