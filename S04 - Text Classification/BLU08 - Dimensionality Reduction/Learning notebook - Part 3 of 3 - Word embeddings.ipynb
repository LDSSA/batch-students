{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Word vectors\n",
    "\n",
    "So far we've seen simple feature selection methods, a statistical feature selection approach, dimensionality reduction techniques such as PCA and SVD, but in the last few years, with the rise in popularity of neural networks, a new technique has become the state of the art for representing words in NLP tasks.\n",
    "\n",
    "This technique is commonly referred to as **word vectors** or **word embeddings**, and its inner workings are really simple. It consists of defining a vocabulary and a vector for each word in it with a maximum number of dimensions. Then all the vectors are found through the use of **neural networks** and we can use them off-the-shelf. In essence, word embeddings try to capture information on a word's meaning and usage. This not only allows us to significantly reduce the number of features fed to our models, but it also allows meaningful and easy transferable representations across data sets. \n",
    "\n",
    "Pretty cool, huh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/what-year-is-this.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word vectors explained\n",
    "\n",
    "First of all, by now you could be thinking: \"But wait Doc, didn't I get a bunch of vectors before also?\". Why yes, yes you did, Marty. You could consider the columns of the matrix with document-term counts a possible word vector representation. You could construct an even simpler matrix. \n",
    "\n",
    "If you assume a vocabulary of size V, and each word having an index in this vocabulary, a natural representation would be a one-hot encoding, where each word is represented by a vector of size V - the vocabulary size - with the single component corresponding to this word set to 1, and the remaining fields zeroed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media/one-hot-vec.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going in the right direction! But keep in mind that this representation fits in a very large space and we suddenly fall into the pitfalls of high dimensionality. You could think of applying PCA or SVD to these one-hot vectors but as for most tasks nowadays, neural networks have proven to be better at this. To put it simply, there is a more elegant way. \n",
    "\n",
    "<img src=\"media/but-how-doc.jpg\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Training word vectors\n",
    "\n",
    "So you know the data - a bunch of words. You know the goal - a vector with K features. And you know the means - neural networks. So how does it all work? \"You shall know a word by the company it keeps\". These are the words of John Rupert Firth (at least according to Wikipedia), and they are the basis of the following method - Word2vec. \n",
    "\n",
    "**Word2vec** is a popular technique to produce word embeddings with neural networks, and it encompasses two main approaches - Continuous Bag Of Words (CBOW) and skipgram - that work as follows.\n",
    "\n",
    "Initially, we center a window of length n around each word in the training text. The word at the center is called the center word and the rest are context words. Each window will produce a training example that we will plug into a neural network. There are two approaches to the training:\n",
    "\n",
    "1 - **CBOW**: the input words are the context words, and we predict the center word \n",
    "\n",
    "2 - **Skip-gram**: complementary to the previous method, the input is the center word, and the predictions are the context words\n",
    "\n",
    "The values we are trying to predict are called the **weights** of the neural network, and they will map to our word vectors directly. We aren't going to deepdive into neural networks at the moment, and there are definitely more details on how to set up these models, but the basic intuition can be seen in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/word2vec.png\" width=\"500\">\n",
    "\n",
    "The projection layer contains these **weights** we mentioned, which we'll iteratively train based on a large amount of data so that we get strong word vectors at the end of training.\n",
    "\n",
    "Remember this next time you use chatGPT - it is only putting together the most probable chain of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pretrained word vectors\n",
    "\n",
    "The best thing about these vectors however, is that they are universal and ready to use off the shelf. They were trained on a huge amount of text data in the same language and we just take them and use them for our task. It saves us the time and effort of gathering, processing and training on our own data.\n",
    "\n",
    "One set of such pretrained vectors is from **Spacy**. [Spacy](https://spacy.io) is a toolkit similar to NLTK, but it contains word embedding trained with deep learning models and it typically has better performance for industrial applications. The pretrained word vectors are easy to use out of the box by importing the Spacy library.\n",
    "\n",
    "Spacy has several library versions with different vocabulary and vector size, we are loading the medium one. You can try to switch between versions in the following experiments and see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the previous cell throws an error, the library has not been downloaded during the virtual environment creation, so you need to do it manually by running `python -m spacy download en_core_web_md` in your terminal.\n",
    "\n",
    "There are other libraries of word vectors out there, such as [FastText](https://fasttext.cc) and [Glove](https://nlp.stanford.edu/projects/glove/). These all provide good quality embeddings for NLP tasks. Their training methods are usually based on the Word2vec, but there are differences in the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word representations in Spacy\n",
    "\n",
    "Now let's dig into the vectors and see what we can get from them. We can start by seeing the representation for a particular word, for example *house*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.5073e-01,  8.1650e-02,  9.0288e-02, -3.4719e-01, -6.0598e-01,\n",
       "       -2.6782e-02, -1.7644e-01, -4.5973e-01,  4.8586e-01,  2.9120e+00,\n",
       "       -8.2821e-01,  4.4448e-01,  1.4028e-01,  1.1009e-01, -4.5023e-01,\n",
       "       -2.1889e-01, -4.8917e-01,  7.4006e-01,  1.5316e-01,  5.5353e-01,\n",
       "        9.6078e-02,  1.7717e-01,  2.0261e-02, -3.5839e-02, -4.3881e-02,\n",
       "       -1.1955e-01, -1.9034e-02, -3.0087e-01, -5.3294e-03, -1.6692e-01,\n",
       "        1.7790e-01, -4.8102e-01,  1.5397e-01,  1.5131e-01,  1.2383e-01,\n",
       "       -1.0739e-01, -9.7154e-02, -1.4213e-01,  1.8673e-01,  2.1388e-01,\n",
       "       -1.6718e-01,  4.0141e-01,  4.9244e-01, -7.0160e-01, -1.6582e-01,\n",
       "        1.9418e-01,  2.6764e-01, -3.4181e-01, -1.5499e-01,  2.2845e-01,\n",
       "       -5.2560e-02,  2.7192e-01, -2.0114e-01,  3.4382e-02,  3.0014e-01,\n",
       "       -1.8081e-01,  1.8739e-01, -1.0126e-01, -3.3541e-01,  6.5063e-03,\n",
       "       -9.6152e-02, -3.9275e-01,  8.8390e-02, -1.4326e-01,  5.5553e-01,\n",
       "       -2.0286e-01,  4.2895e-01,  6.1838e-01,  3.3416e-01, -4.7662e-01,\n",
       "        2.7773e-01,  1.9445e-01,  3.4664e-01, -1.0697e-01, -3.4542e-01,\n",
       "        3.4373e-01,  6.2362e-01, -6.4473e-02,  9.1416e-02,  4.2528e-01,\n",
       "       -2.0646e-01,  2.4770e-01, -1.3523e-01, -7.3583e-02, -4.1975e-01,\n",
       "        6.9260e-02, -2.8202e-02,  9.9520e-03,  5.8684e-01, -1.1167e-01,\n",
       "        3.8202e-02, -6.3212e-02, -4.1043e-01, -2.3745e-01, -1.7620e-01,\n",
       "       -3.8275e-02, -2.5440e-01, -1.3288e-01, -4.7841e-01, -5.0255e-03,\n",
       "       -3.1475e-01, -6.8493e-02, -6.2857e-01, -1.3137e-01,  8.4347e-02,\n",
       "       -1.0206e+00,  1.1359e-01,  3.6712e-01,  1.2456e-03,  1.2298e-01,\n",
       "       -3.9581e-02, -3.1181e-01,  1.0710e-01, -1.3516e-01,  1.5723e-01,\n",
       "       -1.3278e-01, -2.2609e-01, -4.5392e-01, -7.1908e-03,  4.3692e-03,\n",
       "        2.6999e-01,  3.3890e-01,  5.9122e-02,  9.6725e-02, -3.0617e-01,\n",
       "       -3.5927e-01, -3.9494e-01,  6.5454e-02, -9.1461e-02, -1.3096e-01,\n",
       "        1.6288e-01,  3.4960e-01,  1.6886e-01, -3.3125e-01,  9.1410e-03,\n",
       "        1.0708e-01, -1.8034e-01, -1.9940e-01, -3.2442e-01, -4.3343e-01,\n",
       "       -1.4761e+00,  3.4623e-01,  3.8946e-01, -1.3654e-01, -1.5954e-01,\n",
       "       -3.2569e-01,  2.3179e-01,  5.1308e-01, -8.4581e-02,  1.5220e-01,\n",
       "        2.6776e-01,  1.7460e-02, -2.1571e-01, -3.7438e-01,  1.4138e-01,\n",
       "       -5.2014e-02,  2.6114e-01,  9.6760e-02, -6.8415e-01,  1.0464e-01,\n",
       "        3.3308e-01,  2.8728e-01, -5.6032e-02,  2.1836e-01,  1.2074e-01,\n",
       "       -8.8215e-02,  4.8419e-01,  3.1212e-02,  4.5606e-01,  2.5513e-01,\n",
       "       -3.4513e-01, -2.8589e-01,  4.6174e-01,  4.1577e-03,  1.4968e-01,\n",
       "        3.4966e-01, -1.8795e-01,  3.5592e-01,  3.0900e-01,  4.5104e-01,\n",
       "        5.1843e-01, -5.2481e-02, -2.2508e-01, -8.7954e-03, -2.5836e-01,\n",
       "        4.1106e-01, -6.5868e-01, -3.2066e-01,  1.3824e-01, -7.3236e-02,\n",
       "        3.5861e-01, -1.0987e-01, -8.8021e-02, -2.9454e-01,  2.9620e-01,\n",
       "       -2.2191e-01, -4.0455e-02,  3.2829e-01,  2.1456e-01,  4.2981e-01,\n",
       "        4.0900e-01, -5.4774e-01,  3.0733e-01, -2.0681e-01, -5.1839e-01,\n",
       "       -3.2243e-01,  5.2888e-01, -2.8582e-02,  2.4299e-01,  9.2166e-02,\n",
       "        2.9088e-01,  3.7548e-01, -2.9645e-01,  1.0666e-01,  6.9564e-01,\n",
       "       -4.4895e-01,  5.2755e-02, -9.0064e-03, -6.6103e-01,  2.9655e-01,\n",
       "       -3.1708e-01, -8.4710e-02, -5.3129e-01, -3.7884e-01, -2.6507e-01,\n",
       "       -4.2389e-01, -2.0696e-01,  6.8911e-01, -9.2065e-02,  1.4784e-01,\n",
       "        5.1440e-01, -4.4691e-01,  1.1699e-01, -7.4862e-02, -2.9763e-01,\n",
       "        1.0319e-01,  2.6896e-01,  1.7953e-01, -4.7203e-01,  2.5994e-01,\n",
       "       -1.5207e-02, -6.9744e-02,  5.8417e-02,  4.1050e-01,  4.8497e-01,\n",
       "       -4.7363e-01, -1.2660e-01, -3.7049e-01, -2.3683e-01,  4.1325e-01,\n",
       "        4.2893e-01, -1.0476e-01,  2.5687e-02, -1.9751e-01,  5.0634e-02,\n",
       "        8.0262e-01, -4.8974e-01, -3.9151e-01, -1.2298e-01,  2.1623e-01,\n",
       "       -1.7469e-01,  5.6030e-01, -1.6414e-01,  3.3723e-01,  4.6297e-01,\n",
       "       -5.3414e-01, -3.7214e-02, -2.4500e-01,  5.4923e-01,  4.2396e-02,\n",
       "        1.3729e-01, -3.9566e-01, -4.1200e-03, -2.1788e-01, -1.4594e-01,\n",
       "       -3.3395e-01,  1.0686e-01, -3.0278e-01,  1.2290e-01, -1.5734e-01,\n",
       "       -2.5648e-01, -1.3100e-01, -1.2962e-01,  5.4344e-01,  5.5791e-03,\n",
       "        2.1595e-01,  1.3850e-01, -1.8448e-01, -5.5459e-01,  1.4897e-01,\n",
       "       -6.2992e-01, -3.4711e-02,  9.6065e-02, -1.6232e-01,  2.7856e-01,\n",
       "        4.9664e-02,  9.3182e-02, -2.5006e-01,  3.5672e-02,  4.5065e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('house').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function to get a word vector just to make it easier and avoid rewriting the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the size of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('house').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word embeddings are 300-dimensional, or, in other words, they have 300 features. We'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cosine similarity\n",
    "\n",
    "As the words are represented as vectors, we can measure similarities between words with cosine similarity. The cosine similarity is a measure of similarity between vectors expressed as the cosine of the angle between them. It is defined by the following expression:\n",
    "\n",
    "$$\\text{cos-similarity} = \\frac{A \\cdot B}{\\| A \\| \\| B \\|}$$\n",
    "\n",
    "More similar vectors point to a similar direction, so the angle between them is low and the cosine similarity is high. The values of cosine similarity are between -1 and 1. At 1, the vectors are pointing in the same direction, at 0, they are perpendicular, and at -1, they are pointing in opposite directions. It is very easy to see this in the 2D plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/cosine.png\" width=\"400\">\n",
    "\n",
    "In this example, there are three animals with two features - if the animal lives in the woods and how much it hunts. The vectors represent where each animal is in this feature space. If the vectors are closer together, the animals are more similar.\n",
    "\n",
    "Let's define a function to compute cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out. Using cosine similarity, closer words - like *house* and *home* - should have higher scores. On the other hand words with different meanings, even if they are close in terms of characters - like *house* and *mouse* - should produce a low score, if our word vectors really capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47925475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009982352"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec('house'), vec('mouse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, *house* is closer to *home* than it is to *mouse*. Makes sense!\n",
    "\n",
    "<img src=\"./media/future.jpg\" width=\"400\">\n",
    "\n",
    "Once again, to simplify our next steps, let's create a function that gets us the closest words to the word that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10, dont_include_list=[]):\n",
    "    return sorted([(x, cosine(vec_to_check, vec(x))) for x in token_list if x not in dont_include_list],\n",
    "                  key=lambda x: x[1],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply this function in further examples. To simplify a bit, let's limit the vocabulary to the one from the Twitter data set we used in the previous learning notebook. We can then find the closest words from this data set to the word *house*.  We start by reading the dataset and getting its vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.Tweet)\n",
    "\n",
    "tweet_vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the 10 closest words to *house*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 1.0000001),\n",
       " ('making', 1.0000001),\n",
       " ('your', 1.0000001),\n",
       " ('invest', 1.0000001),\n",
       " ('investing', 1.0000001),\n",
       " ('own', 1.0000001),\n",
       " ('saving', 1.0000001),\n",
       " ('funds', 1.0000001),\n",
       " ('keep', 1.0000001),\n",
       " ('fund', 1.0000001)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              vec('house'),\n",
    "              dont_include_list=['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting, so 'house' is all about owning and investing nowadays.\n",
    "\n",
    "## 4. Word relations\n",
    "\n",
    "There is much more that we can do to show you that these vectors capture the meaning, or at least some semantic information, of our vocabulary. Hopefully, if you still don't believe it, this will help. For example, what do you think will happen if we subtract man from king and add woman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.7344864),\n",
       " ('black', 0.7344864),\n",
       " ('gal', 0.7344864),\n",
       " ('doll', 0.7344864),\n",
       " ('actress', 0.7344864),\n",
       " ('nurse', 0.6498234),\n",
       " ('chick', 0.6498234),\n",
       " ('latin', 0.63816786),\n",
       " ('latina', 0.63816786),\n",
       " ('mature', 0.63816786)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              vec('king') - vec('man') + vec('woman'),\n",
    "              dont_include_list=['king', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-2.png\" width=\"300\">\n",
    "\n",
    "And what is the mean between morning and evening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scenic', 0.7720237),\n",
       " ('tour', 0.7720237),\n",
       " ('breakfast', 0.7720237),\n",
       " ('afternoon', 0.7720237),\n",
       " ('trip', 0.7720237),\n",
       " ('bar', 0.7720237),\n",
       " ('lunch', 0.7720237),\n",
       " ('restaurant', 0.7720237),\n",
       " ('guests', 0.7720237),\n",
       " ('dinner', 0.7720237)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              np.mean(np.array([vec('morning'), vec('evening')]), axis=0),\n",
    "              dont_include_list=['morning', 'evening'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-3.png\" width=\"300\">\n",
    "\n",
    "\n",
    "What the sky is to blue, the grass is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hay', 0.6579658),\n",
       " ('hollow', 0.6579658),\n",
       " ('comb', 0.6579658),\n",
       " ('tent', 0.6579658),\n",
       " ('hats', 0.6579658),\n",
       " ('blanket', 0.6579658),\n",
       " ('hat', 0.6579658),\n",
       " ('sand', 0.6579658),\n",
       " ('tie', 0.6579658),\n",
       " ('cotton', 0.6579658)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('blue') - vec('sky') + vec('grass'),\n",
    "              dont_include_list=['blue', 'sky', 'grass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-4.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying word vectors to sentences\n",
    "\n",
    "There are several ways to construct a sentence representation from these vectors, such as:\n",
    "\n",
    "* sum\n",
    "* average \n",
    "* concatenation\n",
    "\n",
    "The average is a good enough approach to start with, so let's implement a function to get the sentence vector representation from the average of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    sent_vec = np.array([w.vector for w in sent])\n",
    "    if len(sent_vec)>0:\n",
    "        return np.mean(sent_vec, axis=0)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the same logic to get the closest sentence according to the sentence representation we chose. Below you have the implementation of the previous function that used cosine similarity, but for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(sentvec(x), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the closest sentences from the Twitter data for the sentence 'i am against the trump administration .'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am opposed to this proposal and will fight to keep fair and just .\n",
      "---\n",
      "rt : we cannot allow this # citizenship question to pit communities against each other . we all have a stake in this fight . we must …\n",
      "---\n",
      "rt : \" it is sad that we have to continue to remind this administration that immigrants founded this country and have fought …\n",
      "---\n",
      "i am not the least bit surprised that is pushing to weaken # airquality regulations . we must stand for # environmentaljustice !\n",
      "---\n",
      "congressional republicans and the trump administration must realize that now is the time for progress – not rollbacks . # earthday\n",
      "---\n",
      "rt : we need to respect the work people do . a man in wv told me his brother goes 6 feet underground to get coal so i can have …\n",
      "---\n",
      "senator mccain has faced every battle in his life with dignity , respect and heroism .\n",
      "---\n",
      "rt : once again administration showing inhumanity . we need compassion ! thank you for stepping u …\n",
      "---\n",
      "“ and i am sure that one day the united states will come back and join the paris climate agreement !”\n",
      "---\n",
      "rt : , thanks for engaging with the environmental justice community , clean air advocates , and everyone who likes t …\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for sent in spacy_closest_sent(df.Tweet.values[:2000], \"i am against the trump administration .\"):\n",
    "    print(sent)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked quite well, wouldn't you agree, Marty?\n",
    "\n",
    "If you are still not convinced about this, you can try to project all your vectors into a 2D space (by applying PCA, for example) and convince yourself that words are somewhat organized by meaning and we can extract word relations from their distances. If you project your vectors, you should get something similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. NLP practical example\n",
    "\n",
    "All that is left is to use these new word representations as the features for our model. We start by defining a function to build sentence vectors for the Twitter data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vecs(docs):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec(doc)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get a baseline (it should match the one from the previous notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5062994960403168\n"
     ]
    }
   ],
   "source": [
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get baselines for our previous methods - SVD and PCA. We'll use 300 components so that we can compare with the word vector technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncated SVD Accuracy: 0.6040316774658028\n",
      "PCA Accuracy: 0.599712023038157\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=seed)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_test_svd =  svd.transform(X_test)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_svd, y_train)\n",
    "y_pred = clf.predict(X_test_svd)\n",
    "print('Truncated SVD Accuracy: {}'.format(accuracy_score(y_pred, y_test)))\n",
    "\n",
    "pca = PCA(n_components=300, random_state=seed)\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "pca.fit(X_train_dense)\n",
    "X_train_pca = pca.transform(X_train_dense)\n",
    "X_test_pca =  pca.transform(X_test_dense)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print('PCA Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 300 features, PCA and SVD have a reasonable accuracy. Now let's split the data and build the vectors. We'll print the shape of the output vector - you should see that our feature vector now has 300 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12963, 300)\n"
     ]
    }
   ],
   "source": [
    "#calculate sentence vectors for each tweet\n",
    "X_train = build_sentence_vecs(train_data.Tweet.values)\n",
    "X_test = build_sentence_vecs(test_data.Tweet.values)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same model and see how much accuracy we can get out of our 300 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5890928725701944\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit worse than SVD and PCA. But we can go further, let's try to remove stopwords from the equation.\n",
    "\n",
    "First we need to download the set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maria/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.615370770338373\n"
     ]
    }
   ],
   "source": [
    "# Redefine functions to use stopwords information\n",
    "\n",
    "def sentvec_tfidf(s, stopwords):\n",
    "    sent = nlp(s)\n",
    "    sent_vec = np.array([w.vector for w in sent if w.text not in stopwords])\n",
    "    if len(sent_vec)>0:\n",
    "        return np.average(sent_vec, axis=0)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def build_sentence_vecs_tfidf(docs, stopwords):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec_tfidf(doc, stopwords)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)\n",
    "\n",
    "# Run with english stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "X_train = build_sentence_vecs_tfidf(train_data.Tweet.values, stopwords)\n",
    "X_test = build_sentence_vecs_tfidf(test_data.Tweet.values, stopwords)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, train_data.Party)\n",
    "pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got a little bit more accuracy with this simple improvement and got ahead of SVD and PCA.\n",
    "\n",
    "## 7. Final remarks\n",
    "\n",
    "In this last part, we've shown you that word vectors are pretty useful and intuitive, keeping meaningful information about words in a compact feature space. If you wish to dig further into these word representations we suggest this [paper](https://arxiv.org/pdf/1301.3781.pdf). As before, take into consideration that although word vectors can be used as an out-of-the-box solution for several NLP tasks, you still need work on a few things that can affect the model performance. So once again, you should still be careful with:\n",
    "\n",
    "- initial text preprocessing\n",
    "- choice of classifier\n",
    "- parameter selection.\n",
    "\n",
    "Neural networks show extremely good performance for most NLP tasks, and if you really want to get into this field, you should learn more about that. However, these basic techniques are essential to understand some of the reasoning when handling text and can still prove quite useful.\n",
    "\n",
    "And that's it for this BLU. You have come out on the other side with a much wider view of the different methods you can use when handling features in NLP (and outside NLP) in a high dimensional space. There is so much more, but these basic tools should suffice for you to start working with text data and to understand more complex approaches built on top of these methods. See you in the next BLU!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/see-you-in-the-future.png\" width=\"500\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
