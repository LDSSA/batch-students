{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7527be455d372803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy\n",
    "seed = 42\n",
    "\n",
    "plt.rcParams['figure.figsize']=[4.8,3.6]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aba29a0b918c734f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In times where even [facebook goes down](https://www.theguardian.com/technology/2021/oct/04/facebook-instagram-whatsapp-outage-what-to-do), books are our reliable companions. For true book lovers, it's hard to resist the urge to buy huge piles to add to the \"To read\" list. But how do you know which book to buy?\n",
    "\n",
    "In any online book store, you can find hundreds of reviews. You also have [goodreads](https://www.goodreads.com/), [book riot](https://bookriot.com/), and many others.\n",
    "\n",
    "The only problem is that it is hard to navigate through the hundreds and hundreds of reviews, and even if you did have the time, the huge amount of reviews is completely unhelpful. Not to mention the ones that definitely seem posted by bots.\n",
    "\n",
    "<img src=\"media/fake-reviews.jpg\" width=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 1 - Baseline \n",
    "\n",
    "To cope with all of this, you set out to create a model which will find the most helpful reviews for you. You find a data set with reviews and helpfulness metrics online and start there.\n",
    "\n",
    "Load the data set and check its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6273f8aa182f664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/book_review_helpfulness.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b545fd65caa490e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Prepare the train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b44c5eae9658f0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['reviewText'], \n",
    "    df['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0f698ca70b9e015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1 - Create features\n",
    "\n",
    "First thing to do is to create features from the review text. You end up following this workflow:\n",
    "\n",
    "- tokenize the text using a `WordPunctTokenizer`\n",
    "- lowercase the text\n",
    "- vectorize the text using a `CountVectorizer`.\n",
    "\n",
    "Implement the function below which applies these steps to the train and test data sets. The function should return the fitted vectorizer, the number of extracted features, and the processed train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1492b0bfab58773c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(X_train, X_test):\n",
    "    \"\"\"Converts the provided text training and test data into \n",
    "    feature counts. Additionally, returns the used vectorizer, \n",
    "    the processed data sets and the number of extracted features.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: pd.Series: training data\n",
    "        X_test: pd.Series: test data\n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: fitted count vectorizer \n",
    "        num_features: number of features used by the vectorizer, for a sanity check\n",
    "        X_train_vec: vectorized training features\n",
    "        X_test_vec: vectorized test features\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vectorizer, num_features, X_train_vec, X_test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-761d12c2e662383e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "assert isinstance(vectorizer, CountVectorizer), 'The vectorizer is not correct.'\n",
    "assert len(vectorizer.vocabulary_)==44103, 'The vectorizer is not fitted correctly.'\n",
    "assert np.sum(X_train_vec.todense())==1302972, 'The data is not vectorized correctly.'\n",
    "assert np.sum(X_test_vec.todense())==544923, 'The data is not vectorized correctly.'\n",
    "assert num_features == 44103, 'The number of features is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28fc4a869d434823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2 -  Baseline model\n",
    "\n",
    "You now want to use your newly found features to build a baseline. Create a function that receives the vectorized data sets from exercise 1.1 and trains a na√Øve Bayes model on it. Then use the test test to predict helpfulness and calculate the precision and recall of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4212f8a48fad26c7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test):\n",
    "    \"\"\"Returns a fitted Naive Bayes model, the predictions on the test set\n",
    "    and the precision and recall scores for these predictions\n",
    "    \n",
    "    Parameters:\n",
    "        X_train_vec (sparse matrix): Vectorized text data for training\n",
    "        y_train (pd.Series): Labels corresponding to X_train\n",
    "        X_test_vec (sparse matrix): Vectorized text data for testing\n",
    "        y_test (pd.Series): Labels corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        clf (MultinomialNB): MultinomialNB classifier fitted to the vectorized training data\n",
    "        y_pred (Series): The predictions computed with the classifier\n",
    "        precision (float): The precision score on the test data\n",
    "        recall (float): The recall score on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return clf, y_pred, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9987bb9aaf33cdf7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_pred, precision, recall = train_model_naive_bayes(X_train_vec, y_train, X_test_vec, y_test)\n",
    "assert isinstance(clf, MultinomialNB), 'Did you use the correct classifier?'\n",
    "assert hashlib.sha256(json.dumps(''.join([str(i) for i in y_pred])).encode()).hexdigest() == \\\n",
    "'073bf975b145e7a122189fbc70fff1598c85af025ea3a8553b93af03dfa2cb1a', 'The prediction is not correct.'\n",
    "np.testing.assert_almost_equal(precision, 0.627, decimal=3, err_msg=\"The precision is not correct.\")\n",
    "np.testing.assert_almost_equal(recall, 0.704, decimal=3, err_msg=\"The recall is not correct.\")\n",
    "print('This is your baseline:')\n",
    "print(f\"Number of features: {num_features}\")\n",
    "print(f\"Baseline precision: {precision}\")\n",
    "print(f\"Baseline recall: {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-336c8c45674c5528",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the rest of the notebook, we will used a preprocessed dataset without stopwords, so let's get a baseline for that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a25cab6bcd67318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_pre = pd.read_csv('data/book_review_helpfulness_preprocessed.csv')\n",
    "X_train_pre, X_test_pre, y_train_pre, y_test_pre = train_test_split(\n",
    "    df_pre['reviewText'], \n",
    "    df_pre['helpfulness'], \n",
    "    test_size=0.3, \n",
    "    random_state=42\n",
    ")\n",
    "_, num_features_pre, X_train_vec_pre, X_test_vec_pre = extract_features(X_train_pre, X_test_pre)\n",
    "_, _, precision_pre, recall_pre = train_model_naive_bayes(X_train_vec_pre, y_train_pre, X_test_vec_pre, y_test_pre)\n",
    "print('This is your baseline:')\n",
    "print(f\"Number of features: {num_features_pre}\")\n",
    "print(f\"Baseline precision: {precision_pre}\")\n",
    "print(f\"Baseline recall: {recall_pre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a27675ff9718a6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 2 - Feature analysis and selection\n",
    "\n",
    "The baseline results are not bad to start with, but as you've learned about feature selection you want to try it out. After all, 43 thousand features is still a pretty large number. You start with model based feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c58f1a74e4aeb4c3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.1 - TFIDF feature selection\n",
    "\n",
    "Implement the function below that fits a TF-IDF vectorizer to the train data by using only the top n features ordered by term frequency. The function should return the fitted vectorizer and the used features sorted by inverse document frequency in descending order.\n",
    "\n",
    "**Note**: In case of a tie, that is, if several features have the same IDF score, the features should be sorted alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d22a3960382cfe73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf_ngrams_sorted_by_idf(X_train, top_features=30):\n",
    "    \"\"\"Fits a TfidfVectorizer and returns its features sorted by \n",
    "    idf score in descending order\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Vectorized text data for training\n",
    "        top_features: maximum number of features to use \n",
    "    \n",
    "    Returns:\n",
    "        vectorizer: fitted tf-idf vectorizer \n",
    "        ngrams_sorted (list): The features used by the vectorizer sorted in descending order\n",
    "                              by their idf score. In case of a tie, the features should be sorted \n",
    "                              alphabetically.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return vectorizer, ngrams_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8188311a1f32f69",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, sorted_features = get_tfidf_ngrams_sorted_by_idf(X_train_pre, top_features=100)\n",
    "assert isinstance(vectorizer, TfidfVectorizer), 'Did you use the correct vectorizer?'\n",
    "assert len(vectorizer.get_feature_names_out())==100, 'The number of features used in the vectorizer is not correct.'\n",
    "assert hashlib.sha256(json.dumps(''.join(sorted_features)).encode()).hexdigest() == \\\n",
    "'ea46d68bceee1add30133130d5cd2016ffc6af8825eca5f5eeb6f2b9c5032ed7', 'The order of the features is not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adca65f62fff79d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the features with top inverse document frequency in your vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd01e244d60c3db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('30 most important features:')\n",
    "print(sorted_features[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5e6234e9cbfdcc8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we're seeing the top IDF features, they tend to be more specific words that can be found only in a subset of the documents. But in general, all of these features are not surprising in the context of books. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-904dba53cec8991a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.2 - Compare models with different numbers of features\n",
    "\n",
    "Now we want to see how this vectorization behaves in comparison with our baseline. Use the function from exercise 2.1 to create vectorizations of the `X_train_pre` data with different numbers of features (50, 100, 500, 1000, 2000, 5000 and 10000). Then train a Naive Bayes model on each vectorization and calculate precision, recall, and f1-score for the test data. Store the score values for all vectorizations in a list in the order specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-641b26a61917cf52",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# precision_values = ...\n",
    "# recall_values = ...\n",
    "# f1_values = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d190a2151e63bd0f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(precision_values) == len(recall_values) == len(f1_values) == 7, 'Did you use all the proposed numbers of features?'\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(precision_values), 0.595, decimal=3, err_msg=\"The precision values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(precision_values), 0.573, decimal=3, err_msg=\"The precision values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(precision_values), 0.61, decimal=2, err_msg=\"The precision values are not correct.\")\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(recall_values), 0.75, decimal=2, err_msg=\"The recall values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(recall_values), 0.707, decimal=3, err_msg=\"The recall values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(recall_values), 0.799, decimal=3, err_msg=\"The recall values are not correct.\")\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(f1_values), 0.66, decimal=2, err_msg=\"The f1 values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(f1_values), 0.656, decimal=3, err_msg=\"The f1 values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(f1_values), 0.67, decimal=2, err_msg=\"The f1 values are not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fda55a7260224715",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.3 - Precision and recall trade-off\n",
    "Look at the precision and recall variation with the number of features calculated in exercise 2.2 and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fec6510b6d3c58f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], precision_values, label='precision')\n",
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], recall_values, label='recall')\n",
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], f1_values, label='f1')\n",
    "plt.xlabel('Number of TFIDF features')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-44d6983aaea90cc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for n_features, precision, recall, f1 in zip([50, 100, 500, 1000, 2000, 5000, 10000], precision_values, recall_values, f1_values):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision:1.4} | Recall: {recall:1.4} | f1: {f1:1.4}\")    \n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1192b685dad4fab0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "1. Which number of features yielded the **highest precision**?\n",
    "2. Which number of features yielded the **highest recall**?\n",
    "3. Which number of features yielded the **highest F1-score**?\n",
    "4. Knowing that you won't be able to read that many reviews, but you want to make sure that the ones you do read are helpful, which model would you choose - the one with the highest precision, recall, or f1-score?\n",
    "\n",
    "Assign the answers to the variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cb3a3de21e03e7f6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# num_features_highest_precision = ... \n",
    "# num_features_highest_recall = ... \n",
    "# num_features_highest_f1 = ...\n",
    "# choice_model = ...  # answer with 'precision', 'recall', 'f1-score'\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f5fb9e31f8c9a160",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(json.dumps(str(num_features_highest_precision)+str(num_features_highest_recall)+str(num_features_highest_f1)+\n",
    "choice_model).encode()).hexdigest() == '6c2c7494b8968a7b1fed4e02f614ec011b3f2a23efd757ddaea7427a50e3ff4c', 'Not correct, try again.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28f6e998fa1a40a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.4 - Statistical feature selection\n",
    "\n",
    "Now you decide to use statistical analysis to select the most relevant features.\n",
    "\n",
    "<img src=\"media/chi-squared-not-sure.jpg\" width=\"300\" />\n",
    "\n",
    "Implement the function below to run chi-squared feature selection on the data. The function should take the provided data and vectorizer, vectorize the data, then run feature selection based on chi-squared score. It should return the fitted vectorizer, the fitted feature selector, the transformed training data, and the selected features sorted by chi-squared in descending order.\n",
    "\n",
    "**Note**: In this case, don't limit the feature number of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0c66b2edf797342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_chi2(X_train, y_train, vectorizer, top_features=100):\n",
    "    \"\"\"Vectorizes the provided training data and selects \n",
    "    the best features with the chi-squared method.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.Series): training data\n",
    "        y_train (pd.Series): training labels\n",
    "        vectorizer: vectorizer to use (CountVectorizer or TfidfVectorizer)\n",
    "        top_features: number of best features to select with the chi-squared test\n",
    "\n",
    "    Returns:\n",
    "        vectorizer: fitted vectorizer \n",
    "        ch2: fitted feature selector\n",
    "        X_train_ch2: transformed data after feature selection\n",
    "        features_ch2 (list): the top_features features of the fitted vectorizer sorted in descending order\n",
    "                              by their chi-squared score\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return vectorizer, ch2, X_train_ch2, features_ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0b7f52fc1fdef287",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, ch2, X_train_ch2, ch2_features = extract_features_chi2(X_train_pre, y_train_pre, vectorizer=TfidfVectorizer())\n",
    "assert ch2.k == 100, 'The number of selected features is not correct.'\n",
    "assert X_train_ch2.shape==(6998, 100), 'The shape of the transformed data is not correct.'\n",
    "np.testing.assert_almost_equal(np.sum(X_train_ch2), 1580.2, decimal=1, err_msg=\"The transformed data is not correct.\")\n",
    "assert len(ch2_features) == 100, 'The number of selected features is not correct.'\n",
    "assert hashlib.sha256(json.dumps(''.join(ch2_features)).encode()).hexdigest() == \\\n",
    "'0b08cff867665cb2ffba296f2bb3046038b0c7a4fc1dc063b8c2e9f079d92796', 'The selected features are not correct.'\n",
    "\n",
    "vectorizer, ch2, X_train_ch2, ch2_features  = extract_features_chi2(X_train_pre, y_train_pre, \n",
    "                                                                    vectorizer=CountVectorizer(), top_features=123)\n",
    "assert ch2.k == 123, 'The number of selected features is not correct.'\n",
    "assert X_train_ch2.shape==(6998, 123), 'The shape of the transformed data is not correct.'\n",
    "assert len(ch2_features) == 123, 'The number of selected features is not correct.'\n",
    "np.testing.assert_almost_equal(np.sum(X_train_ch2), 148539, decimal=1, err_msg=\"The transformed data is not correct.\")\n",
    "assert hashlib.sha256(json.dumps(''.join(ch2_features)).encode()).hexdigest() == \\\n",
    "'5daf1e3e2ca7739b64e1d6e9f2ee1423641acaf5e9dcf2b64b34f8b2d258a0d8', 'The selected features are not correct.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b6c9135c1144ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's check out the 15 most important features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de4f48b0315a5c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "ch2_features[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-099ecaf666f3e62e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also see how often the selected features appear in the helpful and unhelpful reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75fa0016fc29f7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for feature in ch2_features[:15]:\n",
    "    print('Documents containing the word \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train_pre.str.lower().str.contains(feature)\n",
    "    print(f'helpful | not helpful')\n",
    "    print(f'{y_train_pre[docs].value_counts().iloc[0]:7} | {y_train_pre[docs].value_counts().iloc[1]} \\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1563c9a666e59d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Most of the selected features are more frequent in the \"helpful\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-51c21136363e2ced",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 2.5 - Precision and recall trade-off again\n",
    "Now let's see how the feature selection with chi-squared translates to model training. Use the function from exercise 2.4 to create vectorizations of the `X_train_pre` data with different numbers of features (50, 100, 500, 1000, 2000, 5000 and 10000). Then train a Naive Bayes model on each vectorization and calculate precision, recall, and f1-score on the test data. Store the score values for all vectorizations in a list in the order specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38b468714a5ca798",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# precision_values_ch2 = ...\n",
    "# recall_values_ch2 = ...\n",
    "# f1_values_ch2 = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0ad99d25ab050a2f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(precision_values_ch2) == len(recall_values_ch2) == len(f1_values_ch2) == 7, 'Did you use all the proposed numbers of features?'\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(precision_values_ch2), 0.630, decimal=3, err_msg=\"The precision values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(precision_values_ch2), 0.587, decimal=3, err_msg=\"The precision values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(precision_values_ch2), 0.661, decimal=3, err_msg=\"The precision values are not correct.\")\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(recall_values_ch2), 0.507, decimal=3, err_msg=\"The recall values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(recall_values_ch2), 0.485, decimal=3, err_msg=\"The recall values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(recall_values_ch2), 0.546, decimal=3, err_msg=\"The recall values are not correct.\")\n",
    "\n",
    "np.testing.assert_almost_equal(np.mean(f1_values_ch2), 0.561, decimal=3, err_msg=\"The f1 values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.min(f1_values_ch2), 0.550, decimal=3, err_msg=\"The f1 values are not correct.\")\n",
    "np.testing.assert_almost_equal(np.max(f1_values_ch2), 0.584, decimal=3, err_msg=\"The f1 values are not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-760cb8df07ac60fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, check the precision and recall scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e431f449562fd072",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], precision_values_ch2, label='precision')\n",
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], recall_values_ch2, label='recall')\n",
    "plt.plot([50, 100, 500, 1000, 2000, 5000, 10000], f1_values_ch2, label='f1')\n",
    "plt.xlabel('Number of TFIDF features')\n",
    "plt.title('Feature selection with chi-squared')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-08024ed59e658e64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for n_features, precision, recall, f1 in zip([50, 100, 500, 1000, 2000, 5000, 10000], precision_values_ch2, \n",
    "                                             recall_values_ch2, f1_values_ch2):\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Precision: {precision:1.4} | Recall: {recall:1.4} | f1: {f1:1.4}\")    \n",
    "    print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1334fe8f60c2d3fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this case, we were able to raise precision even though we sacrificed recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8be788c4e7462464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 - PCA\n",
    "\n",
    "Now let's move on to more complex feature selection methods. In the previous methods, we are limited to the information each feature brings in separately. We will now try PCA, which will distill the most information from all features into the principal components.\n",
    "\n",
    "Write a function that computes principal components from features extracted with a `CountVectorizer` from the provided data set. The function should then calculate the total variance explained by the principal components (use an attribute of sklearn PCA). Finally, it should train a support vector classifier on the calculated principal components and calculate the prediction on the test set.\n",
    "\n",
    "**To avoid using too much memory, and as we'll use dense matrices, please use max_features value=5000 in the vectorizer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Vectorizes the data with a CountVectorizer, computes principal components,\n",
    "    and fits a support vector classifier to the principal components.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (pd.Series): Text data for training\n",
    "        y_train (pd.Series): Labels corresponding to X_train\n",
    "        X_test (pd.Series): Text data for testing\n",
    "        y_test (pd.Series): Labels corresponding to X_test\n",
    "        num_features (int): number of principal components to calculate\n",
    "        seed (int): Seed for random state in PCA\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (CountVectorizer): fitted CountVectorizer\n",
    "        pca (PCA): fitted PCA with the given number of components\n",
    "        clf (SVC): SVC classifier fitted to the principal components\n",
    "        y_pred (Series): predictions from the SVC classifier\n",
    "        explained_variance(float): variance explained by the principal components\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vectorizer, pca, clf, y_pred, explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e0c931cb07614c23",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, pca, clf, y_pred_30, variance_30 = train_model_pca_svm(X_train_pre, y_train_pre, X_test_pre, y_test_pre,\n",
    "                                                                   num_features=30, seed=42)\n",
    "print(\"----- Computed PCA for 30 features -----\\n\")\n",
    "assert isinstance(vectorizer, CountVectorizer), 'Did you use the correct vectorizer?'\n",
    "assert len(vectorizer.vocabulary_)==5000, 'Did you use the correct number of features in the vectorizer?'\n",
    "assert isinstance(pca,PCA), 'Did you use the correct PCA?'\n",
    "assert len(pca.components_)==30, 'Did you use the correct number of principal components?'\n",
    "assert pca.get_params()['random_state']==42, 'Did you use the random state seed?'\n",
    "np.testing.assert_almost_equal(np.mean(y_pred_30), 0.54, decimal=2, err_msg=\"The prediction for 100 pc is not correct.\")\n",
    "np.testing.assert_almost_equal(variance_30, 33.89, decimal=2, err_msg=\"The explained variance for 30 pc is not correct.\")\n",
    "\n",
    "vectorizer, pca, clf, y_pred_50, variance_50 = train_model_pca_svm(X_train_pre, y_train_pre, X_test_pre, y_test_pre,\n",
    "                                                                   num_features=50, seed=42)\n",
    "print(\"----- Computed PCA for 50 features -----\\n\")\n",
    "assert len(pca.components_)==50, 'Did you use the correct number of principal components?'\n",
    "assert pca.get_params()['random_state']==42, 'Did you use the random state seed?'\n",
    "np.testing.assert_almost_equal(np.mean(y_pred_50), 0.54, decimal=2, err_msg=\"The prediction for 100 pc is not correct.\")\n",
    "np.testing.assert_almost_equal(variance_50, 41.37, decimal=2, err_msg=\"The explained variance for 50 pc is not correct.\")\n",
    "\n",
    "vectorizer, pca, clf, y_pred_100, variance_100 = train_model_pca_svm(X_train_pre, y_train_pre, X_test_pre, y_test_pre,\n",
    "                                                                     num_features=100, seed=42)\n",
    "print(\"----- Computed PCA for 100 features -----\\n,\")\n",
    "assert len(pca.components_)==100, 'Did you use the correct number of principal components?'\n",
    "assert pca.get_params()['random_state']==42, 'Did you use the random state seed?'\n",
    "np.testing.assert_almost_equal(np.mean(y_pred_100), 0.54, decimal=2, err_msg=\"The prediction for 100 pc is not correct.\")\n",
    "np.testing.assert_almost_equal(variance_100, 54.0, decimal=1, err_msg=\"The explained variance for 100 pc is not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1ecb5e0f82ec689",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can compare the new precision and recall with the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4362d8b3356bd257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nPredictions with 30 features: \")\n",
    "print(\"Precision: {}\".format(precision_score(y_test_pre, y_pred_30)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test_pre, y_pred_30)))\n",
    "print(\"Explained variance: {}\".format(variance_30))\n",
    "   \n",
    "print(\"\\nPredictions with 50 features: \")\n",
    "print(\"Precision: {}\".format(precision_score(y_test_pre, y_pred_50)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test_pre, y_pred_50)))\n",
    "print(\"Explained variance: {}\".format(variance_50))\n",
    "\n",
    "print(\"\\nPredictions with 100 features: \")\n",
    "print(\"Precision: {}\".format(precision_score(y_test_pre, y_pred_100)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test_pre, y_pred_100)))\n",
    "print(\"Explained variance: {}\".format(variance_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98437874ea1e4117",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Nice! We kept the high precision of the chi-squared feature selection method but with a much higher recall. Notice also that the explained variance only reached about 50% with 100 features and keeps growing as we're adding features, so there is room for using more principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 4 - Word vectors\n",
    "\n",
    "Now we'll change gears and look into word vectors. In the learning notebook 3 we mentioned that word vectors can be visualized after being projected into 2D space as in this diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8a864c8e6d4eb95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-424738c218d8d27a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization. Let's load the spacy word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3847eb100057a7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b982ddaa87eec51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.1 - Get a word vector\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"book\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# book_vector = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert book_vector.shape[0] == 300, 'The size of the vector is not correct.'\n",
    "np.testing.assert_almost_equal(book_vector.sum(), -16.292, decimal=3, err_msg=\"The 'book' vector not correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7566702b1fd9755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.2 - Reduce word vector dimensions\n",
    "\n",
    "Next, write a function that uses sklearn's PCA to reduce word vectors to a convenient number of dimensions for plotting. The function takes an array of word vectors and a seed for the random state and should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors, random_state):\n",
    "    \"\"\"\n",
    "    Returns PCA-reduced word vectors of the input vectors for plotting\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "        random_state (int): random state to use in PCA\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors, random_state=42)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2), 'Did you choose the correct number of dimensions for the vectors?'\n",
    "np.testing.assert_almost_equal(abs(reduced_vecs).sum(), 1.362, decimal=3, err_msg=\"The reduced word vectors are correct.\")\n",
    "np.testing.assert_almost_equal(reduced_vecs.min(), -0.376, decimal=3, err_msg=\"The reduced word vectors are correct.\")\n",
    "np.testing.assert_almost_equal(reduced_vecs.max(), 0.433, decimal=3, err_msg=\"The reduced word vectors are correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f75d7362f3b5b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll create an array of ~100,000 Spacy word vectors and use your function to reduce them for plotting and plot some of them. If you're curious about using the full amount of word vectors, you can change the code to iterate over all vocab words - `list(nlp.vocab.strings)` - instead of our own `vocab_strings`, but beware that it will use a lot of memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-644c6807023bc77f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open('data/word_subset.txt') as fwords:\n",
    "    vocab_strings = fwords.read().splitlines()\n",
    "\n",
    "full_vocab_vecs = []\n",
    "for tok in vocab_strings:\n",
    "    full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ca3da9928dd8b4cf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array, random_state=42)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Time to plot! We're going to plot the words in the list below. Here we find the reduced word vectors for these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9415c85fbda3dbb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing', \n",
    "                 'playstation', 'reading', 'studying', 'sony', 'nintendo', 'sad', 'angry', 'bored']\n",
    "\n",
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9190df0ea45c5bcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "And we plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c059bc4a449b27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(5, 5), facecolor='w', edgecolor='k')\n",
    "min_x=min([x for x in coords_array[:,0]])\n",
    "max_x=max([x for x in coords_array[:,0]])\n",
    "min_y=min([y for y in coords_array[:,1]])\n",
    "max_y=max([y for y in coords_array[:,1]])\n",
    "plt.xlim(min_x-0.1,max_x+0.1)\n",
    "plt.ylim(min_y-0.1,max_y+0.1)\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=10 );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA has worked! In the diagram, we can see similar types of words closer together. Some words are even exactly the same when reduced to two dimensions. Note that the closeness is measured by the angle between the vectors (from the origin to the blue endpoints). But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 4.3 - Word similarity\n",
    "\n",
    "As a final exercise, we'll look at word similarities.\n",
    "\n",
    "Write a function that returns the closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them. All the words are from the `words_to_plot` list defined in the previous exercise and the vectors are the full word vectors from `full_vocab_vecs`.\n",
    "\n",
    "You can use the already imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-92cb8485d40cab4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Here we retrieve the full word vectors for the words in the words_to_plot list.\n",
    "words_to_plot_full_vectors = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    words_to_plot_full_vectors.append(full_vocab_vecs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_list, words_in_list_vectors):\n",
    "    '''Returns a list of the closest words to the input_word from the words_in_vocab list,\n",
    "       based on cosine similarity of the word vectors\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word \n",
    "                             (the input_word is also from the words_in_list list)\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_in_list_vectors (list): full word vectors associated with the words_in_list words\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest words to the\n",
    "                                   input_word, based on cosine similarity between the word vectors\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return closest_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "closest_nintendo = closest_word('nintendo', words_to_plot, words_to_plot_full_vectors)\n",
    "assert hashlib.sha256(json.dumps(''.join(closest_nintendo)).encode()).hexdigest() == \\\n",
    "'83ac2af7b1bda97cc06724c800388e676e584d8e3f9454d8b98b5746470f52b0', 'Close, but not enough, try again.'\n",
    "closest_playing = closest_word('playing', words_to_plot, words_to_plot_full_vectors)\n",
    "assert hashlib.sha256(json.dumps(''.join(closest_playing)).encode()).hexdigest() == \\\n",
    "'9f70f404dde083bd6fcad162324acbfb6cc2d84d18c5c1d08c03118452702132', 'Close, but not enough, try again.'\n",
    "closest_pineapple = closest_word('pineapple', words_to_plot, words_to_plot_full_vectors)\n",
    "assert hashlib.sha256(json.dumps(''.join(closest_pineapple)).encode()).hexdigest() == \\\n",
    "'73259744b0063b29a27495f8e89c05419c593bdbeff9969850022c5c155fb18f', 'Close, but not enough, try again.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a90269e12a5a58f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see what you found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-25a99d80250e2853",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\nClosest words to nintendo:\")\n",
    "print(closest_nintendo)\n",
    "\n",
    "print(\"\\nClosest words to playing:\")\n",
    "print(closest_playing)\n",
    "\n",
    "print(\"\\nClosest words to pineapple:\")\n",
    "print(closest_pineapple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fdffae6311cd8b15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "That is all, congratulations and see you in the next BLU!\n",
    "\n",
    "PS: feel free to share your book reviews and recommendations. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-321d84f138e987ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src=\"media/dont-buy-more-books.jpg\" width=\"400\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
