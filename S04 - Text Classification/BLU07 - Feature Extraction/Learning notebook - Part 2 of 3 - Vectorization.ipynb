{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU07 - Part 2 of 3 - From words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "from numpy import inf\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have noticed, text data is a bit different from other datasets you have seen -- it's just a bunch of words strung together! Where are the features in a tightly organized table? Unlike other data you have worked with in previous BLUs, text is unstructured and thus needs some additional work on our end to make it structured and ready to be handled by a machine learning algorithm.\n",
    "\n",
    "Language can be messy, but one thing is clear: we need features. The process of getting features out of a text is called **vectorization**. Remember those nice tidy datasets where each sample was a row in a table and each feature was a column? Well, in those data, each sample was a vector in the feature space. Feature space is an n-dimensional space where each axis represents the values of a feature. Every sample from the dataset is a point in that space whose coordinates are that sample's feature values.\n",
    "\n",
    "In our situation, each sample is a text document. Vectorization means that we're going to define the features and map each sample as a point (or vector) in that feature space. The simplest way to define the features is to take all the words in the documents as features. In that case, the feature space is the **vocabulary** of all the words in our documents and the vector of each document is represented by the count of each word in the given document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/vectors.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But enough talk - let's get our hands dirty!\n",
    "\n",
    "We're going to work with movie reviews from IMDB and see a few ways of data vectorization. Let's load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Aldolpho (Steve Buscemi), an aspiring film mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Negative</td>\n",
       "      <td>An unfunny, unworthy picture which is an undes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Negative</td>\n",
       "      <td>A failure. The movie was just not good. It has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Positive</td>\n",
       "      <td>I saw this movie Sunday afternoon. I absolutel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Negative</td>\n",
       "      <td>Disney goes to the well one too many times as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                               text\n",
       "0  Negative  Aldolpho (Steve Buscemi), an aspiring film mak...\n",
       "1  Negative  An unfunny, unworthy picture which is an undes...\n",
       "2  Negative  A failure. The movie was just not good. It has...\n",
       "3  Positive  I saw this movie Sunday afternoon. I absolutel...\n",
       "4  Negative  Disney goes to the well one too many times as ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are two columns in this dataset - one for the labels and another for the text of the movie reviews. Each sample is labeled as a positive or negative review. Our goal is to retrieve meaningful features from the text so that in the next notebook we can use a machine learning model to predict if a given review is positive or negative.\n",
    "\n",
    "Let's see a positive and a negative example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n",
      "\"The Lion King\" is without a doubt my favorite Disney movie of all time, so I figured maybe I should give the sequels a chance and I did. Lion King 1 1/2 was pretty good and had it's good laughs and fun with Timon and Pumba. Only problem, I feel sometimes no explanations are needed because they can create plot holes and just the feeling of wanting your own explanation. Well, I would highly recommend this movie for lion King fans or just a night with the family. It's a fun flick with the same laughs and lovable characters as the first. So, hopefully, I'll get the same with the third installment to the Lion King series. Sit back and just think Hakuna Matata! It means no worries! <br /><br />8/10\n"
     ]
    }
   ],
   "source": [
    "pos_example = df.text[4835]\n",
    "print(df.sentiment[4835])\n",
    "print(pos_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! So that is a review about *The Lion King 1 1/2* (a.k.a. *The Lion King 3* in some countries). It seems the reviewer liked it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n",
      "Disney goes to the well one too many times as anybody who has seen the original LITTLE MERMAID will feel blatantly ripped off. Celebrating the birth of their daughter Melody, Ariel and Eric plan on introducing her to King Triton. The celebration is quickly crashed by Ursula 's sister, Morgana who plans to use Melody as a defense tool to get the King 's trident. Stopping the attack, Ariel and Eric build a wall around the ocean while Melody grows up wondering why she cannot go in there.<br /><br />Awful and terrible is what describes this direct to video sequel. LITTLE MERMAID 2 gives you that feeling everything you watch seemed to have come straight other Disney movies. I guess Disney can only plagiarize itself! Do not tell me that the penguin and walrus does not remind you of another duo from the LION KING!<br /><br />Other disappointing moments include the rematch between Sebastien and Louie, the royal chef. They terribly under played it! The climax between Morgana and EVERYONE seemed to be another disappointment.<br /><br />I will not give anything away, but in 75 minutes, everything seemed incredibly cramped and too much to handle. An embarrassment to Disney, LITTLE MERMAID 2 is better left to rent and laugh at. Then you can prepare for the rest of the other sequels Disney is going to drown you in later on.\n"
     ]
    }
   ],
   "source": [
    "neg_example = df.text[4]\n",
    "print(df.sentiment[4])\n",
    "print(neg_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. I guess that's a pass for this one, right?\n",
    "\n",
    "Let's get the first 200 documents of this dataset to run experiments faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df.text[:200]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "As we've learned in the previous learning notebook, we should tokenize and stem our text before feature extraction. Let's initialize our favorite tokenizer and stemmer and put the workflow we learned into practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are keeping the stopwords for now, but do not stem them. Using `ignore_stopwords=True` prevents the stemming of stopwords.\n",
    "\n",
    "As usual, the data is not clean, so we'll have to deal with that before tokenization and stemming. We can see from the examples above that our _corpus_ (just a fancy way of saying a document collection for language analysis) has some HTML substrings `<br />` that are only adding meaningless noise. We can remove these HTML tags with `re.sub()` by substituting every substring that matches the regex `<[^>]*>` with an empty string.\n",
    "\n",
    "We will define a `preprocess()` method that removes these unnecessary HTML tags, tokenizes, and stems our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    # remove html tags\n",
    "    doc = re.sub(\"<[^>]*>\", \"\", doc)\n",
    "    # lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(doc)\n",
    "    # remove punctuation\n",
    "    # string.punctuation is a utility that allows us to not have to define all punctuation characters by ourselves\n",
    "    words = [word for word in words if word not in string.punctuation]\n",
    "    # stem\n",
    "    stems = [stemmer.stem(word) for word in words]\n",
    "    new_doc = \" \".join(stems)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've used `string.punctuation` instead of defining a list of punctuation characters ourselves. This is a handy constant provided by the `string` package. These are the characters in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't cover everything, like the weird quotation marks from the previous unit, but it's a good base. The simple list comprehension we used to filter the tokens in the function above wouldn't work for the following situation though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'No', 'it', 'isn', 't', '...']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "print([word for word in tokenizer.tokenize(text) if word not in string.punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `...` wasn't detected as punctuation with this procedure. It's better to use the punctuation list with a regex in this case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is', 'this', 'a', 'test', 'No', 'it', 'isn', 't']\n"
     ]
    }
   ],
   "source": [
    "text = \"Is this a test? No, it isn't ...\"\n",
    "\n",
    "pattern = re.compile(\"[\" + re.escape(string.punctuation) + \"]\")\n",
    "\n",
    "sentence = \" \".join(tokenizer.tokenize(text))\n",
    "\n",
    "print(re.sub(pattern, '', sentence).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break it down:\n",
    "\n",
    "- First, punctuation is transformed into a regex pattern.\n",
    "- Then, the text is tokenized and the tokens are saved in a string, separated by spaces.\n",
    "- `re.sub()` is applied to the string which removes all characters that are in the regex pattern. Since these characters include the `.` the last three dots are removed.\n",
    "- Finally, the string is split again on spaces to get back the tokens.\n",
    "\n",
    "But back to our dataset. Let's apply the preprocessing function to all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_preprocessed = docs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Little mermaid review after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disney goe to the well one too mani time as anybodi who has seen the origin littl mermaid will feel blatant rip off celebr the birth of their daughter melodi ariel and eric plan on introduc her to king triton the celebr is quick crash by ursula s sister morgana who plan to use melodi as a defens tool to get the king s trident stop the attack ariel and eric build a wall around the ocean while melodi grow up wonder why she cannot go in there aw and terribl is what describ this direct to video sequel littl mermaid 2 give you that feel everyth you watch seem to have come straight other disney movi i guess disney can only plagiar itself do not tell me that the penguin and walrus does not remind you of anoth duo from the lion king other disappoint moment includ the rematch between sebastien and louie the royal chef they terribl under play it the climax between morgana and everyon seem to be anoth disappoint i will not give anyth away but in 75 minut everyth seem incred cramp and too much to handl an embarrass to disney littl mermaid 2 is better left to rent and laugh at then you can prepar for the rest of the other sequel disney is go to drown you in later on'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_preprocessed[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you may not understand it very well now, but we actually made the text much more readable for a machine.\n",
    "\n",
    "You can notice that many of the most common words in the reviews are what we consider stopwords: determiners like \"the,\" \"a\"; prepositions like \"of,\" \"to\"; etc. We will probably want to filter these out later on, but now let's get to the vectorization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of words\n",
    "\n",
    "As we said, the simplest way to vectorize a document collection is by counting the words. This technique is called a **Bag of Words (BoW)**. Look at the following image to see how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/bag_of_words.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we throw all the words in all documents in a bag, those will be our features. Then for each document, we take the words out of the bag one by one and count how many times the given word appears in that document. What we get is a **document vector**. \n",
    "\n",
    "We store the document vectors in a table where the column names (feature names) are the words. By doing this, our data becomes structured and tabular and each column represents the number of times a word of the vocabulary appeared in every document, whereas each row corresponds to a document.\n",
    "\n",
    "Note that this type of vectorization loses all the syntactic information. That is, you could shuffle the words in each document and still get the same vector (that's why it's called a bag of words). This is enough for some applications, but it might not be enough for others and a different representation of text that conserves the word order might be needed.\n",
    "\n",
    "In the following sections, we're going to manually create the BoW representation of our documents before using 'official' sklearn vectorizers in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vocabulary\n",
    "\n",
    "So now we have each document preprocessed into tokens. To transform this data into a vectorized Bag of Words, we first need to define our feature space.\n",
    "\n",
    "As mentioned before, the feature space will be the vocabulary of our data, so the set of unique tokens in our documents.\n",
    "\n",
    "To create our vocabulary, we will use a [Counter](https://docs.python.org/3/library/collections.html#collections.Counter). `Counter()` is a dictionary that counts the number of occurrences of different items in a list. We're going to feed it with our preprocessed documents to count the occurences of the tokens.\n",
    "\n",
    "We then sort our dictionary by counts using `Counter()`'s built-in method `.most_common()` and store everything in an `OrderedDict()`. Like this, the features in the document vectors will be ordered from the most to the least common words in the corpus (this is not required, but makes data visualization much nicer!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(docs):\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(docs_preprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sneak peak at our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5740"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not too large. What are the most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2706),\n",
       " ('a', 1361),\n",
       " ('and', 1349),\n",
       " ('of', 1205),\n",
       " ('to', 1115),\n",
       " ('is', 815),\n",
       " ('it', 786),\n",
       " ('in', 719),\n",
       " ('i', 690),\n",
       " ('this', 594),\n",
       " ('that', 581),\n",
       " ('s', 541),\n",
       " ('movi', 459),\n",
       " ('film', 400),\n",
       " ('as', 377),\n",
       " ('but', 358),\n",
       " ('with', 357),\n",
       " ('for', 315),\n",
       " ('was', 305),\n",
       " ('t', 295)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it also remind you of the list of stopwords? We'll deal with this in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Vectorization\n",
    "\n",
    "Now that we have our vocabulary, we can create the document vectors. We simply count how many times each term from the vocabulary appears in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(docs, vocab):\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        vector = np.array([doc.count(word) for word in vocab])\n",
    "        vectors.append(vector)\n",
    "\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a list of vectors which is not so pretty to look at. We can visualize it better with a pandas dataframe where the column names are the feature names (the words in the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>a</th>\n",
       "      <th>and</th>\n",
       "      <th>of</th>\n",
       "      <th>to</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>in</th>\n",
       "      <th>i</th>\n",
       "      <th>this</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5740 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   the   a  and  of  to  is  it  in   i  this  ...  championship  ...\"  \\\n",
       "0   11  94    7   4   7  16   5  11  76     3  ...             0     0   \n",
       "1    0   6    0   0   1   3   2   0  11     1  ...             0     0   \n",
       "2    8  48    6   3   1   6   3   5  35     1  ...             0     0   \n",
       "3    8  36    3   3   3   5   6   4  45     2  ...             0     0   \n",
       "4   21  70    9   4  16  13   6  15  73     1  ...             0     0   \n",
       "\n",
       "   endear  cortney  incid  erupt  semblanc  miser  shoe  mail  \n",
       "0       0        0      0      0         0      0     0     0  \n",
       "1       0        0      0      0         0      0     0     0  \n",
       "2       0        0      0      0         0      0     0     0  \n",
       "3       0        0      0      0         0      0     0     0  \n",
       "4       0        0      0      0         0      0     0     0  \n",
       "\n",
       "[5 rows x 5740 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = pd.DataFrame(vectorize(docs_preprocessed, vocab), columns=vocab)\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tadaa, that's our bag of words! As you can see, each document is one row in this dataframe and is a vector of the size of the vocabulary (length=5740). Each feature in that vector corresponds to the number of times a word appears in this document.\n",
    "\n",
    "And yes, it's a humongous dataframe, 5740 features is way more than we've ever seen before. We're going to talk about this in the next BLU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Stopwords\n",
    "\n",
    "We're looking for the most meaningful features in our vocabulary so that we can classify each document as a positive or negative review. But, as you can see from the above dataframe, the text is filled with words with no meaning like \"the\" or \"and\". This contrasts with words like \"love\" or \"hate\" that have a very clear semantic meaning. Those meaningless words are **stopwords** - words that _usually_ don't introduce any meaning to a piece of text and are in the document just for syntactic reasons.\n",
    "\n",
    "It is important to emphasize that word \"usually\" in the previous sentence. Sometimes stopwords can be useful features, especially when we use more than just unigrams as features (ex.: bigrams, trigrams, ...), where word order and word combinations start to be relevant.\n",
    "\n",
    "Let's update our `build_vocabulary()` function and remove the stopwords from the text. This way we will reduce our vocabulary - and thus our feature space - making the data representation more lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary_without_stopwords(docs, stop_eng):\n",
    "    vocabulary = Counter()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split(' ') if word not in stop_eng]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "vocab_without_stopwords = build_vocabulary_without_stopwords(docs_preprocessed, stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the size of the new vocabulary (it should be smaller than before):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5595"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has more meaningfull words now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movi', 459),\n",
       " ('film', 400),\n",
       " ('one', 238),\n",
       " ('like', 204),\n",
       " ('time', 124),\n",
       " ('get', 118),\n",
       " ('watch', 110),\n",
       " ('make', 106),\n",
       " ('even', 105),\n",
       " ('see', 105),\n",
       " ('good', 99),\n",
       " ('stori', 99),\n",
       " ('charact', 98),\n",
       " ('end', 97),\n",
       " ('scene', 96),\n",
       " ('would', 94),\n",
       " ('well', 92),\n",
       " ('much', 92),\n",
       " ('peopl', 92),\n",
       " ('love', 86)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn into a list of tuples and get the first 20 items\n",
    "list(vocab_without_stopwords.items())[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is our new BoW vectorization without stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   movi  film  one  like  time  get  watch  make  even  see  ...  \\\n",
       "0     3     5    0     0     2    1      1     3     0    0  ...   \n",
       "1     1     0    0     0     0    0      0     0     0    0  ...   \n",
       "2     6     0    2     1     0    0      0     0     1    0  ...   \n",
       "3     4     0    1     1     1    0      0     0     1    2  ...   \n",
       "4     1     0    1     0     1    1      1     0     0    4  ...   \n",
       "\n",
       "   championship  ...\"  endear  cortney  incid  erupt  semblanc  miser  shoe  \\\n",
       "0             0     0       0        0      0      0         0      0     0   \n",
       "1             0     0       0        0      0      0         0      0     0   \n",
       "2             0     0       0        0      0      0         0      0     0   \n",
       "3             0     0       0        0      0      0         0      0     0   \n",
       "4             0     0       0        0      0      0         0      0     0   \n",
       "\n",
       "   mail  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_no_stopwords = pd.DataFrame(vectorize(docs_preprocessed, vocab_without_stopwords), columns=vocab_without_stopwords)\n",
    "bow_no_stopwords.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Normalized BoW - term frequencies\n",
    "Another thing that we could do is to normalize our word counts. As you can see, different documents have different numbers of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    724\n",
       "1     97\n",
       "2    434\n",
       "3    417\n",
       "4    801\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_no_stopwords.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can introduce bias in our features, so we should normalize each document by its number of words. Instead of having word counts as features, we will have **term frequencies**. This way, the features in any document vector sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = bow_no_stopwords.div(bow_no_stopwords.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.006024</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.005089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         movi      film       one      like      time       get     watch  \\\n",
       "40   0.001506  0.006024  0.004518  0.000000  0.003012  0.000000  0.000000   \n",
       "8    0.005089  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "136  0.013158  0.000000  0.001645  0.000822  0.000000  0.000822  0.003289   \n",
       "\n",
       "     make      even       see  ...  championship  ...\"  endear  cortney  \\\n",
       "40    0.0  0.001506  0.003012  ...           0.0   0.0     0.0      0.0   \n",
       "8     0.0  0.007634  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "136   0.0  0.001645  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "\n",
       "     incid  erupt  semblanc  miser  shoe  mail  \n",
       "40     0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "8      0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "136    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[3 rows x 5595 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, our beautiful, normalized bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Weighting by document frequency\n",
    "\n",
    "Here is another way how we can improve our bag of words features. The idea is that very common words that appear in every document are less informative, while rare words are more interesting and can bring us further in the classification task at hand. This strategy was originally developed for document searching. In our example, words like 'film' or 'movie' appear in many documents, so they're not likely to help decide the category where the review belongs.\n",
    "\n",
    "The weighting term is called **inverse document frequency** - we are giving more weight to rarer words appearing in less documents. Document frequency simply means in how many documents the word appears. Let's take the first feature, the word 'movi' and see how often it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bow_no_stopwords['movi']>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears in 133 documents out of 200, so its document frequency is 133/200 which is 0.665 and the inverse of it is about 1.5. Now another word, 'great':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bow_no_stopwords['great']>0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears in 62 documents out of 200, so its document frequency is 62/200 = 0.31 and the inverse of it is about 3.22.\n",
    "\n",
    "So the values of the feature 'great' would be weighted by a larger weight than the values of the feature 'movi', giving it more importance in a classification model.\n",
    "\n",
    "That's the general idea. In reality, the inverse document frequency weighting factor is calculated in a slighly more complicated way using a logarithm:\n",
    "\n",
    "$$ idf(t) = log{(\\frac{n+1}{df_t+1})} + 1 $$\n",
    "\n",
    "where $t$ is the term (word) for which we are calculating the weight, $n$ is the number of documents, and $df_{t}$ is the number of documents where the term appears.\n",
    "\n",
    "This is the formula used by sklearn, there are other versions out there that differ in the use of the +1 factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final value of the feature for a given document, **tf-idf**, is calculated as the product of the term frequency and the inverse document frequency\n",
    "\n",
    "$$ tf\\text{-}idf(t,d) = tf(t,d) \\times idf(t) $$\n",
    "\n",
    "where $d$ is the document for which the term frequency is calculated. It is common to normalize the resulting sample vectors (rows) to a unit norm.\n",
    "\n",
    "Let's calculate tf-idf for our bag of words from above. First we calculate in how many documents each term appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "movi        133\n",
       "film        117\n",
       "one         147\n",
       "like        103\n",
       "time         87\n",
       "           ... \n",
       "erupt         1\n",
       "semblanc      1\n",
       "miser         1\n",
       "shoe          2\n",
       "mail          2\n",
       "Length: 5595, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(bow_no_stopwords > 0).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate tf-idf starting from the BoW without stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf(bow):\n",
    "    # Term frequency: divide word count by the number of words in the document\n",
    "    # We already did this in the previous section\n",
    "    tf = bow.div(bow.sum(axis=1), axis=0)\n",
    "\n",
    "    # Document frequency: number of documents containing the word\n",
    "    df = (bow > 0).sum(axis=0)\n",
    "\n",
    "    # n: number of documents\n",
    "    n = bow.shape[0]\n",
    "\n",
    "    return tf * (np.log((n+1) / (df+1)) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005824</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.00791</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.014489</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019430</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003132</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.007474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.002479</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movi      film       one      like      time       get     watch  \\\n",
       "0  0.005824  0.010584  0.000000  0.000000  0.005044  0.002461  0.002743   \n",
       "1  0.014489  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.019430  0.000000  0.006019  0.003822  0.000000  0.000000  0.000000   \n",
       "3  0.013482  0.000000  0.003132  0.003978  0.004379  0.000000  0.000000   \n",
       "4  0.001755  0.000000  0.001631  0.000000  0.002280  0.002224  0.002479   \n",
       "\n",
       "      make      even       see  ...  championship  ...\"  endear  cortney  \\\n",
       "0  0.00791  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "1  0.00000  0.000000  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "2  0.00000  0.004314  0.000000  ...           0.0   0.0     0.0      0.0   \n",
       "3  0.00000  0.004490  0.007474  ...           0.0   0.0     0.0      0.0   \n",
       "4  0.00000  0.000000  0.007782  ...           0.0   0.0     0.0      0.0   \n",
       "\n",
       "   incid  erupt  semblanc  miser  shoe  mail  \n",
       "0    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "1    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "2    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "3    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "4    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = calculate_tf_idf(bow_no_stopwords)\n",
    "\n",
    "tf_idf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further [normalize](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.normalize.html#sklearn.preprocessing.normalize) the sample vectors (each document, row) to unit vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movi</th>\n",
       "      <th>film</th>\n",
       "      <th>one</th>\n",
       "      <th>like</th>\n",
       "      <th>time</th>\n",
       "      <th>get</th>\n",
       "      <th>watch</th>\n",
       "      <th>make</th>\n",
       "      <th>even</th>\n",
       "      <th>see</th>\n",
       "      <th>...</th>\n",
       "      <th>championship</th>\n",
       "      <th>...\"</th>\n",
       "      <th>endear</th>\n",
       "      <th>cortney</th>\n",
       "      <th>incid</th>\n",
       "      <th>erupt</th>\n",
       "      <th>semblanc</th>\n",
       "      <th>miser</th>\n",
       "      <th>shoe</th>\n",
       "      <th>mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.028932</td>\n",
       "      <td>0.052582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025058</td>\n",
       "      <td>0.012224</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>0.039294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.051401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027819</td>\n",
       "      <td>0.017667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019941</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013582</td>\n",
       "      <td>0.017251</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019472</td>\n",
       "      <td>0.03241</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.007648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009936</td>\n",
       "      <td>0.009694</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5595 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       movi      film       one      like      time       get     watch  \\\n",
       "0  0.028932  0.052582  0.000000  0.000000  0.025058  0.012224  0.013626   \n",
       "1  0.051401  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.089806  0.000000  0.027819  0.017667  0.000000  0.000000  0.000000   \n",
       "3  0.058460  0.000000  0.013582  0.017251  0.018988  0.000000  0.000000   \n",
       "4  0.007648  0.000000  0.007107  0.000000  0.009936  0.009694  0.010806   \n",
       "\n",
       "       make      even      see  ...  championship  ...\"  endear  cortney  \\\n",
       "0  0.039294  0.000000  0.00000  ...           0.0   0.0     0.0      0.0   \n",
       "1  0.000000  0.000000  0.00000  ...           0.0   0.0     0.0      0.0   \n",
       "2  0.000000  0.019941  0.00000  ...           0.0   0.0     0.0      0.0   \n",
       "3  0.000000  0.019472  0.03241  ...           0.0   0.0     0.0      0.0   \n",
       "4  0.000000  0.000000  0.03392  ...           0.0   0.0     0.0      0.0   \n",
       "\n",
       "   incid  erupt  semblanc  miser  shoe  mail  \n",
       "0    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "1    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "2    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "3    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "4    0.0    0.0       0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5595 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_norm = pd.DataFrame(preprocessing.normalize(tf_idf),columns=tf_idf.columns)\n",
    "tf_idf_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, the tf-idf is now ready to be fed into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Bag of words with N-Grams\n",
    "\n",
    "We created our bag of words using individual words, or unigrams. In principle, we could have used other N-grams as features if we considered that this makes sense for the given classification task.\n",
    "\n",
    "However, it is important to mention that the number of features grows with *N*. Using bigrams gives us roughly twice as many features as the word count of the corpus, trigrams three times as many, and so on. This makes the resulting dataframe sparser (vectors with many zeros), introducing more complexity and noise and potentially harming the machine learning model's ability to find patterns in the data. Additionally, with larger datasets, the vocabulary size already becomes bigger by itself, so combining this with N-grams with *N* > 1 has an even bigger effect on data sparsity.\n",
    "\n",
    "On the other hand, using N-grams as features partially preserves word order information, which may be useful for some tasks!\n",
    "\n",
    "Therefore, the choice of *N* to use in document vectorization should take into account both these advantages and disadvantages. You'll see this decision-making process in action in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing Part 2! The next and last part will be about sentiment analysis, a very common introductory NLP exercise. We're going to apply our new skills to classify movie reviews as positive or negative.\n",
    "\n",
    "You'll also see that `scikit-learn` comes with implementations of both Bag of Words and TF-IDF vectorizers, which will make our lives easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
