{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLU07 - Part 1 of 3 - Preprocessing for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Natural language processing\n",
    "\n",
    "This set of BLUs revolves around the topic of Natural Language Processing (NLP). As the name implies, this field is all about the processing and handling of natural language in a way that enables a computer to do useful things with it. There are plenty of interesting applications around it, namely:\n",
    "\n",
    "- **Speech recognition**: the task of extracting words from a sample of audio. Additional features like prosody can be extracted.\n",
    "- **Natural language generation**: the task of putting computational formulations into actual text. For example, automated generation of image labels, summarisation of text and data, creation of dialogue systems.\n",
    "- **Natural language understanding**: the task of getting some meaning out of text data. For instance, recognizing entities in sentences, semantic roles, or classifying sentences according to their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, some of the main tasks and areas of NLP research are:\n",
    "\n",
    "- **Part of speech tagging**: Determine the role of each word in a given sentence. Is it an adjective, verb, noun?\n",
    "- **Word segmentation**: Break continuous text into words.\n",
    "- **Parsing**: Define a tree that represents the grammatical structure of a sentence.\n",
    "- **Machine translation**: Translate sentences from a source language to a target language automatically.\n",
    "- **Named entity recognition**: Find parts of the text that correspond to certain entities, like the names of places, people, companies.\n",
    "- **Question answering**: Given a question in human language, find the most appropriate answer.\n",
    "- **Text to speech**: Transform written text into audible, human-like sounds that correspond to the given input.\n",
    "\n",
    "Many of these tasks are out of the scope of these learning units, but we still think they're a motivating entry point into exploring this exciting topic. Nowadays, most of these tasks are accomplished with large language models or LLMs. In this course, we'll keep to the NLP basics, so that you have a solid foundation when delving into LLMs later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing text \n",
    "\n",
    "Most fields that use text data require its transformation into an easier to use format. ML models that we used until now require a set of features in tabular form. How can we get there from a piece of text? Maybe you can think of counting words, analyze the diversity of verbs or adjectives, or even look at word combinations like adjectives plus nouns. All this and much more is done in basic text analysis. Before we get to the feature extraction though, we need to preprocess the text. This task usually includes the following steps:\n",
    "\n",
    "- Splitting sentences into words\n",
    "- Lowercasing\n",
    "- Removing punctuation\n",
    "- Removing most common words with little information value (stopwords)\n",
    "- Remove suffixes or prefixes to get to the word base\n",
    "- Filter by part of speech categories\n",
    "\n",
    "At first, these text processing tasks may seem _easy_. For instance, separating the words in a sentence is as simple as looking at the spaces or punctuation between words. But when you really think about the diversity of languages, you start to understand how daunting all these tasks are. Take a look at Mandarin Chinese, for instance. Our heuristic is suddenly not valid anymore. And for many of the tasks, there are plenty of edge cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/xkcd_language_nerd.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom line is: language is hard! But that's what makes this field one of the most challenging but also most rewarding to work in.\n",
    "\n",
    "The first part of this BLU explains fundamental concepts that will be helpful for all the practical tasks that come next in this specialization (and also in the future, if you ever need to work with text data!). We will start by introducing **regular expressions**, followed by three important concepts in data preprocessing (**tokenization**, **stopwords**, and **stemming**). Finally, we will learn about **n-grams**. It helps if you still remember some theory from your language classes. ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding patterns in text\n",
    "\n",
    "To process text efficiently and automatically, we need to be able to parse and detect patterns in the text programatically. For simplicity, we'll only focus on English in the examples.\n",
    "\n",
    "Let's say you have the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_example = \"\"\"\n",
    "Bommer Canyon is an open space preserve in southern Irvine, California featuring hiking and \n",
    "biking trails as well as private event areas. The canyon is part of the Irvine Ranch, which \n",
    "itself is a National Natural Landmark, the first California Natural Landmark,[1][2] and part \n",
    "of the City of Irvine Open Space Preserve.[3][4] The preserve is adjacent to the affluent \n",
    "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16,000 acres of \n",
    "preserved open space.[5] Approximately 15 of these acres are preserved as a \"Cattle Camp\" \n",
    "named for the area's previous cattle operations and are now rented for private events such \n",
    "as campouts, company picnics, and family reunions.[6] The trails in Bommer Canyon feature \n",
    "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
    "residents who use them for nature walks, hiking and mountain biking.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you answer the question:\n",
    "\n",
    "> What are all the words in the following text that start with the letter `a`?\n",
    "\n",
    "You could obviously count them manually. But what if you had thousands or millions of lines of text? That becomes impossible!\n",
    "\n",
    "A second option, given your recently acquired Python skills, could be to write a function that does that for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_words_starting_with_a(text):\n",
    "    \n",
    "    # Assuming all words can be split by spaces - big assumption\n",
    "    list_of_a_words = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w.startswith(\"a\"):\n",
    "            list_of_a_words.append(w)\n",
    "    \n",
    "    return list_of_a_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas.\n",
      "- a\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- a\n",
      "- area's\n",
      "- and\n",
      "- are\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked quite well. Of course, we only found words starting with `a`, not `A`.\n",
    "\n",
    "Now let's take a slightly more complex task: we want to find and remove all punctuation, replacing it with a space when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \n",
    "    big_list_of_punctuation = [\n",
    "        \".\", \",\", \"?\", \"!\", \"-\", \"_\", \":\", \";\",\n",
    "        \"\\\"\", \"'\", \"|\", \"(\", \")\", \")\", \"/\", \"\\\\\",\n",
    "        \"[\", \"]\",\n",
    "    ]\n",
    "    processed_text = \"\"\n",
    "    for idx, letter in enumerate(text):\n",
    "        previous_letter = text[idx - 1] if idx > 1 else \"\"\n",
    "        next_letter = text[idx + 1] if idx < len(text) - 2 else \"\"\n",
    "        \n",
    "        # If the letter is punctuation, and the previous and next letters are not spaces, add a space\n",
    "        if letter in big_list_of_punctuation:\n",
    "            if previous_letter != \" \" and next_letter != \" \":\n",
    "                processed_text += \" \"\n",
    "        else:\n",
    "            processed_text += letter\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and \n",
      "biking trails as well as private event areas The canyon is part of the Irvine Ranch which \n",
      "itself is a National Natural Landmark the first California Natural Landmark  1  2 and part \n",
      "of the City of Irvine Open Space Preserve  3  4 The preserve is adjacent to the affluent \n",
      "Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of \n",
      "preserved open space  5 Approximately 15 of these acres are preserved as a Cattle Camp \n",
      "named for the area s previous cattle operations and are now rented for private events such \n",
      "as campouts company picnics and family reunions  6 The trails in Bommer Canyon feature \n",
      "groves of oak and sycamore trees as well as rough rock outcrops and are popular with area \n",
      "residents who use them for nature walks hiking and mountain biking \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(remove_punctuation(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly more complicated, but seems to have worked! Now let's see apply the function on a different text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adenanthos venosus is an openly branched shrub that typically grows to a height of 1–2 m \n",
      " 3 ft 3 in – 6 ft 7 in and forms a lignotuber Its leaves are mostly arranged in clusters \n",
      "at the ends of branches egg shaped sometimes with the narrower end towards the base \n",
      "mostly 15–20 mm 0 59–0 79 in long 10 mm 0 39 in wide and sessile The leaves are \n",
      "mostly glabrous and have a pointed tip The flowers are ”dull crimson” to ”pinkish purple” \n",
      "with a cream coloured band in the centre and many glandular hairs on the outside \n",
      "\n"
     ]
    }
   ],
   "source": [
    "different_text_example = \"\"\"\n",
    "Adenanthos venosus is an openly-branched shrub that typically grows to a height of 1–2 m \n",
    "(3 ft 3 in – 6 ft 7 in) and forms a lignotuber. Its leaves are mostly arranged in clusters \n",
    "at the ends of branches, egg-shaped, sometimes with the narrower end towards the base, \n",
    "mostly 15–20 mm (0.59–0.79 in) long, 10 mm (0.39 in) wide and sessile. The leaves are \n",
    "mostly glabrous and have a pointed tip. The flowers are ”dull crimson” to ”pinkish purple” \n",
    "with a cream-coloured band in the centre and many glandular hairs on the outside. \n",
    "\"\"\"\n",
    "\n",
    "print(remove_punctuation(different_text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like this time it did not go so well... In particular, we missed some `”` characters which are different from the ones defined in our function.\n",
    "\n",
    "Of course, we could go back and re-write our function... But there's a much better way: **regular expressions**.\n",
    "\n",
    "Regular expressions help us to easily remove punctuation and execute many more typical text processing tasks like:\n",
    "- Replace all numbers with a placeholder\n",
    "- Remove all decimals from numbers\n",
    "- Count all uppercase letters in a text\n",
    "- Find all words that have less than 3 letters (or any other number)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Regular expressions (aka Regex)\n",
    "\n",
    "Regular expressions are sequences of characters that allow us to define search patterns in a standardized way. They are one of the most fundamental concepts in computer science when working with text data.\n",
    "\n",
    "The idea is simple: by defining a text pattern using regex rules, you can easily locate and replace it in any piece of text!\n",
    "\n",
    "Most of the tasks that we defined before can be performed using regex. Let's see our first example: finding all words starting with `a` in the text. We will use the powerful [re](https://docs.python.org/3/library/re.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_words_starting_with_a_regex(text):\n",
    "    \n",
    "    pattern = r\"\\ba\\w+\\b\"\n",
    "    return re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found that start with 'a':\n",
      "- an\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- areas\n",
      "- and\n",
      "- adjacent\n",
      "- affluent\n",
      "- and\n",
      "- and\n",
      "- acres\n",
      "- acres\n",
      "- are\n",
      "- as\n",
      "- area\n",
      "- and\n",
      "- are\n",
      "- as\n",
      "- and\n",
      "- and\n",
      "- as\n",
      "- as\n",
      "- and\n",
      "- are\n",
      "- area\n",
      "- and\n"
     ]
    }
   ],
   "source": [
    "print(\"Words found that start with 'a':\")\n",
    "for w in find_all_words_starting_with_a_regex(text_example):\n",
    "    print(\"- \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more compact, right? Now let's do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_regex(text):\n",
    "    \n",
    "    pattern_punct = r\"[\\.,\\?\\!\\-\\_\\:\\;\\\"'\\|\\(\\)/\\\\\\[\\]]\"\n",
    "    \n",
    "    # Replaces all punctuation characters by spaces\n",
    "    text_no_punct = re.sub(pattern_punct, \" \", text)\n",
    "    \n",
    "    # Collapses multiple spaces\n",
    "    text_no_punct = re.sub(r\"\\s+\", \" \", text_no_punct)\n",
    "\n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bommer Canyon is an open space preserve in southern Irvine California featuring hiking and biking trails as well as private event areas The canyon is part of the Irvine Ranch which itself is a National Natural Landmark the first California Natural Landmark 1 2 and part of the City of Irvine Open Space Preserve 3 4 The preserve is adjacent to the affluent Irvine villages of Shady Canyon and Turtle Ridge and features roughly 16 000 acres of preserved open space 5 Approximately 15 of these acres are preserved as a Cattle Camp named for the area s previous cattle operations and are now rented for private events such as campouts company picnics and family reunions 6 The trails in Bommer Canyon feature groves of oak and sycamore trees as well as rough rock outcrops and are popular with area residents who use them for nature walks hiking and mountain biking \n"
     ]
    }
   ],
   "source": [
    "print(remove_punctuation_regex(text_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool right? \n",
    "\n",
    "Regex enables us to do many interesting things with text. If you're still not convinced, try out this [website](https://regex101.com/)! You can play around with regex and validate that the patterns you wrote are performing as expected.\n",
    "\n",
    "You might have noticed a bunch of backslashes in the regex pattern and an `r` at the beginning. The backslash `\\` character is used  in both Python strings and regex expressions to give special meaning to otherwise ordinary characters. For example, `\\n` means 'newline' in Python strings and `\\s`  means 'whitespace' in regex expressions. Backslash can also be used to escape characters that would otherwise have a special meaning, like the backslash itself or the dot.\n",
    "\n",
    "Python only allows approved escape sequences in strings. If we use backslash to create a regex escape sequence which Python does not know, we will get a warning. That is the reason why we put an `r` in front of our string, to make it into a so-called 'raw string'. You can put whatever you want in a raw string because Python will take it literally just like a sequence of characters and not try to interpret it into escape sequences. The backslashes only acquire special meaning when the string is interpreted by regex as a regex pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find digits\n",
    "pattern_digits = \"[0-9]*\"\n",
    "\n",
    "# Find words smaller than 3 characters\n",
    "pattern_words_until_3 = r\"\\b\\w{1,3}\\b\"\n",
    "\n",
    "# Find URLs in a text\n",
    "pattern_urls = r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you may be looking a bit like this:\n",
    "\n",
    "<img src=\"./media/regex-confusion.png\" width=\"500\">\n",
    "\n",
    "But worry not! \n",
    "\n",
    "Most of us don't know regex patterns by heart, and we need to take a look at cheatsheets from time to time, like the one shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Regex cheatsheet ([see more](https://cheatography.com/davechild/cheat-sheets/regular-expressions/))\n",
    "|   |   |   |\n",
    "| :---    | --- | :---    | \n",
    "| `.`  matches any character, except newline. | | `x*` matches 0 or more x symbols. |\n",
    "| `\\d, \\s, \\S`  digit, whitespace, not whitespace. | | `x+` matches 1 or more x symbols. |\n",
    "| `\\b, \\B`  word, not word boundary. | | `x?` matches 0 or 1 x symbol. |\n",
    "| `[xyz]`  matches x, y or z. | | `.?`, `*?`, `+?` non-greedy search, matches as few characters as possible| \n",
    "| `[^xyz]`  matches anything that is not x, y or z. | | `x{5}` matches exactly 5 x symbols. |\n",
    "| `[x-z]`  matches characters from x to z. | | `x{5, 8}` matches between 5 and 8 x symbols. |\n",
    "| `^xyz$`  `^` is the start of the string, `$` is the end of the string. | | `xy\\|yz` - matches `xy` or `yz`.|\n",
    "| `\\.`  use escaping to match special characters. | | `\\t`, `\\n` - tab and newline. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Finding one pattern match with `search()` \n",
    "\n",
    "Using `search()` we can take a pattern and look for it in a text. This function will return a `Match` object, from which we can obtain the first text portion that was matched by our pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for \"Madrid\":\n",
      "<re.Match object; span=(7, 13), match='Madrid'>\n",
      "\n",
      "Looking for \"Rome\":\n",
      "None\n",
      "\n",
      "Looking for \"Lisbon\":\n",
      "<re.Match object; span=(0, 6), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "print(\"Looking for \\\"Madrid\\\":\")\n",
    "match = re.search(\"Madrid\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Rome\\\":\")\n",
    "match = re.search(\"Rome\", text)\n",
    "print(match)\n",
    "\n",
    "print(\"\\nLooking for \\\"Lisbon\\\":\")\n",
    "match = re.search(\"Lisbon\", text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is already possible to observe some things about `re.search()`:\n",
    "\n",
    "- When there is no match, `search()` returns `None`.\n",
    "\n",
    "- The `Match` object has the index of the beginning and end of the match. They can be  used via `match.start()` and `match.end()`.\n",
    "\n",
    "- If there is more than one instance of the word in the text, only the first one will be retrieved.\n",
    "\n",
    "#### 3.1.3 Finding all pattern matches with `findall()`  or `finditer()`\n",
    "\n",
    "If we want to return all the matches to our pattern in a given text we can use the function `findall()`. In this case, the matched portions of the text will be returned, instead of the `Match` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon\n",
      "Lisbon\n",
      "Lisbon\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "pattern = r\"Lisbon\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that one of the words was written as _Lisbona_ , but we still match the _Lisbon_ portion of that word. If we add the condition of having a white space after the letter *n* we will only get two matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisbon \n",
      "Lisbon \n"
     ]
    }
   ],
   "source": [
    "pattern = r\"Lisbon\\s\"\n",
    "\n",
    "for match in re.findall(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we really want the `Match` objects for some reason, `finditer()` can help us on that. It returns an iterator with `Match` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 6), match='Lisbon'>\n",
      "<re.Match object; span=(14, 20), match='Lisbon'>\n",
      "<re.Match object; span=(34, 40), match='Lisbon'>\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Lisbon\"\n",
    "\n",
    "for match in re.finditer(pattern, text):\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Replacing all pattern matches with `sub()`\n",
    "\n",
    "If we want to replace the matches of our pattern in a given text with something else we need to use the function `sub()`. In this case, the matched portions of the text will be replaced, and the changed text will be returned.\n",
    "\n",
    "For example, if we wanted to remove the word `Lisbon` from a text we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Madrid  Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "# \\b indicates a word boundary so using it around a word \n",
    "# will only replace the text when it shows as a proper word, \n",
    "# between spaces or punctuation \n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If instead we wanted to replace it by another text we can specify it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lisboa Madrid Lisboa Toulose Oslo Lisbona\n"
     ]
    }
   ],
   "source": [
    "text = \"Lisbon Madrid Lisbon Toulose Oslo Lisbona\"\n",
    "\n",
    "pattern = r\"\\bLisbon\\b\"\n",
    "\n",
    "print(re.sub(pattern, \"Lisboa\", text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 A primer on patterns\n",
    "\n",
    "Now that you are familiar with the `re` functions, we'll use them to explore the different patterns that can be expressed with regex.\n",
    "\n",
    "Let's start with simple patterns from the cheatsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"x xy xyy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.` will match any character after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x ', 'xy', 'xy']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"x.\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`*` will match 0 or more y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xyy']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy*\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`+` will match 1 or more y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xy', 'xyy']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` will match 0 or 1 y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'xy', 'xy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy?\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{i}` will match i y symbols after x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xyy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"xy{2}\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Jani Senna conway Kobayashi Lopez buemi Nakajima alonso\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match only the names that start with capital letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text) # find substrings starting with a capital letter\n",
    "                                # followed by 1 or more lowercase letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to match all the names that don't start with letters \"B\" and \"L\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jani', 'Senna', 'conway', 'Kobayashi', 'Nakajima', 'alonso']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\b[^bBlL\\s][A-Za-z]+\", text) # find substrings after a word boundary that\n",
    "                                          # do not begin with B or L or whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using that hacky `r` for raw strings again to tell Python to interpret backslashes `\\` literally (notice how our regex has `\\b` and `\\s`). For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With r:\n",
      "\n",
      "lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\n",
      "\n",
      "\n",
      "Without r:\n",
      "\n",
      "lotterer \n",
      " Jani \n",
      " Senna conway Kobayashi Lopez buemi Nakajima alonso\n"
     ]
    }
   ],
   "source": [
    "print(\"With r:\\n\")\n",
    "print(r\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")\n",
    "print(\"\\n\")\n",
    "print(\"Without r:\\n\")\n",
    "print(\"lotterer \\n Jani \\n Senna conway Kobayashi Lopez buemi Nakajima alonso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case Python takes `\\n` literally. In the second case, Python interprets it as the symbol for newline.\n",
    "\n",
    "Another important thing to know is that, since regex interprets some characters in a special way, you need to escape them if you want to match them. For that purpose you also use the `\\`. Whatever comes after this character is considered escaped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=r\"Text \\ with + special [characters].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\".+[ ]\\ \", text): # If we don't escape the characters we mean to\n",
    "    print(m)                           # find, we won't match anything and could \n",
    "                                       # even have a broken regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:\n",
      "\n",
      "\\\n",
      "+\n",
      "[\n",
      "]\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "print(\"Matches:\\n\")\n",
    "\n",
    "for m in re.findall(r\"[\\.\\+\\[\\]\\\\]\", text): # If we escape the characters we mean to \n",
    "    print(m)                                 # find, we'll match them as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice in particular the `\\\\`. As the backlash is used to escape any character, it's also used to escape itself.  \n",
    "\n",
    "<img src=\"./media/backslashes.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine now that we have some extra information after the names, and we receive a file with many lines. We still want only names starting with capital letters. So we run the previous regex and..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"lotterer Rebellion\\nJani Rebellion\\nSenna Rebellion\\nconway Toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima Toyota\\nalonso Toyota\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion',\n",
       " 'Jani',\n",
       " 'Rebellion',\n",
       " 'Senna',\n",
       " 'Rebellion',\n",
       " 'Toyota',\n",
       " 'Kobayashi',\n",
       " 'Toyota',\n",
       " 'Lopez',\n",
       " 'Toyota',\n",
       " 'Toyota',\n",
       " 'Nakajima',\n",
       " 'Toyota',\n",
       " 'Toyota']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, actually we just want the first name! So let's try to add the symbol `^` to make sure the expression only captures the beginning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hum.. we got a handful of nothing. Why is this happening? Well, the regex processes all the text as a single line, and the first name doesn't start with a capital letter. To prove this is the case, let's change `lotterer` to `Lotterer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Lotterer Rebellion\\nJani rebellion\\nSenna Rebellion\\nconway toyota\\nKobayashi Toyota\\nLopez Toyota\\nbuemi Toyota\\nNakajima toyota\\nalonso Toyota\"\n",
    "re.findall(\"^[A-Z][a-z]+\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still only capture one line. Luckily, we have [`re.MULTILINE`](https://docs.python.org/3/library/re.html#re.MULTILINE), that allows us to process multiline strings easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer', 'Jani', 'Senna', 'Kobayashi', 'Lopez', 'Nakajima']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"^[A-Z][a-z]+\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we were able to get all the information we wanted! And what if we wanted the second name on each line? Well, in this case, that is the last word of the line, so we can use `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rebellion', 'Rebellion', 'Toyota', 'Toyota', 'Toyota', 'Toyota']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\"[A-Z][a-z]+$\", text, re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want all full lines ending with `rebellion`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lotterer Rebellion', 'Jani rebellion', 'Senna Rebellion']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\".*rebellion$\", text, flags=(re.MULTILINE|re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that here we are also taking advantage of the flag `re.IGNORECASE`. This is a convenient flag to add if you want case-insensitive matches. Multiple regex flags can be strung together with pipes: `|`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can get hard to read really fast, but even knowing the basics will be certainly helpful sometime in the future. To better understand how they work, there's nothing like practicing, and sites like [this](https://regex101.com/) are valuable visual tools to do so. The Python library that we used has a lot more powerful methods, so make sure to check them out if you're interested!\n",
    "\n",
    "Here are more reading suggestions about regex:\n",
    "- https://towardsdatascience.com/regular-expressions-clearly-explained-with-examples-822d76b037b4\n",
    "- https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "Now that we've seen how to find patterns in text, let's explore how to partition it into smaller, meaningful tokens.\n",
    "\n",
    "This task is called **tokenizatio* and it's an essential step when dealing with text. In practice, tokenization is about splitting the strings of a corpus into substrings. This is important because we can transform a string into parts that are more suitable to be used by natural language processing tools. For instance, if we are working with the sentence:\n",
    "\n",
    "_\"The car went too fast on the second lap. This damaged the tires.\"_ ,\n",
    "\n",
    "it will get easier if we split it into substrings:\n",
    "\n",
    "_[\"The\", \"car\", \"went\", \"too\", \"fast\", \"on\", \"the\", \"second\", \"lap\", \".\", \"This\", \"damaged\", \"the\", \"tires\", \".\"]_ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/tokenizer.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully by now you've realized that this task is more than just splitting the text on spaces and requires a bit more thought.\n",
    "\n",
    "We will now use a specialized NLP library [NLTK](https://www.nltk.org/_modules/nltk/tokenize/regexp.html) which implements different tokenization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The car went too fast on the second lap. This damaged the tires...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [RegexpTokenizer](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.RegexpTokenizer) is using regular expressions to define the token creation. We can choose the regex to match either the tokens or the gaps between them. In this case we want tokens that are either words, amounts in dollars or non-whitespace characters like punctuation.\n",
    "\n",
    "In the following example, the tokenizer selects the words that begin with capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'This']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Z]\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [nltk.tokenize.regexp](https://www.nltk.org/api/nltk.tokenize.regexp.html) module has several tokenizers with predefined regular expressions. These are:\n",
    "- `BlanklineTokenizer` - Tokenize on blank lines as delimiters.\n",
    "- `WordPunctTokenizer` - Tokenize into sequences of alphabetic and non-alphabetic characters.\n",
    "- `WhitespaceTokenizer`- Tokenize on spaces, tabs, and newlines as delimiters.\n",
    "\n",
    "Let's see how they tokenize the same sentence as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The car went too fast on the second lap. This damaged the tires...']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BlanklineTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap',\n",
       " '.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires',\n",
       " '...']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordPunctTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'car',\n",
       " 'went',\n",
       " 'too',\n",
       " 'fast',\n",
       " 'on',\n",
       " 'the',\n",
       " 'second',\n",
       " 'lap.',\n",
       " 'This',\n",
       " 'damaged',\n",
       " 'the',\n",
       " 'tires...']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WordPunctTokenizer()` is similar to the first one we defined (`RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')`. This tokenizer is one of the most commonly used. So, when we talk about tokenization without specifying further details, this is by default the type of tokenization that we expect you to use (for example, in exercises)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stemming and lemmatization\n",
    "\n",
    "Now that we have the tokens, we're going to stem them. **Stemming** allows us to get the root or base of the words. For example, 'fishing' and 'fisher' can both be reduced to the same stem 'fish'.  This is important because, in certain tasks, we are more interested in the broader semantics of a word and not the specific variation of it, like if it's a verb or an adjective.\n",
    "\n",
    "We will be using the NLTK [snowball stemmer](https://www.nltk.org/api/nltk.stem.snowball.html). There are different stemming algorithms, some of them specialized for certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'this', 'damag', 'the', 'tire', '...']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the stem doesn't have to be a word or correspond to the morphological root. Notice as well that all the words have been lowercased. Lowercasing the text is typically one of the first steps in text preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides stemming there is also the process of **lemmatization**. Both processes share the goal of getting the root of the word, or more formally, reduce inflectional forms of a word to a common base form [\\[7\\]](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html), but they act differently. Stemming follows a heuristic approach that removes the word endings in order to get closer to the common base form. Lemmatization reduces different inflectional forms of a word to the base or dictionary form of a word known as _lemma_ and respects categories like nouns and verbs.\n",
    "\n",
    "For example, given the word _consultations_, stemming would return only *consult*, while lemmatization would take into account that the word is a noun and return *consultation*.\n",
    "\n",
    "As you may expect, lemmatization is much more expensive in computational terms and, for certain applications, stemming might be more than enough to obtain good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stopwords \n",
    "\n",
    "**Stopwords** are common words that don't have much semantic meaning such as pronouns or articles. Most times, we don't want to include them in further analysis.\n",
    "\n",
    "As an example of why removing stopwords might be an important step, imagine a search engine going through a large set of documents. Words as \"*the*\", \"*a*\", \"*at*\", etc. will be present in so many documents that using them in the search will not help at all to find the best documents to answer the query. So filtering them out will speed up the search and remove noise.\n",
    "\n",
    "Let's start by downloading the stopwords corpus provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/maria/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get a list of stopwords in any language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_eng = set(stopwords.words('english'))\n",
    "\n",
    "stop_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estar',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'haver',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'ser',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "stop_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all stopwords from our tokens list. We first need to lowercase the words since stopwords are all saved in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'went', 'fast', 'second', 'lap', '.', 'damaged', 'tires', '...']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in words if word.lower() not in stop_eng]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. N-Grams\n",
    "\n",
    "Creating **N-gram** is one of the oldest NLP strategies. N-gram is a sequence of N consecutive tokens. The elements can be words, but also punctuation, depending on how we define the tokens. Depending on the value choosen for N, we can have unigrams, bigrams, trigrams, four-grams, etc.\n",
    "\n",
    "For instance, for the text\n",
    "\n",
    "`\"The driver made a mistake\"`,\n",
    "\n",
    "we have:\n",
    "\n",
    "- unigrams: `The`, `driver`, `made`, `a`, `mistake`\n",
    "- bigrams: `The driver`, `driver made`, `made a`, `a mistake`\n",
    "- trigrams: `The driver made`, `driver made a`, `made a mistake`\n",
    "- four-grams: `The driver made a`, `driver made a mistake`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create N-grams by taking advantage of the [NLTK N-gram](https://www.nltk.org/api/nltk.util.html#nltk.util.ngrams) implementation. We will be using the tokenized list `words` created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'car', 'went', 'too', 'fast', 'on', 'the', 'second', 'lap', '.', 'This', 'damaged', 'the', 'tires', '...']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The',), ('car',), ('went',), ('too',), ('fast',), ('on',), ('the',), ('second',), ('lap',), ('.',), ('This',), ('damaged',), ('the',), ('tires',), ('...',)]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car'), ('car', 'went'), ('went', 'too'), ('too', 'fast'), ('fast', 'on'), ('on', 'the'), ('the', 'second'), ('second', 'lap'), ('lap', '.'), ('.', 'This'), ('This', 'damaged'), ('damaged', 'the'), ('the', 'tires'), ('tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'car', 'went'), ('car', 'went', 'too'), ('went', 'too', 'fast'), ('too', 'fast', 'on'), ('fast', 'on', 'the'), ('on', 'the', 'second'), ('the', 'second', 'lap'), ('second', 'lap', '.'), ('lap', '.', 'This'), ('.', 'This', 'damaged'), ('This', 'damaged', 'the'), ('damaged', 'the', 'tires'), ('the', 'tires', '...')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use N-grams in several ways, for instance as features for a text classification model as we'll see in the following notebooks.\n",
    "\n",
    "For now let's just look at situations where N-grams can be useful:\n",
    "\n",
    "- By comparing the frequency of each N-gram in two different texts, we can calculate the similarity between them.\n",
    "- By looking at the frequency of the 2-gram 'Very good' we may have an indicator of the sentiment associated with the text. On the other hand, solely looking at the frequency of the unigrams 'Very' and 'good' might not convey the same meaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Wrap up and further reading\n",
    "\n",
    "And you've reached the end of our first notebook, congratulation! You've learned the following concepts:\n",
    "\n",
    "* regex\n",
    "* tokenization\n",
    "* stemming\n",
    "* stopwords\n",
    "* N-grams\n",
    "\n",
    "It may seem overwhelming for now, but you'll see everything will become more intuitive as you navigate your journey in the NLP world! So keep the motivation and see you in Part 2 where we'll use the preprocessed tokens in a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/info_everywhere.jpg\" width=\"400\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful resources:\n",
    "\n",
    "- [RegExr](https://regexr.com/3lvai)\n",
    "- [RegexOne](https://regexone.com/)\n",
    "- [Python Module of the Week - re](https://pymotw.com/2/re/)\n",
    "- [NLTK Book](https://www.nltk.org/book/)\n",
    "- [Language Model](https://en.wikipedia.org/wiki/Language_model)\n",
    "- [Stanford CS124 Language Modeling slides](https://web.stanford.edu/class/cs124/lec/lm2021.pdf)\n",
    "- [Stemming and lemmatization](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "Other commonly used NLP libraries:\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [gensim](https://radimrehurek.com/gensim/)\n",
    "- [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/other-languages.html#python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
